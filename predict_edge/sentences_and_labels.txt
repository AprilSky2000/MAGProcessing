Others+Since we only use shallow methods for textual analysis that do not generate a dependency structure , we can not use complex methods for text reduction as described , e.g. , in ( Jing , 2000 ) .
Others+particular topic ( Lin and Hovy , 2000 ) .
Others+Recently , content features were also well studied , including centroid ( Radev et al. , 2004 ) , signature terms ( Lin and Hovy , 2000 ) and high frequency words ( Nenkova e t al. , 2006 ) .
Others+More advanced methods for query expansion use “ topic signatures ” – words and grammatically related pairs of words that model the query and even the expected answer from sets of documents marked as relevant or not ( Lin & Hovy , 2000 ; Harabagiu , 2004 ) .
Others+Topic signatures are words highly descriptive of the input , as determined by the application of loglikelihood test ( Lin and Hovy , 2000 ) .
Others+2 NP - rewrite enhanced frequency summarizer Frequency and frequency - related measures of importance have been traditionally used in text summarization as indicators of importance ( Luhn , 1958 ; Lin and Hovy , 2000 ; Conroy et al. , 2006 ) .
Others+ROUGE ( Lin and Hovy 2000 ) compares any summary to any other ( typically human - generated ) summary using a recall - oriented approach .
Others+This is similar to the idea of topic signature introduced in ( Lin and Hovy 2000 ) .
Others+The notion of topic signatures was first introduced in ( Lin and Hovy , 2000 ) .
Others+To date , researchers have harvested , with varying success , several resources , including concept lists ( Lin and Pantel 2002 ) , topic signatures ( Lin and Hovy 2000 ) , facts ( Etzioni et al. 2005 ) , and word similarity lists ( Hindle 1990 ) .
Others+Under this approach , topic representations like those introduced in ( Lin and Hovy , 2000 ) and ( Harabagiu , 2004 ) are used to identify a set of text passages that are relevant to a user ’s domain of interest .
Others+The scores are usually computed based on a combination of statistical and linguistic features , including term frequency , sentence position , cue words , stigma words , topic signature ( Hovy and Lin , 1997 ; Lin and Hovy , 2000 ) , etc. .
Others+In summarization , topic signatures are a set of terms indicative of a topic ( Lin and Hovy , 2000 ) .
Others+Among them , query relevance , centroid ( Radev et al. , 2004 ) and signature term ( Lin and Hovy , 2000 ) are most remarkable .
Others+For single document summarization , the sentence score is usually computed by empirical combination of a number of statistical and linguistic feature values , such as term frequency , sentence position , cue words , stigma words , topic signature ( Luhn 1969 ; Lin and Hovy , 2000 ) .
Others+In the task of single document summarization , various features have been investigated for ranking sentences in a document , including term frequency , sentence position , cue words , stigma words , and topic signature ( Luhn 1969 ; Lin and Hovy , 2000 ) .
Others+In such cases , neither global features ( Chieu and Ng , 2002 ) nor aggregated contexts ( Chieu and Ng , 2003 ) can help .
Extends+Several benchmark data sets have been used to evaluate IE approaches on semistructured texts ( Soderland , 1999 ; Ciravegna , 2001 ; Chieu and Ng , 2002a ) .
Extends+More recently , machine learning approaches have been used for IE from semi-structured texts ( Califf and Mooney , 1999 ; Soderland , 1999 ; Roth and Yih , 2001 ; Ciravegna , 2001 ; Chieu and Ng , 2002a ) , named entity extraction ( Chieu and Ng , 2002b ) , template element extraction , and template relation extraction ( Miller et al. , 1998 ) .
Extends+They include sentence segmentation ( Ratnaparkhi , 1998 ) , part - ofspeech tagging ( Charniak et al. , 1993 ) , named entity recognition ( Chieu and Ng , 2002b ) , full parsing ( Collins , 1999 ) , and coreference resolution ( Soon et al. , 2001 ) .
Extends+To give an idea of the informative features used in the classifier of a slot , we rank the features used for a slot classifier according to their correlation metric values ( Chieu and Ng , 2002a ) , where informative features are ranked higher .
Extends+They are automatically derived based on the correlation metric value used in ( Chieu and Ng , 2002a ) .
Others+Chieu and Ng ( 2002 ) propose a solution to this problem : for each token , they define additional features taken from other occurrences of the same token in the document .
Others+( Borthwick , 1999 ) made a second tagging pass which uses information on token sequences tagged in the first pass ; ( Chieu and Ng , 2002 ) used as features information about features assigned to other instances of the same token .
Others+Global information is known to be useful in other NLP tasks , especially in the named entity recognition task , and several studies successfully used global features ( Chieu and Ng , 2002 ; Finkel et al. , 2005 ) .
Others+Chieu and Ng ( 2002 ) conducted named entity recognition using global features as well as local features .
Others+et al. , 2005 ; Chieu and Ng , 2002 ; Sutton and McCallum , 2004 ; Bunescu and Mooney , 2004 ) .
Others+Chieu and Ng ( 2002 ) propose a solution to this problem : for each token , they define additional features based on known information , taken from other occurrences of the same token in the document .
Others+Furthermore , Bos et al. ( 2004 ) and Bos ( 2005 ) have demonstrated that the output of CCGbank parsers can be successfully translated into Kamp and Reyle ’s ( 1993 ) Discourse Representation Theory structures , to support question answering and the textual entailment task ( Bos and Markert 2005 ) .
Others+Finally , a few efforts ( Akhmatova , 2005 ; Fowler et al. , 2005 ; Bos and Markert , 2005 ) have tried to translate sentences into formulas of first - order logic , in order to test logical entailment with a theorem prover .
Others+Early deep semantic models ( e.g. , ( Norvig , 1987 ) ) as well as more recent ones ( e.g. , ( Tatu and Moldovan , 2005 ; Bos and Markert , 2005 ; Roth and Sammons , 2007 ) ) rely on specific world knowledge encoded in rules for drawing decisions .
Others+These transformations are logical rules in ( Bos and Markert , 2005 ) or sequences of allowed rewrite rules in ( de Salvo Braz et al. , 2005 ) .
Others+Some NLP applications deal indirectly with negation , e.g. , machine translation ( van Munster , 1988 ) , text classification ( Rose et al. , 2003 ) and recognizing entailments ( Bos and Markert , 2005 ) .
Others+NutCracker3 ( Bos and Markert , 2005 ) is a system based on logical representation and automatic theorem proving , but utilizes only WordNet ( Fellbaum , 1998 ) as a lexical knowledge resource .
Others+Conversely , computational models of formal semantics have shown low recall on practical applications , stemming from their reliance on ontologies such as WordNet ( Miller , 1995 ) to model the meanings of content words ( Bobrow et al. , 2007 ; Bos and Markert , 2005 ) .
Others+Related methods incorporate measurements of similarity at various levels : lexical ( Malakasiotis and Androutsopoulos , 2007 ) , syntactic ( Malakasiotis , 2009 ; Zanzotto et al. , 2009 ) , and semantic ( Rinaldi et al. , 2003 ; Bos and Markert , 2005 ) .
Others+Logic - based approach is to map the language expressions to logical meaning representations , and then rely on logical entailment checks , possibly by invoking theorem provers ( Rinaldi et al. , 2003 ; Bos & Markert , 2005 ; Tatu & Moldovan , 2005 , 2007 ) .
Extends+Of existing RTE approaches , the closest to ours is by Bos and Markert ( 2005 ) , who employ a purely logical approach that uses Boxer to convert both the premise and hypothesis into first - order logic and then checks for entailment using a theorem prover .
Extends+This approach can be viewed as a bridge between Bos and Markert ( 2005 ) ’s purely logical approach , which relies purely on hard logical rules and theorem proving , and distributional approaches , which support graded similarity between concepts but have no notion of logical operators or entailment .
Extends+For the RTE task , systems were evaluated using both accuracy and confidence - weighted score ( cws ) as used by Bos and Markert ( 2005 ) and the RTE1 challenge ( Dagan et al. , 2005 ) .
Extends+For the RTE task , systems were evaluated using both accuracy and confidence - weighted score ( cws ) as used by Bos and Markert ( 2005 ) and the RTE1 challenge ( Dagan et al. , 2005 ) .
Others+Moving to sentences increases the complexity exponentially and as a result has led to measurements of similarity at various levels : lexical ( Malakasiotis and Androutsopoulos , 2007 ) , syntactic ( Malakasiotis , 2009 ; Zanzotto et al. , 2009 ) , and semantic ( Rinaldi et al. , 2003 ; Bos and Markert , 2005 ) .
Others+Several models , ranging from the simple lexical similarity between T and H to advanced Logic Form Representations , have been proposed ( Corley and Mihalcea , 2005 ; Glickman and Dagan , 2004 ; de Salvo Braz et al. , 2005 ; Bos and Markert , 2005 ) .
Others+They , therefore , tend to have high precision at the cost of low recall ( Bos and Markert , 2005 ) .
Others+Bos and Markert ( 2005 ) describe a system for Recognizing Textual Entailment ( RTE ) that uses Boxer to convert both the premise and hypothesis of an RTE pair into first - order logical semantic representations and then uses a theorem prover to check for logical entailment .
Others+Note that when running an example in the theorem prover , weights are not possible , so any rule that would be weighted in an MLN is simply treated as a “ hard clause ” following Bos and Markert ( 2005 ) .
Others+The BioNLP ’09 shared task involved documents contained also in the GENIA treebank ( Tateisi et al. , 2005 ) , creating an opportunity for direct study of intrinsic and task - oriented evaluation results .
Others+For comparison and evaluation , the texts in the GENIA treebank ( Tateisi et al. , 2005 ) are converted to the various formats as follows .
Others+We used the Stanford parser ( Klein and Manning , 2003 ) , and also a variant of the Stanford parser ( i.e. , Stanford - Genia ) , which was trained on the GENIA treebank ( Tateisi et al. , 2005 ) for biomedical text ; • Deep parsers aim to compute in - depth syntactic and semantic structures based on syntactic theories such as HPSG ( Pollard and Sag , 1994 ) and CCG ( Steedman ,
Others+Our biomedical data comes from the GENIA treebank8 ( Tateisi et al. , 2005 ) , a corpus of abstracts from the Medline database .9 We downloaded additional sentences
Others+Because our target is biomedical texts , we re-trained a parser ( Hara et al. , 2005 ) with the GENIA treebank ( Tateisi et al. , 2005 ) , and also applied a bidirectional part - ofspeech tagger ( Tsuruoka and Tsujii , 2005 ) trained with the GENIA treebank as a preprocessor .
Others+The GENIA treebank ( Tateisi et al. , 2005 ) consists of 500 abstracts ( 4,446 sentences ) extracted from MEDLINE .
Others+The POS analyzer and HPSG parser are trained by using the GENIA corpus ( Tsuruoka et al. , 2005 ; Hara et al. , 2005 ) , which comprises around 2,000 MEDLINE abstracts annotated with POS and Penn Treebank style syntactic parse trees ( Tateisi et al. , 2005 ) .
Others+Attention has mostly been limited to selectional preferences of verbs , which have been used for example for syntactic disambiguation ( Hindle and Rooth , 1993 ) , word sense disambiguation ( McCarthy and Carroll , 2003 ) and semantic role labeling ( Gildea and Jurafsky , 2002 ) .
Others+Several machine learning approaches for argument identification and classification have been developed ( Gildea and Jurasfky , 2002 ; Gildea and Palmer , 2002 ; Surdeanu et al. , 2003 ; Hacioglu et al. , 2003 ) .
Others+These standard features , firstly proposed in ( Gildea and Jurasfky , 2002 ) , refer to a flat information derived from parse trees , i.e. Phrase Type , Predicate Word , Head Word , Governing Category , Position and Voice .
Others+This latter choice allows us to compare the results with previous literature works , e.g. ( Gildea and Jurasfky , 2002 ; Surdeanu et al. , 2003 ; Hacioglu et al. , 2003 ) .
Others+In ( Gildea and Jurafsky , 2002 ) the authors presented a statistical approach to learning ( for FrameNet ) , with some success .
Others+Techniques for weakening the independence assumptions made by the IBM models 1 and 2 have been proposed in recent work ( Brown et al. , 1993 ; Berger et al. , 1996 ; Och and Weber , 98 ; Wang and Waibel , 98 ; Wu and Wong , 98 ) .
Others+The only trainable approaches ( known to the author ) to surface generation are the purely statistical machine translation ( MT ) systems such as ( Berger et al. , 1996 ) and the corpus - based generation system described in ( Langkilde and Knight , 1998 ) .
Others+The features used in NLG2 are described in the next section , and the feature weights a j , obtained from the Improved Iterative Scaling algorithm ( Berger et al. , 1996 ) , are set to maximize the likelihood of the training data .
Others+Because their joint distributions have such closed - form expressions , the parameters can be estimated directly from the training data without the need for an iterative fitting procedure ( as is required , for example , to estimate the parameters of maximum entropy models ; ( Berger et al. , 1996 ) ) .
Others+M a x i m u m Entropy models have been used to express the interactions among multiple feature variables ( e.g. , ( Berger et al. , 1996 ) ) , but within this framework no systematic study of interactions has been proposed .
Others+( Tae - il Kim , 2000 ) use up to ve phonemes in feature function ( Berger et al. , 1996 ) .
Others+Thus , a lot of alignment techniques have been suggested at the sentence ( Gale et al. , 1993 ) , phrase ( Shin et al. , 1996 ) , noun phrase ( Kupiec , 1993 ) , word ( Brown et al. , 1993 ; Berger et al. , 1996 ; Melamed , 1997 ) , collocation ( Smadja et al. , 1996 ) and terminology level .
Others+Recently , many approaches based on the maximum entropy model have been applied to natural language processing ( Berger et al. , 1994 ; Berger et al. , 1996 ; Pietra et al. , 1997 ) .
Others+We referred to the studies of ( Berger et al. , 1996 ; Pietra et al. , 1997 ) .
Others+For every class the weights of the active features are combined and the best scoring class is chosen ( Berger et al. , 1996 ) .
Others+We implemented these models within an maximum entropy framework ( Berger et al. , 1996 ; Ristad , 1997 ; Ristad , 1998 ) .
Others+We consider three learning algorithms , namely , the C4 .5 decision tree induction system ( Quinlan , 1993 ) , the RIPPER rule learning algorithm ( Cohen , 1995 ) , and maximum entropy classification ( Berger et al. , 1996 ) .
Others+As a model learning method , we adopt the maximum entropy model learning method ( Della Pietra et al. , 1997 ; Berger et al. , 1996 ) .
Others+Feature Selection by One - by - one Feature A d d i n g The feature selection process presented in Della Pietra et al. ( 1997 ) and Berger et al. ( 1996 ) is an incremental procedure that builds up S by successively adding features oneby - one .
Others+Berger et al. ( 1996 ) and Jelinek ( 1997 ) make this same point and arrive at the same estimator , albeit through a maximum entropy argument .
Others+Introduction Conditional Maximum Entropy models have been used for a variety of natural language tasks , including Language Modeling ( Rosenfeld , 1994 ) , partof - speech tagging , prepositional phrase attachment , and parsing ( Ratnaparkhi , 1998 ) , word selection for machine translation ( Berger et al. , 1996 ) , and finding sentence boundaries ( Reynar and Ratnaparkhi , 1997 ) .
Others+Berger et al. ( 1996 ) use an algorithm that might appear sequential , but an examination of the definition of f # and related work shows that it is not .
Others+A number of other researchers ( Berger et al. , 1996 ; Niessen and Ney , 2004 ; Xia and McCord , 2004 ) have described previous work on preprocessing methods .
Others+In previous work ( Foster , 2000 ) , I described a Maximum Entropy / Minimum Divergence ( MEMD ) model ( Berger et al. , 1996 ) for p ( w [ hi , s ) which incorporates a trigram language model and a translation component which is an analog of the well - known IBM translation model 1 ( Brown et al. , 1993 ) .
Others+For a given choice of q and f , the IIS algorithm ( Berger et al. , 1996 ) can be used to find m a x i m u m likelihood values for the parameters ~ .
Others+Under the maximum entropy framework ( Berger et al. , 1996 ) , evidence from different features can be combined with no assumptions of feature independence .
Others+However , our implementation uses a quasi-Newton gradient - climber BFGS for optimization , which has been shown to converge much faster ( Malouf , 2002 ; Sha and Pereira , 2003 ) .
Others+extensive comparisons among methods ( McDonald 2005 ; Sha & Pereira 2003 ; Kudo & Matsumoto 2001 ) .
Others+As for the task of shallow parsing , CRFs also outperform many other state - of - the - art models ( Sha & Pereira 2003 ; McDonald et al. 2005 ) .
Others+We employ similar predicate sets defined in Sha & Pereira ( 2003 ) .
Others+Since the CRF model is one of the successful models in sequential labeling tasks ( Lafferty et al. 2001 ; Sha & Pereira 2003 ; McDonald et al. 2005 ) , in this section , we also compare LDCRFs with CRFs .
Others+Since the CRF model is one of the successful models in sequential labeling tasks ( Lafferty et al. 2001 ; Sha & Pereira 2003 ; McDonald et al. 2005 ) , in this section , we also compare LDCRFs with CRFs .
Others+We observe that the L - BFGS optimizer is slightly faster than CG on LDCRFs ( see Figure 3 ) , which echoes the comparison between the L - BFGS and the CG optimizing technique on the CRF model ( Sha & Pereira 2003 ) .
Others+Although this non-concavity prevents efﬁcient global maximization of equation ( 3 ) , it still allows us to incorporate incomplete annotations using gradient ascent iterations ( Sha and Pereira , 2003 ) .
Others+CRFs have been successfully applied to many sequence labeling tasks ( Sha and Pereira , 2003 ; McCallum and Li , 2003 ) .
Others+To address this difficulty , we use the forward - backward algorithm ( Sha and Pereira , 2003 ; Culotta and McCallum , 2004 ) to estimate separately for each position the probability of a hyphen at that position .
Others+CRFs have been applied with impressive empirical results to the tasks of named entity recognition ( McCallum and Li , 2003 ; Cohn et al. , 2005 ) , part - of - speech ( PoS ) tagging ( Lafferty et al. , 2001 ) , noun phrase chunking ( Sha and Pereira , 2003 ) and extraction of table data ( Pinto et al. , 2003 ) , among other tasks .
Others+We built machine learners using a conventional Maximum Entropy ( ME ) learner ,2 as well as two structural learners , namely : ( 1 ) SVM - HMMs ( Joachims et al. , 2009 ) , as implemented in SVMstruct3 , with a linear kernel ; and ( 2 ) conditional random fields ( CRFs ) using CRF .4 SVMHMMs and CRFs have been successfully applied to a range of sequential tagging tasks such as syllabification ( Bartlett et al. , 2009 ) , chunk parsing ( Sha and Pereira , 2003 ) and word segmentation ( Zhao et al. , 2006 ) .
Others+Segment confidence is estimated using constrained forward - backward ( Culotta and McCallum , 2004 ) .
Others+It is calculated by constrained forwardbackward algorithm ( Culotta and McCallum , 2004 ) , and confident segments are added to the dictionary in order to improve segmentation accuracy .
Others+Culotta and McCallum ( 2004 ) describe the constrained forward - backward algorithm to efficiently estimate the conditional probability that a segment of text is correctly extracted by a CRF .
Others+To obtain the probability at each position of a linear - chain CRF , the constrained forward - backward technique described in ( Culotta and McCallum , 2004 ) is used .
Others+This is analogous to the task of estimating record confidence using field confidence scores in information extraction ( Culotta and McCallum , 2004 ) .
Others+To address this difficulty , we use the forward - backward algorithm ( Sha and Pereira , 2003 ; Culotta and McCallum , 2004 ) to estimate separately for each position the probability of a hyphen at that position .
Others+explored by many previous works ( see ( Caruana and Niculescu - Mizil , 2006 ) for a survey ) , and applied to several NL processing tasks such as syntactic parsing ( Reichart and Rappoport , 2007a ; Yates et al. , 2006 ) , machine translation ( Ueffing and Ney , 2007 ) , speech ( Koo et al. , 2001 ) , relation extraction ( Rosenfeld and Feldman , 2007 ) , IE ( Culotta and McCallum , 2004 ) , QA ( Chu - Carroll et al. , 2003 ) and dialog systems ( Lin and Weng , 2008 ) .
Others+Applications of this algorithm include k - best parsing ( McDonald et al. , 2005 ; Mohri and Roark , 2006 ) and machine translation ( Chiang , 2007 ) .
Others+In the present work , we exploit L1 - regularization , though other techniques such as structural zeros ( Mohri and Roark , 2006 ) could also potentially be used .
Others+It is interesting to note that these grammars capture many of the “ structural zeros ” described by Mohri and Roark ( 2006 ) and pruning rules with probability below e −10 reduces the grammar size drastically without influencing parsing performance .
Others+Although ( Ang et al. , 2002 ; Litman et al. , 2001 ; Batliner et al. , 2000 ) have hand - labeled naturally - occurring utterances in a variety of corpora for various emotions , then extracted acoustic , prosodic and lexical features and used machine - learning techniques to develop predictive
Others+Lexical information has been shown to improve speech - based emotion prediction in other domains ( Litman et al. , 2001 ; Lee et al. , 2002 ; Ang et al. , 2002 ; Batliner et al. , 2003 ; Devillers et al. , 2003 ; Shafran et al. , 2003 ) , so our first non-acoustic-prosodic feature represents the transcription3 of each student turn as a word occurrence vector ( indicating the lexical items that are present in the turn ) .
Others+Lexical information has been shown to improve speech - based emotion prediction in other domains ( Litman et al. , 2001 ; Lee et al. , 2002 ; Ang et al. , 2002 ; Batliner et al. , 2003 ; Devillers et al. , 2003 ; Shafran et al. , 2003 ) , so our first non-acoustic-prosodic feature represents the transcription3 of each student turn as a word occurrence vector ( indicating the lexical items that are present in the turn ) .
Others+In other papers ( Swerts et al. , 2000 ; Hirschberg et al. , 2001 ; Litman et al. , 2001 ) , we have already given some descriptive statistics on corrections and aware sites and we have been looking at methods to automatically predict these two utterance categories .
Others+In terms of distinguishing features which might explain or help to identify these turns , we have previously examined the acoustic and prosodic features of aware sites ( Litman et al. , 2001 ) .
Others+These experiments show that corrections and aware sites can be classi ed as such automatically , with a considerable degree of accuracy ( Litman et al. , 2001 ; Hirschberg et al. , 2001 ) .
Others+Moreover , speech contains prosodic and acoustic information which has been shown to improve the accuracy of predicting emotional states ( Ang et al. , 2002 ; Batliner et al. , 2000 ) and user responses to system errors ( Litman et al. , 2001 ) that are useful for triggering system adaptation .
Others+For more information on our tasks and features , see ( Litman et al. , 2000 ; Hirschberg et al. , 2001 ; Litman et al. , 2001 ) .
Extends+The Model In previous work using the PropBank corpus , ( Gildea and Palmer , 2002 ) proposed a model predicting argument roles using the same statistical method as the one employed by ( Gildea and Jurafsky , 2002 ) for predicting semantic roles based on the FrameNet corpus ( Baker et al. , 1998 ) .
Extends+To achieve high accuracy and resolve the data sparsity problem the method reported in ( Gildea and Palmer , 2002 ; Gildea and Jurafsky , 2002 ) employed a backoff solution based on a lattice that combines the model features .
Extends+Our model considers two sets of features : Feature Set 1 ( FS1 ) : features used in the work reported in ( Gildea and Palmer , 2002 ) and ( Gildea and Jurafsky , 2002 ) ; and Feature Set 2 ( FS2 ) : a novel set of features introduced in this paper .
Extends+The Model In previous work using the PropBank corpus , ( Gildea and Palmer , 2002 ) proposed a model predicting argument roles using the same statistical method as the one employed by ( Gildea and Jurafsky , 2002 ) for predicting semantic roles based on the FrameNet corpus ( Baker et al. , 1998 ) .
Others+There is a large body of work on classifying the polarity of a document ( e.g. , Pang et al. ( 2002 ) , Turney ( 2002 ) ) , a sentence ( e.g. , Liu et al. ( 2003 ) , Yu and Hatzivassiloglou ( 2003 ) , Kim and Hovy ( 2004 ) , Gamon et al. ( 2005 ) ) , a phrase ( e.g. , Wilson et al. ( 2005 ) ) , and a specific object ( such as a product ) mentioned in a document ( e.g. , Morinaga et al. ( 2002 ) , Yi et al. ( 2003 ) , Popescu and Etzioni ( 2005 ) ) .
Others+There is a large body of work on classifying the polarity of a document ( e.g. , Pang et al. ( 2002 ) , Turney ( 2002 ) ) , a sentence ( e.g. , Liu et al. ( 2003 ) , Yu and Hatzivassiloglou ( 2003 ) , Kim and Hovy ( 2004 ) , Gamon et al. ( 2005 ) ) , a phrase ( e.g. , Wilson et al. ( 2005 ) ) , and a specific object ( such as a product ) mentioned in a document ( e.g. , Morinaga et al. ( 2002 ) , Yi et al. ( 2003 ) , Popescu and Etzioni ( 2005 ) ) .
Others+While we do not have a direct comparison , we note that Turney ( 2002 ) performs worse on movie reviews than on his other datasets , the same type of data as the polarity dataset .
Others+The work most similar in spirit to ours that of Turney ( 2002 ) .
Others+Similarly for summarization , systems that have employed language models trained only on unsummarized text ( Banko et al. , 2000 ; Daume and Marcu , 2002 ) .
Others+This basic kind of verb knowledge has been shown to be useful in many NLP tasks such as information extraction ( Surdeanu et al. , 2003 ; Venturi1 et al. , 2009 ) , parsing ( Carroll et al. , 1998 ; Carroll and Fang , 2004 ) and word sense disambiguation ( Kohomban and Lee , 2005 ; McCarthy et al. , 2007 ) .
Others+Lapata ( Lapata , 2003 ) proposes another approach to information ordering based on a probabilistic model that assumes the probability of any given sentence is determined by its adjacent sentence and learns constraints on sentence order from a corpus of domain specific texts .
Others+Features proposed to create the appropriate order include publication date of document ( Barzilay et al. , 2002 ) , content words ( Lapata , 2003 ; Althaus et al. , 2004 ) , and syntactic role of
Others+Lapata ( 2003 ) proposed most of these features .
Others+A simple ordering criterion is the chronological order of the events represented in the sentences , which is often augmented with other ordering criteria such as lexical overlap ( Conroy et al. , 2006 ) , lexical cohesion ( Barzilay et al. , 2002 ) or syntactic features ( Lapata 2003 ) .
Others+Text Structuring The insertion task is closely related to the extensively studied problem of sentence ordering .3 Most of the existing algorithms represent text structure as a linear sequence and are driven by local coherence constraints ( Lapata , 2003 ; Karamanis et al. , 2004 ; Okazaki et al. , 2004 ; Barzilay and Lapata , 2005 ; Bollegala et al. , 2006 ; Elsner and Charniak , 2007 ) .
Others+To identify the exact location of the sentence within the chosen paragraph , local ordering methods such as ( Lapata , 2003 ) could be used .
Others+Features used in our experiments are inspired by previous work on corpus - based approaches for discourse analysis ( Marcu and Echihabi , 2002 ; Lapata , 2003 ; Elsner et al. , 2007 ) .
Others+This measure was first introduced in the context of sentence ordering by Lapata ( 2003 ) .
Others+Features used in our experiments are inspired by previous work on corpus - based approaches for discourse analysis ( Marcu and Echihabi , 2002 ; Lapata , 2003 ; Elsner et al. , 2007 ) .
Others+In accordance with recent work in the emerging field of text - to - text generation ( Barzilay et al. , 2002 ; Lapata , 2003 ) , we assume that the input to text structuring is a set of clauses .
Others+For works taking no use of source document , Lapata ( 2003 ) proposed a probabilistic model which learns constraints on sentence ordering from a corpus of texts .
Others+The probability model originates from ( Lapata , 2003 ) , and we implement the model with four features of lemmatized noun , verb , adjective or adverb , and verb and noun related dependency .
Others+The two features of noun probability and dependency probability play an important role as demonstrated in ( Lapata , 2003 ) .
Others+As to analysis of NPs , there have been a lot of work on statistical techniques for lexical dependency parsing of sentences ( Collins and Roark , 2004 ; McDonald et al. , 2005 ) , and these techniques potentially can be used for analysis of NPs if appropriate resources for NPs are available .
Others+We also view this as one of the key ideas of the incremental perceptron algorithm of ( Collins and Roark , 2004 ) , which searches through a complex decision space step - by - step and is immediately updated at the first wrong move .
Others+Online learning algorithms have been shown to be robust even with approximate rather than exact inference in problems such as word alignment ( Moore , 2005 ) , sequence analysis ( Daum ´ e and Marcu , 2005 ; McDonald et al. , 2005a ) and phrase - structure parsing ( Collins and Roark , 2004 ) .
Others+In particular , most of the work on parsing with kernel methods has focussed on kernels over parse trees ( Collins and Duffy , 2002 ; Shen and Joshi , 2003 ; Shen et al. , 2003 ; Collins and Roark , 2004 ) .
Others+To tune the parameters w of the model , we use the averaged perceptron algorithm ( Collins , 2002 ) because of its efficiency and past success on various NLP tasks ( Collins and Roark , 2004 ; Roark et al. , 2004 ) .
Others+Following Collins and Roark ( 2004 ) , we apply the “ early update ” strategy to perceptron training : at any step during decoding , if neither the candidate output nor any item in the agenda is correct , decoding is stopped and the parameters are updated using the current highest scored item in the agenda or the candidate output , whichever has the higher score .
Others+Another alternative for future work is to compare the dynamic programming approach taken here with the beam - search approach of Collins and Roark ( 2004 ) , which allows more “ global ” features .
Others+From prior work ( Zelenko et al. , 2003 ; Culotta and Sorensen , 2004 ; Bunescu and Mooney , 2005 ) to current research ( Zhang et al. , 2006 ; Zhou et al. , 2007 ) , kernel methods have been showing more and more potential in relation extraction .
Others+Culotta and Sorensen ( 2004 ) proposed a slightly generalized version of this kernel between dependency trees , in which a successful match of two relation instances requires the nodes to be at the same layer and in the identical path starting from the roots to the current nodes .
Others+This dependency relationship offers a very condensed representation of the information needed to assess the relationship in the forms of the dependency tree ( Culotta and Sorensen , 2004 ) or the shortest dependency path ( Bunescu and Mooney , 2005 ) that includes both entities .
Others+Culotta and Sorensen ( 2004 ) extended this work to estimate similarity between augmented dependency trees .
Others+During the last years , many authors have focused on resolving TE detection , as solutions to this problem have proved to be useful in many natural language processing tasks , such as question answering ( Harabagiu and Hickl , 2006 ) or machine translation ( MT ) ( Mirkin et al. , 2009 ; Pad ´ o et al. , 2009 ) .
Others+TE has been successfully applied to a variety of natural language processing applications , including information extraction ( Romano et al. , 2006 ) and question answering ( Harabagiu and Hickl , 2006 ) .
Others+Extensive research has been done in questionanswering , e.g. ( Berger et al. , 2000 ; Jeon et al. , 2005 ; Cui et al. , 2005 ; Harabagiu and Hickl , 2006 ; Dang et al. , 2007 ) .
Others+Blitzer et al. ( 2007 ) investigate domain adaptation for sentiment classifiers using structural correspondence learning .
Others+We selected four binary NLP datasets for evaluation : 20 Newsgroups1 and Reuters ( Lewis et al. , 2004 ) ( used by Tong and Koller ) and sentiment classification ( Blitzer et al. , 2007 ) and spam ( Bickel , 2006 ) .
Others+It is worth noting that Blitzer et al. ( 2007 ) deal with the domain adaptation problem for sentiment classification where labeled data from one domain is used to train a classifier for classifying data from a different domain .
Others+As the training data from DVDs is much more similar to books than that from kitchen ( Blitzer et al. , 2007 ) , we should give the data from DVDs a higher weight .
Others+In addition , such an approach relies heavily on the availability of accurately parsed sentences , which could be problematic for domains such as biomedical texts ( Clegg and Shepherd , 2007 ; McClosky and Charniak , 2008 ) .
Others+Following previous work ( Mintz et al. , 2009 ; Zelenko et al. , 2003 ; Culotta and Sorensen , 2004 ) we make one more simplifying assumption : every candidate tuple can be member of at most one relation .
Others+We will follow ( Mintz et al. , 2009 ) and call the term R ( c1 , . . . cn ) with c ∈ R a relation instance .3 It denotes the membership of the tuple c in the relation R .
Others+We also heuristically align our knowledge base to text by making the distant supervision assumption ( Bunescu and Mooney , 2007 ; Mintz et al. , 2009 ) .
Others+Mintz et al. ( 2009 ) refer to this as the distant supervision assumption .
Others+On inspection , we find that these preferences are often not satisfied in a baseline distant supervision system akin to Mintz et al. ( 2009 ) .
Others+The feature functions of this template are taken from Mintz et al. ( 2009 ) .
Others+For each candidate tuple c with arity 2 and each of its mention tuples i we extract a set of features Xic similar to those used in ( Mintz et al. , 2009 ) : lexical , Part - Of - Speech ( POS ) , named entity and syntactic features , i.e. features obtained from the dependency parsing tree of a sentence .
Others+For each candidate tuple c with arity 2 and each of its mention tuples i we extract a set of features Xic similar to those used in ( Mintz et al. , 2009 ) : lexical , Part - Of - Speech ( POS ) , named entity and syntactic features , i.e. features obtained from the dependency parsing tree of a sentence .
Others+Our work is inspired by Mintz et al. ( 2009 ) who also use Freebase as distant supervision source .
Extends+A particularly attractive approach , called distant supervision ( DS ) , creates labeled data by heuristically aligning entities in text with those in a knowledge base , such as Freebase ( Mintz et al. , 2009 ) .
Extends+using Freebase as a knowledge base and found that ( i ) our model identified patterns expressing a given relation more accurately than baseline methods and ( ii ) our method led to better extraction performance than the original DS ( Mintz et al. , 2009 ) and MultiR ( Hoffmann et al. , 2011 ) , which is a state - of - the - art multiinstance learning system for relation extraction ( see Section 7 ) .
Extends+Following Mintz et al. ( 2009 ) , we carried out our experiments using Wikipedia as the target corpus and Freebase ( September , 2009 , ( Google , 2009 ) ) as the knowledge base .
Extends+Following Mintz et al. ( 2009 ) , we performed an automatic held - out evaluation and a manual evaluation .
Extends+We used syntactic features ( i.e. , features obtained from the dependency parse tree of a sentence ) and lexical features , and entity types , which essentially correspond to the ones developed by Mintz et al. ( 2009 ) .
Others+Here one aligns existing database records with the sentences in which these records have been “ rendered ” – – effectively labeling the text — and from this labeling we can train a machine learning system as before ( Craven and Kumlien , 1999 ; Mintz et al. , 2009 ; Bunescu and Mooney , 2007 ; Riedel et al. , 2010 ) .
Others+Distant Supervision In Distant Supervision ( DS ) a set of facts from pre-existing structured sources is aligned with surface patterns mentioned in text ( Bunescu and Mooney , 2007 ; Mintz et al. , 2009 ; Riedel et al. , 2010 ; Hoffmann et al. , 2011 ; Surdeanu et al. , 2012 ) , and this alignment is then used to train a relation extractor .
Others+Our first baseline is MI09 , a distantly supervised classifier based on the work of Mintz et al. ( 2009 ) .
Extends+Current work has been spurred by two papers , ( Yarowsky , 1995 ) and ( Blum and Mitchell , 1998 ) .
Extends+( Yarowsky , 1995 ) , citing ( Yarowsky , 1994 ) , actually uses a superficially different score that is , however , a monotone transform of precision , hence equivalent to precision , since it is used only for sorting .
Extends+The Yarowsky ( 1995 ) algorithm was one of the first bootstrapping algorithms to become widely known in computational linguistics .
Others+Yarowsky ( 1995 ) describes a &apos; semi-unsupervised &apos; approach to the problem of sense disambiguation of words , also using a set of initial seeds , in this case a few high quality sense annotations .
Others+Early work by Yarowsky ( 1995 ) falls within this framework .
Others+Each decision rule in a decision list is sorted TOur modeling is slightly different from those of other standard approaches to statistical dependency analysis ( Collins , 1996 ; Fujio and Matsumoto , 1998 ; Haruno et al. , 1998 ) which simply distinguish the two cases : the case where dependency relation holds between the given two vp chunks or clauses , and the case where dependency relation does not hold .
Others+Each decision rule in a decision list is sorted TOur modeling is slightly different from those of other standard approaches to statistical dependency analysis ( Collins , 1996 ; Fujio and Matsumoto , 1998 ; Haruno et al. , 1998 ) which simply distinguish the two cases : the case where dependency relation holds between the given two vp chunks or clauses , and the case where dependency relation does not hold .
Others+Many freely available natural language processing tools require their input to be divided into sentences , but make no mention of how to accomplish this ( e.g. ( Brill , 1994 ; Collins , 1996 ) ) .
Others+Our investigation of these models was motivated rather by our desire to obtain a generalizable result for these simple and well - understood models , since obtaining similar results for more sophisticated models ( e.g. ( Collins , 1996 ; Ratnaparkhi , 1997 ) ) might have been attributed to special properties of these models .
Others+Instead of producing only a phrasal parse for the question and answer , we make use of one of the new statistical parsers of large real - world text coverage ( Collins , 1996 ) .
Others+The label propagation rules are identical to the rules for mapping from trees to dependency structures used my Michael Collins ( cfXXX ( Collins , 1996 ) ) .
Others+Note that , while we restrict our discussion to analysis of Japanese sentences in this paper , what we present below should also be straightforwardly applicable to more wideranged tasks such as English dependency analysis just like the problem setting considered by Collins ( 1996 ) .
Others+( Charniak , 1995 ; Collins , 1996 ) use the lexical information This research was partially supported by KOSEF special basic research program ( 1997.9 2000.8 ) .
Others+The features are extracted using a statistical parser ( Collins , 1996 ) , and consist of the head and modifiers of each phrase .
Others+A typical current parser ( e.g. , statistical parsers such as ( Collins , 1996 ; Ratnaparkhi , 1997 ; Charniak , 2000 ) ) interleaves PP attachment with all its other disambiguation tasks .
Others+With the emergence of the important role of word - to - word relations in parsing ( Charniak , 2000 ; Collins , 1996 ) , dependency grammars have gained a certain popularity ; e.g. , Yamada and Matsumoto ( 2003 ) for English , Kudo and Matsumoto ( 2000 ; 2002 ) , Sekine et al. ( 2000 ) for Japanese , Chung and Rim ( 2004 ) for Korean , Nivre et al. ( 2004 ) for Swedish , Nivre and Nilsson ( 2005 ) for Czech , among others .
Others+Collins ( 1996 ) employs this distance ∆ i , H ( i ) in the computation of word - toword dependency probabilities
Others+Collins ( 1996 ) employs this distance ∆ i , H ( i ) in the computation of word - toword dependency probabilities
Others+To calculate the edge weights , we adapt the definition of Collins ( 1996 ) to use direction rather than relation type ( represented in the original as triples of non-terminals ) .
Others+We adopt the same smoothing strategy as Collins ( 1996 ) , which backs off to PoS for unseen dependency events .
Others+There have been two main robust parsing paradigms : Finite State Grammar - based approaches ( such as Abney ( 1990 ) , Grishman ( 1995 ) , and Hobbs et al. ( 1997 ) ) and Statistical Parsing ( such as Charniak ( 1996 ) , Magerman ( 1995 ) , and Collins ( 1996 ) ) .
Others+Volume 34 , Number 3 Maximum likelihood estimation to make inferences about the underlying probability models ( Collins 1996 ; Chung and Rim 2004 )
Others+In all of the following models , dist ( i , H ( i ) ) is taken as the number of actual word boundaries between the dependent and the head unit regardless of whether full words or IGs were used as units of parsing .9 To alleviate the data sparseness , we use the interpolation of other estimates while calculating the probabilities in Equation ( 2 ) .10 We use a strategy similar to Collins ( 1996 ) and we interpolate with estimates based on less context :
Others+In all of the following models , dist ( i , H ( i ) ) is taken as the number of actual word boundaries between the dependent and the head unit regardless of whether full words or IGs were used as units of parsing .9 To alleviate the data sparseness , we use the interpolation of other estimates while calculating the probabilities in Equation ( 2 ) .10 We use a strategy similar to Collins ( 1996 ) and we interpolate with estimates based on less context :
Extends+One of the most straightforward projective dependency parsing strategies was introduced by Collins ( 1996 ) , and is based on the CYK bottom – up parsing strategy ( Kasami 1965 ; Younger 1967 ) .
Extends+C OMBINER steps follow the same mechanism as those in the algorithm of Eisner ( 1996 ) , and L INK steps work analogously to those of Collins ( 1996 ) , so this schema can be seen as being intermediate between those two algorithms .
Extends+By generalizing the linking steps in Eisner and Satta ’s parser so that the head of each item can be in any position , we obtain an O ( n5 ) parser which can be ﬁltered into the parser of Collins ( 1996 ) by eliminating the C OMBINER steps .
Others+In SIFT ’s statistical model , augmented parse trees are generated according to a process similar to that described in Collins ( 1996 , 1997 ) .
Others+This was done trying to overcome the limitations of lexicalized approaches to parsing ( Magerman , 1995 ; Collins , 1996 ; Charniak , 1997 ; Collins , 2003 ) , where related words , like scissors and knife can not be generalized .
Others+Examples of formalisms using this approach include the work of Magerman ( 1995 ) , Charniak ( 1997 ) , Collins ( 1997 ) , and Goodman ( 1997 ) .
Others+We will say that the elements of Supp ( 1 ) are c o v e r e d by I. Introduction Dependency grammar has a long tradition in syntactic theory , dating back to at least Tesni ~ re 's work from the thirties3 Recently , it has gained renewed attention as empirical methods in parsing are discovering the importance of relations between words ( see , e.g. , ( Collins , 1997 ) ) , which is what dependency grammars model explicitly do , but context - free phrasestructure grammars do not .
Others+Recently some researchers have pointed out the importance of the lexicon and proposed lexicalized models ( Jelinek et al. , 1994 ; Collins , 1997 ) .
Others+Several recent real - world parsers have improved state - of - the - art parsing accuracy by relying on probabilistic or weighted versions of bilexical grammars ( Alshawi , 1996 ; Eisner , 1996 ; Charniak , 1997 ; Collins , 1997 ) .
Others+We want to extract from the Penn Treebank an LTAG whose derivations mirror the dependency analysis implicit in the head - percolation rules of ( Magerman , 1995 ; Collins , 1997 ) .
Others+We want to extract from the Penn Treebank an LTAG whose derivations mirror the dependency analysis implicit in the head - percolation rules of ( Magerman , 1995 ; Collins , 1997 ) .
Others+Introduction In the field of statistical parsing , various probabilistic evaluation models have been proposed where different models use different feature types [ Black , 1992 ] [ Briscoe , 1993 ] [ Brown , 1991 ] [ Charniak , 1997 ] [ Collins , 1996 ] [ Collins , 1997 ] [ Magerman , 1991 ] [ Magerman , 1992 ] [ Magerman , 1995 ] [ Eisner , 1996 ] .
Others+Many probabilistic evaluation models have been published inspired by one or more of these feature types [ Black , 1992 ] [ Briscoe , 1993 ] [ Charniak , 1997 ] [ Collins , 1996 ] [ Collins , 1997 ] [ Magerman , 1995 ] [ Eisner , 1996 ] , but discrepancies between training sets , algorithms , and hardware environments make it difficult , if not impossible , to compare the models objectively .
Others+The system is trained by rst using the Collins parser ( Collins , 1997 ) to parse the 36,995 training sentences , matching annotated frame elements to parse constituents , and extracting various features from the string of words and the parse tree .
Others+( Collins ( 1997 ) discusses the recovery of one kind of empty node , viz. , WH - traces ) .
Others+In this paper , we examine how the information provided by modern statistical parsers such as Collins ( 1997 ) and Charniak ( 1997 ) contributes to solving this problem .
Others+In previous work using the FrameNet corpus , Gildea and Jurafsky ( 2002 ) developed a system to predict semantic roles from sentences and their parse trees as determined by the statistical parser of Collins ( 1997 ) .
Others+Like the models of Goodman ( 1997 ) , the additional features in our model are generated probabilistically , whereas in the parser of Collins ( 1997 ) distance measures are assumed to be a function of the already generated structure and are not generated explicitly .
Others+Like the models of Goodman ( 1997 ) , the additional features in our model are generated probabilistically , whereas in the parser of Collins ( 1997 ) distance measures are assumed to be a function of the already generated structure and are not generated explicitly .
Others+This statistical technique of labeling predicate argument operates on the output of the probabilistic parser reported in ( Collins , 1997 ) .
Extends+Section 3 describes two standard lexicalized models ( Carroll and Rooth , 1998 ; Collins , 1997 ) , as well as an unlexicalized baseline model .
Extends+Lexicalization has been shown to improve parsing performance for the Penn Treebank ( e.g. , Carroll and Rooth 1998 ; Charniak 1997 , 2000 ; Collins 1997 ) .
Extends+In contrast to Carroll and Rooth ’s ( 1998 ) approach , the model proposed by Collins ( 1997 ) does not compute rule probabilities directly .
Extends+In contrast to Carroll and Rooth ’s ( 1998 ) approach , the model proposed by Collins ( 1997 ) does not compute rule probabilities directly .
Extends+compared to the models of Carroll and Rooth ( 1998 ) , Collins ( 1997 ) , and Charniak ( 2000 ) Negra , based on Collins ’s ( 1997 ) model for nonrecursive NPs in the Penn Treebank ( which are also flat ) .
Extends+Charniak , 1997 , 2000 ; Collins , 1997 ) , and the lexicalized model proposed by Collins ( 1997 ) has been successfully applied to Czech ( Collins et al. , 1999 ) and Chinese ( Bikel and Chiang , 2000 ) .
Others+However , such constructions prove to be difficult for stochastic parsers ( Collins et al. , 1999 ) and they either avoid tackling the problem ( Charniak , 2000 ; Bod , 2003 ) or only deal with a subset of the problematic cases ( Collins , 1997 ) .
Others+controlled NP - traces ( NP – NP ) , we follow the standard technique of marking nodes dominating the empty element up to but not including the parent of the antecedent as defective ( missing an argument ) with a gap feature ( Gazdar et al. , 1985 ; Collins , 1997 ) .1 Furthermore , to make antecedent co-indexation possible with many types of EE s , we generalize Collins ’ approach by enriching the annotation of non-terminals with the type of the EE in question ( eg. .
Others+The idea of threading EE s to their antecedents in a stochastic parser was proposed by Collins ( 1997 ) , following the GPSG tradition ( Gazdar et al. , 1985 ) .
Others+Previous approaches to the problem ( Collins , 1997 ; Johnson , 2002 ; Dienes and Dubey , 2003a , b ; Higgins , 2003 ) have all been learning - based ; the primary difference between the present algorithm and earlier ones is that it is not learned , but explicitly incorporates principles of GovernmentBinding theory ( Chomsky , 1981 ) , since that theory underlies the annotation .
Others+We employ a robust statistical parser ( Collins , 1997 ) to determine the constituent structure for each sentence , from which subjects ( s ) , objects ( o ) , and relations other than subject or object ( x ) are identified .
Extends+Another area for future work is to empirically compare PMI - IR and the algorithm of Hatzivassiloglou and McKeown ( 1997 ) .
Extends+Hatzivassiloglou and McKeown ( 1997 ) use a four - step supervised learning algorithm to infer the semantic orientation of adjectives from constraints
Extends+Another area for future work is to empirically compare PMI - IR and the algorithm of Hatzivassiloglou and McKeown ( 1997 ) .
Extends+As an example , they present the following three sentences ( Hatzivassiloglou & McKeown , 1997 ) :
Extends+As an example , they present the following three sentences ( Hatzivassiloglou & McKeown , 1997 ) :
Extends+better than the WSJ model for this task , but combining the two training corpora results in a better model ( as in Gildea ( 2001 ) ) .
Others+The accuracy presented so far for the biomediCross - domain speed improvement When applying parsers out of domain they are typically slower and less accurate ( Gildea , 2001 ) .
Others+Collins ( 2002 ) adapted the perceptron learning algorithm to tagging tasks , via sentence - based global feedback .
Others+Here we used the averaged perceptron ( Collins , 2002 ) , where the weight matrix used to classify the test data is the average of all of
Extends+We adopt the basic feature set used in ( Ratnaparkhi , 1996 ) and ( Collins , 2002 ) .
Extends+parameter estimation of θ , we use the averaged perceptron as described in ( Collins , 2002 ) .
Others+There is also a preliminary implementation of online learning methods such as the structured perceptron algorithm ( Collins , 2002 ) , and regularized structured SVMs trained using FOBOS ( Duchi and Singer , 2009 ) .
Others+We extracted tagged sentences from the parse trees .5 We split the data into training , development , and test sets as in ( Collins , 2002 ) .
Others+Whereas Ratnaparkhi ( 1996 ) used feature support cutoffs and early stopping to stop overfitting of the model , and Collins ( 2002 ) contends that including low support features harms a maximum entropy model , our results show that low support features are useful in a regularized maximum entropy model .
Others+It is usually modeled as a sequence model , often in combination with part - of - speech tagging and lemmatization ( Collins , 2002 ; Hajiˇc , 2004 ; Smith et al. , 2005 ; Chrupała et al. , 2008 , and others ) .
Others+We use the common method of setting the final weight vector as the average of the weight vectors after each iteration ( Collins , 2002 ) , which has been shown to alleviate overfitting .
Others+This algorithm can thus be viewed as a large - margin version of the perceptron algorithm for structured outputs Collins ( 2002 ) .
Others+Conditional Random Fields have been applied to NLP tasks such as parsing ( Ratnaparkhi et al. , 1994 ; Johnson et al. , 1999 ) , and tagging or segmentation tasks ( Lafferty et al. , 2001 ; Sha and Pereira , 2003 ; McCallum and Li , 2003 ; Pinto et al. , 2003 ) .
Others+CRFs have been applied with impressive empirical results to the tasks of named entity recognition ( McCallum and Li , 2003 ) , simplified part - of - speech ( POS ) tagging ( Lafferty et al. , 2001 ) , noun phrase chunking ( Sha and Pereira , 2003 ) and extraction of tabular data ( Pinto et al. , 2003 ) , among other tasks .
Others+Introduction In recent years , conditional random fields ( CRFs ) ( Lafferty et al. , 2001 ) have shown success on a number of natural language processing ( NLP ) tasks , including shallow parsing ( Sha and Pereira , 2003 ) , named entity recognition ( McCallum and Li , 2003 ) and information extraction from research papers ( Peng and McCallum , 2004 ) .
Others+The modeling power of CRFs has been of great benefit in several applications , such as shallow parsing ( Sha and Pereira , 2003 ) and information extraction ( McCallum and Li , 2003 ) .
Others+However , MCE - F showed the better performance of 85.29 compared with ( McCallum and Li , 2003 ) of 84.04 , which used the MAP training of CRFs with a feature selection architecture , yielding similar results to the MAP results described here .
Others+These belong to two main categories based on machine learning ( Bikel et al. , 1997 ; Borthwick , 1999 ; McCallum and Li , 2003 ) and language or domain specific rules ( Grishman , 1995 ; Wakao et al. , 1996 ) .
Others+( See Section 3.4 for more about CRFs ) Linear CRF model has been successfully applied in NLP and text mining tasks ( McCallum and Li , 2003 ; Sha and Pereira , 2003 ) .
Others+Current studies of NER mainly focus on formal text such as news articles ( Mccallum and Li , 2003 ; Etzioni et al. , 2005 ) .
Others+Empirical successes with CRFs have been reported recently in part - of - speech tagging ( Lafferty et al. , 2001 ) , shallow parsing ( Sha and Pereira , 2003 ) , named entity recognition ( McCallum and Li , 2003 ) , Chinese word segmentation ( Peng et al. , 2004 ) , and Information Extraction ( Pinto et al. , 2003 ; Peng and McCallum , 2004 ) .
Others+Note that our formulation of CRFs is different from the widely - used formulations ( e.g. , ( Sha and Pereira , 2003 ; McCallum and Li , 2003 ; Peng et al. , 2004 ; Pinto et al. , 2003 ; Peng and McCallum , 2004 ) ) .
Others+While global statistical approaches , such as sequential averaged perceptrons or CRFs ( McCallum and Li , 2003 ) , appear better suited to the NER problem than local symbolic learners , the two approaches search different hypothesis spaces .
Others+Here we adapt self - training , a simple technique that leverages a supervised learner ( like the perceptron ) to perform semisupervised learning ( Clark et al. , 2003 ; Mihalcea , 2004 ; McClosky et al. , 2006 ) .
Others+Self - training ( Clark et al. , 2003 ; Mihalcea , 2004 ; McClosky et al. , 2006 ) is widely used in NLP and has inspired related techniques that learn from automatically labeled data ( Liang et al. , 2008 ; Petrov et al. , 2010 ) .
Extends+The most clearly relevant study is Light et al. ( 2004 ) where the focus is on introducing the problem , exploring annotation issues and outlining potential applications rather than on the specificities of the ML approach , though they do present some results using a manually crafted substring matching classifier and a supervised SVM on a collection of Medline abstracts .
Extends+To further elucidate the nature of the task and improve annotation consistency , we have developed a new set of guidelines , building on the work of Light et al. ( 2004 ) .
Extends+denotes our probabilistic learning model and classifier ( § 9 ) denotes probabilistic learning model with SVM classifier denotes committee - based model ( § 10.4 ) with probabilistic classifier denotes committee - based model with SVM classifier denotes substring matching classifier of ( Light et al. , 2004 ) An important issue in incremental learning scenarios is identification of the optimum stopping point .
Others+We have achieved average results in the CoNLL domain adaptation track open submission ( Marcus et al. , 1993 ; Johansson and Nugues , 2007 ; Kulick et al. , 2004 ; MacWhinney , 2000 ; Brown , 1973 ) .
Others+For the second adaptation task we were given a large collection of unlabeled data in the chemistry domain ( Kulick et al , 2004 ) as well as a test set of 5000 tokens ( 200 sentences ) to parse ( english_pchemtbtb_test .
Others+In the biomedical domain , for example , several annotated corpora such as GENIA ( Kim et al. , 2003 ) , PennBioIE ( Kulick et al. , 2004 ) , and GENETAG ( Tanabe et al. , 2005 ) have been created and made publicly available , but the named entity categories annotated in these corpora are tailored to their specific needs and not always sufficient or suitable for text mining tasks that other researchers need to address .
Others+The Pennsylvania biology corpus ( Kulick et al. , 2004 ) partially solves this problem by separating a token where two or more subtokens are connected with hyphens , but in the cases where a shared part of the word is not separated by a hyphen ( e.g. ‘ metric ’ of ‘ stereoand isometric alleles ’ ) the word including the part is left uncut .
Extends+respondence learning ( SCL ) domain adaptation algorithm ( Blitzer et al. , 2006 ) for use in sentiment classification .
Extends+Then , it models the correlations between the pivot features and all other features by training linear pivot predictors to predict occurrences of each pivot in the unlabeled data from both domains ( Ando and Zhang , 2005 ; Blitzer et al. , 2006 ) .
Extends+Ando and Zhang ( 2005 ) and Blitzer et al. ( 2006 ) suggest λ = 10 −4 , µ = 0 , which we have used in our results so far .
Extends+As we noted in Section 5 , we are able to significantly outperform basic structural correspondence learning ( Blitzer et al. , 2006 ) .
Extends+First , we showed that for a given source and target domain , we can significantly improve for sentiment classification the structural correspondence learning model of Blitzer et al. ( 2006 ) .
Others+Most RTE systems are based on advanced NLP components , machine learning techniques , and / or syntactic transformations ( Zanzotto et al. , 2007 ; Kouleykov and Magnini , 2005 ) .
Others+Most entailment systems function as weak proof theory ( Hickl et al. , 2006 ; MacCartney et al. , 2006 ; Zanzotto et al. , 2007 ) , but contradictions require deeper inferences and model building .
Others+and hypothesis and base at least part of their decision on properties of this alignment ( Burchardt et al. , 2007 ; Hickl and Bensley , 2007 ; Iftene and Balahur - Dobrescu , 2007 ; Zanzotto et al. , 2007 ) .
Others+Systems addressing TE exploiting machine learning techniques with a variety of features , including lexical - syntactic and semantic features ( e.g. Kozareva and Montoyo ( 2006 ) , Zanzotto et al. ( 2007 ) ) tend towards the opposite extreme of this framework , since even if linguistic features are used , they bring information about a specific aspect relevant to the inference task but they do not provide an independent judgment on it .
Others+Recent comparisons of approaches that can be trained on corpora ( van Halteren et al. , 1998 ; Volk and Schneider , 1998 ) have shown that in most cases statistical aproaches ( Cutting et al. , 1992 ; Schmid , 1995 ; Ratnaparkhi , 1996 ) yield better results than finite - state , rule - based , or memory - based taggers ( Brill , 1993 ; Daelemans et al. , 1996 ) .
Others+The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in ( Ratnaparkhi , 1996 ) .
Others+According to current tagger comparisons ( van Halteren et al. , 1998 ; Zavrel and Daelemans , 1999 ) , and according to a comparsion of the results presented here with those in ( Ratnaparkhi , 1996 ) , the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here .
Others+widely described in the recent past , starting with the ( Church , 1988 ) paper , followed by numerous others using various methods : neural networks ( Julian Benello and Anderson , 1989 ) , H M M tagging ( Merialdo , 1992 ) , decision trees ( Schmid , 1994 ) , transformation - based error - driven learning ( Brill , 1995 ) , and m a x i m u m entropy ( Ratnaparkhi , 1996 ) , to selectjust a few .
Others+widely described in the recent past , starting with the ( Church , 1988 ) paper , followed by numerous others using various methods : neural networks ( Julian Benello and Anderson , 1989 ) , H M M tagging ( Merialdo , 1992 ) , decision trees ( Schmid , 1994 ) , transformation - based error - driven learning ( Brill , 1995 ) , and m a x i m u m entropy ( Ratnaparkhi , 1996 ) , to selectjust a few .
Others+We used a publicly available tagger ( Ratnaparkhi , 1996 ) to tag the words and then used these in the input to the system .
Others+Support Vector Machines ( SVMs ) ( Vapnik , 1995 ) and Maximum Entropy ( ME ) method ( Berger et al. , 1996 ) are powerful learning methods that satisfy such requirements , and are applied successfully to other NLP tasks ( Kudo and Matsumoto , 2000 ; Nakagawa et al. , 2001 ; Ratnaparkhi , 1996 ) .
Others+We use the maximum entropy tagging method described in ( Kazama et al. , 2001 ) for the experiments , which is a variant of ( Ratnaparkhi , 1996 ) modified to use HMM state features .
Others+Much research has been done to improve tagging accuracy using several different models and methods , including : hidden Markov models ( HMMs ) ( Kupiec , 1992 ) , ( Charniak et al. , 1993 ) ; rule - based systems ( Brill , 1994 ) , ( Brill , 1995 ) ; memory - based systems ( Daelemans et al. , 1996 ) ; maximum - entropy systems ( Ratnaparkhi , 1996 ) ; path voting constraint systems ( Tiir and Oflazer , 1998 ) ; linear separator systems ( Roth and Zelenko , 1998 ) ; and majority voting systems ( van Halteren et al. , 1998 ) .
Extends+Barzilay and Elhadad ( 1997 ) dealt with some of tile limitations in Hirst and St - Onge &apos;s algorithm by examining every possible lexical chain which could be computed , not just those possible at a given point in the text .
Extends+As mentioned above , this research is based on the work of Barzilay and Elhadad ( 1997 ) on lexical chains .
Others+Previous approaches include supervised learning ( Teufel and Moens , 1997 ) , vectorial similarity computed between an initial abstract and sentences in the given document , or intra-document similarities ( Salton et al. , 1997 ) .
Others+( Collins and Singer , 1999 ) add a special final round to boost recall , yielding 91.2 / 80.0 / 85.2 for the Yarowsky algorithm and 91.3 / 80.1 / 85.3 for their version of the original co-training algorithm .
Others+Similar approaches are used among others in ( Thelen and Riloff , 2002 ) for learning semantic lexicons , in ( Collins and Singer , 1999 ) for namedentity recognition , and in ( Fagni and Sebastiani , 2007 ) for hierarchical text categorization .
Others+Collins and Singer ( 1999 ) and Cucerzan and Yarowsky ( 1999 ) apply bootstrapping to the related task of named - entity recognition .
Others+Co-training has also been used for named entity recognition ( NER ) ( Collins and Singer , 1999 ) , coreference resolution ( Ng and Cardie , 2003 ) , text categorization ( Nigam and Ghani , 2000 ) and improving gene name data ( Wellner , 2005 ) .
Others+In the same vein , for the case of entity / relation extraction and classification ( Collins and Singer , 1999 ; Zhang , 2004 ; Chen et al. , 2006 ) the context of the entity or entities in consideration provides a highly relevant feature space .
Others+Most of the participants took language - independent approaches toward leveraging this complexity into better performance : generating machine learning features based on each item in a token ’s list of morphological attributes ( Nivre et al. , 2006b ; Carreras et al. , 2006 ) ; using the entire list as an atomic feature ( Chang et al. , 2006 ; Titov and Henderson , 2007 ) ; or generating features based on each pair of attributes in the cross-product of the lists of a potential head and dependent ( McDonald et al. , 2006 ; Nakagawa , 2007 ) .
Others+Both the graph - based ( McDonald et al. , 2005a ; McDonald and Pereira , 2006 ; Carreras et al. , 2006 ) and the transition - based ( Yamada and Matsumoto , 2003 ; Nivre et al. , 2006 ) parsing algorithms are related to our word - pair classification model .
Others+The ML classifier used for this experiment is a conditional Markov model tagger which is designed for , and proved successful in , named entity recognition in newspaper and biomedical text ( Klein et al. , 2003 ; Finkel et al. , 2005 ) .
Others+In addition , because of data sparsity ( out - of - vocabulary ) problem due to the long - tailed distribution of words in natural language , sophisticated unknown word models are generally needed for good performance ( Klein et al. 2003 ) .
Others+It differs from the more strict synchronous grammar formalisms ( Wu , 1995 ; Melamed et al. , 2004 ) because it does not try to perform simultaneous parsing on parallel grammars ; instead , the model learns an augmented target - language grammar whose rules make “ soft alignments ” with a given source tree .
Others+Several researchers ( Melamed et al. , 2004 ; Zhang et al. , 2006 ) have already proposed methods for binarizing synchronous grammars in the context of machine translation .
Others+separate nonterminal .2 Our measure of alignment complexity is analogous to what Melamed et al. ( 2004 ) call “ fanout . ”
Others+separate nonterminal .2 Our measure of alignment complexity is analogous to what Melamed et al. ( 2004 ) call “ fanout . ”
Others+Wellington et al. therefore argue that in order to extract as many rules as possible , a more powerful formalism than synchronous CFG / TSG is required : for example , generalized multitext grammar ( Melamed et al. , 2004 ) , which is equivalent to synchronous set - local multicomponent CFG / TSG ( Weir , 1988 ) .
Others+We utilized an offthe - shelf system , Stanford Named Entity Recognizer 4 ( Finkel et al. , 2005 ) for detecting entity mentions on the English sentences .
Others+We first extract named entities from scattered opinions DT using Stanford Named Entity Recognizer ( Finkel et al. , 2005 ) .
Others+To determine entailment , B IU T EE performs the following main steps : Preprocessing First , all documents are parsed and processed with standard tools for named entity recognition ( Finkel et al. , 2005 ) and coreference resolution .
Others+Decision on Attitude Label The decision on the most appropriate final label for the clause , in case @AM annotates it using different attitude types according to the words with multiple annotations ( e.g. , see word ‘ unfriendly ’ in Table 1 ) or based on the availability of the words conveying different attitude types , is made based on the analysis of : 1 ) morphological tags of nominal heads and their premodifiers in the clause ( e.g. , first person pronoun , third person pronoun , demonstrative pronoun , nominative or genitive noun , etc. ) ; 2 ) the sequence of hypernymic semantic relations of a particular noun in WordNet ( Miller , 1990 ) , which allows to determine its conceptual domain ( e.g. , “ person , human being ” , “ artifact ” , “ event ” , etc. ) ; 3 ) the annotations from the Stanford Named Entity Recognizer ( Finkel et al. 2005 ) that labels PERSON , ORGANIZATION , and LOCATION entities .
Others+whose value is the NE label of NPi , as determined by the Stanford CRF - based NE recognizer ( Finkel et al. , 2005 ) .
Others+Some prior work ( Ji et al. , 2005 ; Jing et al. , 2007 ) demonstrated the effectiveness of using semantic relations to improve entity coreference resolution ; while ( Downey et al. , 2005 ; Sutton and McCallum , 2004 ; Finkel et al. , 2005 ; Mann , 2007 ) experimented with information fusion of relations across multiple documents .
Others+To compute the features which we extract in the next section , all instances in our data sets were part - of - speech tagged by the MXPOST tagger ( Ratnaparkhi , 1996 ) , parsed with the MaltParser2 , and named entity tagged with the Stanford NE tagger ( Finkel et al. , 2005 ) .
Others+We used the Stanford NE tagger ( Finkel et al. , 2005 ) , and encoded three named entity classes ( “ person ” , “ location ” , “ organiszation ” ) in the feature vector .
Others+Named Entity Recognizer tagging ( NER ) : We integrated Stanford ’s NER tagger ( Finkel et al. , 2005 ) .
Others+Finkel et al. ( 2005 ) and Ji and Grishman ( 2008 ) incorporate global information by enforcing event role or label consistency over a document or across related documents .
Others+The named - entity features are generated by the freely available Stanford NER tagger ( Finkel et al. , 2005 ) .
Others+Semantic ( 1 ) : The named entity ( NE ) tag of wi obtained using the Stanford CRF - based NE recognizer ( Finkel et al. , 2005 ) .
Others+Instead , we opt to utilize the Stanford NER tagger ( Finkel et al. , 2005 ) over the sentences in a document and annotate each NP with the NER label assigned to that mention head .
Others+We used the features generated by the CRF package ( Finkel et al. , 2005 ) .
Others+For a fair comparison with previous work , we do not use gold named entity labels or mention types but , instead , take the labels provided by the Stanford named entity recognizer ( NER ) ( Finkel et al. , 2005 ) .
Others+Data preprocessing We preprocess our textual data as follows : We first use the Stanford named entity recognizer ( Finkel et al. , 2005 ) to find entity mentions in the corpus .
Others+We used as candidates all strings labeled in the annotated data as well as all named entities found by the Stanford NER tagger for CoNLL ( Finkel et al. , 2005 ) .
Others+Next we recognize named entities ( Finkel et al. , 2005 ) by labelling tokens with PERSON , ORGANIZATION , LOCATION , MISC and NONE tags .
Others+In addition there has been been work on SkipChain CRFs ( Sutton , 2004 ; Finkel et al. , 2005 ) which enforce consistency when classifying multiple occurrences of an entity within a document .
Others+We use Stanford Named Entity Recognizer ( Finkel et al. , 2005 ) to collect named entities which are not in the Wikipedia list .
Others+We use the Stanford Core NLP suite ( Toutanova et al. , 2003 ; Finkel et al. , 2005 ; Klein and Manning , 2003 ; Lee et al. , 2011 ) to annotate each document with POS and NER tags , parse trees , and coreference chains .
Others+Finkel et al. ( 2005 ) also integrated non-local information into entity annotation algorithms using Gibbs sampling .
Others+E XEMPLAR employs the Stanford NER ( Finkel et al. , 2005 ) to recognize named entities .
Others+The QA - SYS performs Part of Speech tagging using Stanford POS tagger ( Toutanova et al. , 2003 ) , and Named Entity Recognition using Stanford NER ( Finkel et al. , 2005 ) , and then builds a Lucene index over the set of input documents .
Others+For training the CRF model , we used a comprehensive set of features from Finkel et al. ( 2005 ) that gives state - of - the - art results on this task .
Others+NER is a fairly researched field ( Finkel et al. , 2005 ; Ratinov et al. , 2011 ; Bunescu and Pasca , 2006 ; Kulkarni et al. , 2009 ; Milne and Witten , 2008 ) and is also used in several commercial applications such as Zemanta , OpenCalais and AlchemyAPI4 , which are able to automatically insert links for a NE pointing to a knowledge base such as Wikipedia or IMDB .
Others+A common approach is to utilize a Named Entity Recognition ( NER ) system like Stanford NER ( Finkel et al. , 2005 ) , which recognizes the names of things ( e.g. , person and product names ) from texts .
Others+To assign types to arguments , we apply Stanford Named Entity Recognizer ( Finkel et al. , 2005 ) 5 , and also look up the argument in WordNet 2.1 and record
Others+We identified these mentions of persons using Stanford NER ( Finkel et al. , 2005 ) and treated each person mention as a single token .
Others+ner-eng-ie.crf-3-all2008-distsim.ser.gz ( Finkel et al. , 2005 ) ( line 3 in Table 1 ) , • GATE NER or in short GATE ( Cunningham et al. , 2002 ) ( line 4 in Table 1 ) , • and several hybrid systems which are given by
Others+We parse the data using the Collins Parser ( Collins , 1997 ) , and then tag person , location and organization names using the Stanford Named Entity Recognizer ( Finkel et al. , 2005 ) .
Others+Some stem from work on graphical models , including loopy belief propagation ( Sutton and McCallum , 2004 ; Smith and Eisner , 2008 ) , Gibbs sampling ( Finkel et al. , 2005 ) , sequential Monte Carlo methods such as particle filtering ( Levy et al. , 2008 ) , and variational inference ( Jordan et al. , 1999 ; MacKay , 1997 ; Kurihara and Sato , 2006 ) .
Others+We also use the Stanford NER tagger ( Finkel et al. , 2005 ) to identify Named Entities within the NP .
Others+Named - entity information is provided by Stanford Tagger ( Finkel et al. , 2005 ) .
Others+For this purpose , we applied the Stanford NER ( Finkel et al. , 2005 ) .
Others+Such techniques include Gibbs sampling ( Finkel et al. , 2005 ) , a general - purpose Monte Carlo method , and integer linear programming ( ILP ) , ( Roth and Yih , 2005 ) , a general - purpose exact framework for NP - complete problems .
Others+• In seminar announcements , a given field ( speaker , start time , etc. ) should appear with at most one value in each announcement , although the field and value may be repeated ( Finkel et al. , 2005 ) .
Others+Undirected graphical models such as Conditional Random Fields ( CRFs ) ( Lafferty et al. , 2001 ) have shown great success for problems involving structured output variables ( e.g. Wellner et al. ( 2004 ) , Finkel et al. ( 2005 ) ) .
Others+In spite of the discrepancy between the training model and the testing model , it has been empirically shown that ( 1 ) performing global inference only during testing can improve performance ( e.g. Finkel et al. ( 2005 ) , Roth and Yih ( 2005 ) ) , and ( 2 ) full - blown global training can often perform worse due to insufficient training data ( e.g. Punyakanok et al. ( 2005 ) ) .
Others+Others have attempted to train global scoring functions using Gibbs sampling ( Finkel et al. , 2005 ) , message propagation , ( Bunescu and Mooney , 2004 ; Sutton and McCallum , 2004 ) , and integer linear programming ( Roth and Yih , 2004 ) .
Others+Recent applications of statistical coreference models are beginning to show promise ( Finkel et al , 2005 ; Ji & Grishman , 2005 ) .
Others+For the named entity features , we used a fairly standard feature set , similar to those described in ( Finkel et al. , 2005 ) .
Others+Our features were based on those in ( Finkel et al. , 2005 ) .
Others+We focus on training using Gibbs sampling ( Geman and Geman , 1984 ) , because it has been popularly applied in the natural language literature , e.g. , ( Finkel et al. , 2005 ; DeNero et al. , 2008 ; Blunsom et al. , 2009 ) .
Others+∙ C ’s title represents the title of corresponding Wikipedia article of C. C ’s titleExpand represents the union set of the redirect set of C and the anchor text set of C. C ’s article represents the Wikipedia article of C. ∙ C ’s nameEntitySet represents the set of all named entities in C ’s article labeled by Stanford NER ( Finkel et al. , 2005 ) .
Others+One such technique is Markov chain Monte Carlo , and in particular Gibbs sampling ( Finkel et al. , 2005 ) , another is ( loopy ) sum - product belief propagation ( Smith and Eisner , 2008 ) .
Others+Namedentity information was obtained by the Stanford tagger ( Finkel et al. , 2005 ) .
Others+Sequential CRFs have been used successfully for semi-structured information extraction ( Sutton and McCallum , 2005 ; Finkel et al. , 2005 ) .
Others+The Stanford named entity recognition ( NER ) software1 ( Finkel et al. , 2005 ) is an implementation of linear chain Conditional Random Field ( CRF ) sequence models , which includes a three class ( person , organization , location and other ) named entity recognizer for English .
Others+For identification of named entities , we use Stanford NER ( Finkel et al. , 2005 ) .
Others+that partition words in a sentence into flow and inert groups ; we estimate this posterior using Gibbs sampling ( Finkel et al. , 2005 ) .
Others+We used the CRF - based Stanford NER tagger ( using Viterbi decoding ) as our baseline monolingual NER tool .6 English features were taken from Finkel et al. ( 2005 ) .
Others+For syntactic analysis we use the Stanford Parser ( Finkel et al. , 2005 ) .
Others+Global information is known to be useful in other NLP tasks , especially in the named entity recognition task , and several studies successfully used global features ( Chieu and Ng , 2002 ; Finkel et al. , 2005 ) .
Others+Finkel et al. ( 2005 ) used simulated annealing with Gibbs sampling to find a solution in a similar situation .
Others+Finkel et al. ( 2005 ) proposed a method incorporating non-local structure for information extraction .
Others+We also conduct experiments using simulated annealing in decoding , as conducted by Finkel et al. ( 2005 ) for information extraction .
Extends+Finkel et al. ( 2005 ) hand - set penalties for inconsistency in entity labeling at different occurrences in the text , based on some statistics from training data .
Extends+A very common case of this in the CoNLL dataset is that of documents containing references to both The China Daily , a newspaper , and China , the country ( Finkel et al. , 2005 ) .
Extends+proach keeps inference time down to just the inference time of two sequential CRFs , when compared to approaches such as those of Finkel et al. ( 2005 ) who report that their inference time with Gibbs sampling goes up by a factor of about 30 , compared to the Viterbi algorithm for the sequential CRF .
Extends+A very common case of this in the CoNLL dataset is that of documents containing references to both The China Daily , a newspaper , and China , the country ( Finkel et al. , 2005 ) .
Extends+consistency , has attempted to create all n2 pairwise dependencies between the different occurrences of an entity , ( Finkel et al. , 2005 ; Sutton and McCallum , 2004 ) , where n is the number of occurrences of the given entity .
Extends+The approach of Finkel et al. ( 2005 ) makes it possible a to model a broader class of longdistance dependencies than Sutton and McCallum ( 2004 ) , because they do not need to make any initial assumptions about which nodes should be connected and they too model dependencies between whole token sequences representing entities and between entity token sequences and their token supersequences that are entities .
Others+We employ Gibbs sampling , previously used in NLP by Finkel et al. ( 2005 ) and Goldwater et al. ( 2006 ) , among others .
Others+This corpus had a third portion , NPAPER , but we found that several documents where too long for lp solve to find a solution .4 We added named entity ( NE ) tags to the data using the tagger of Finkel et al. ( 2005 ) .
Others+Starting out with a chunking pipeline , which uses a classical combination of tagger and chunker , with the Stanford POS tagger ( Toutanova et al. , 2003 ) , the YamCha chunker ( Kudoh and Matsumoto , 2000 ) and the Stanford Named Entity Recognizer ( Finkel et al. , 2005 ) , the desire to use richer syntactic representations led to the development of a parsing pipeline , which uses Charniak and Johnson ’s reranking parser ( Charniak and Johnson , 2005 ) to assign POS tags and uses base NPs as chunk equivalents , while also providing syntactic trees that can be used by feature extractors .
Others+We perform named entity tagging using the Stanford four - class named entity tagger ( Finkel et al. , 2005 ) .
Others+We run the Stanford Named Entity Recognizer ( Finkel et al. , 2005 ) and record the number of PERSONs , ORGANIZATIONs , and LOCATIONs .
Others+We observed that the Stanford Named Entity Recognizer ( Finkel et al. , 2005 ) is superior to the Person detection of the
Others+The Linear CRF model is used as the fine model , with the following considerations : 1 ) It is wellstudied and has been successfully used in state - ofthe - art NER systems ( Finkel et al. , 2005 ; Wang , 2009 ) ; 2 ) it can output the probability of a label sequence , which can be used as the labeling confidence that is necessary for the semi-supervised learning framework .
Others+Since knowledge extraction from webbased encyclopedia is typically noisy ( Ponzetto and Poesio , 2009 ) , we use YAGO to determine whether two NPs have a relation only if one NP is a named entity ( NE ) of type person , organization , or location according to the Stanford NE recognizer ( Finkel et al. , 2005 ) and the other NP is a common noun .
Others+Finkel et al. ( 2005 ) used Gibbs sampling , a simple Monte Carlo method used to perform approximate inference in factored probabilistic models .
Others+Semantic features : we use the Stanford NER tagger ( Finkel et al. , 2005 ) to determine if the targeted NP is a named entity , and we use the Sundance parser ( Riloff and Phillips , 2004 ) to assign semantic class labels to each NP ’s head noun .
Others+To create d we extract all NEs from the text using the Stanford NE Recognizer ( Finkel et al. , 2005 ) and represent each NE by its Wikipedia URI .
Others+We use the Stanford Named Entity Recognizer ( Finkel et al. , 2005 ) for this purpose .
Others+Due to the large size of the corpora , we uniformly sampled a subset of documents for each corpus and ran the Stanford NER tagger ( Finkel et al. , 2005 ) , which tagged named entities mentions as person , location , and organization .
Others+The Figure also shows the results of the Stanford NER tagger for English ( Finkel et al. , 2005 ) ( we used the MUC - 7 classifier ) .
Others+Following ( Yao et al. , 2011 ) , we filter out noisy documents and use natural language packages to annotate the documents , including NER tagging ( Finkel et al. , 2005 ) and dependency parsing ( Nivre et al. , 2004 ) .
Others+Run Stanford CoreNLP with POS tagging and named entity recognition ( Finkel et al. , 2005 ) ; 2 .
Others+Accordingly , we extract all the named entities in a sentence using Stanford ’s Name Entity Recognizer ( Finkel et al. , 2005 ) .
Others+It includes : cleaning up and normalization of the input using regular expressions , sentence segmentation , tokenization and lemmatization using GATE ( Cunningham et al. , 2002 ) , syntactical parsing and dependency parsing ( collapsed ) using the Stanford Parser ( de Marneffe et al. , 2006 ) , and Named Entity Recognition using Stanford NER ( Finkel et al. , 2005 ) .
Others+B IU T EE provides state - of - the - art pre-processing utilities : Easy - First parser ( Goldberg and Elhadad , 2010 ) , Stanford named - entity - recognizer ( Finkel et al. , 2005 ) and ArkRef coreference resolver ( Haghighi and Klein , 2009 ) , as well as utilities for sentencesplitting and numerical - normalizations .
Others+The Stanford CRF - based NER tagger was used as the monolingual component in our models ( Finkel et al. , 2005 ) .
Others+Another promising direction for improving NER performance is in enforcing global label consistency across documents , which is an idea that has been greatly explored in the past ( Sutton and McCallum , 2004 ; Bunescu and Mooney , 2004 ; Finkel et al. , 2005 ) .
Others+We utilize the Stanford tools ( Toutanova et al. , 2003 ; Finkel et al. , 2005 ; Marneffe et al. , 2006 ) .
Others+The difference between Joint , Joint ( - parse ) , Shallow vs. Linguistic Features We validate the hypothesis that using linguistic features , e.g. , part - of - speech tags ( Toutanova and Manning , 2000 ) , named - entity tags ( Finkel et al. , 2005 ) , and dependency trees ( de Marneffe et al. , 2006 ) , helps improve the quality of our approach , called Joint .
Others+Similarly , distributional features support generalization in Named Entity Recognition ( Finkel et al. , 2005 ) .
Others+We used Open NLP POS tagger , Standford NER ( Finkel et al. , 2005 ) and MaltParser ( Nivre et al. , 2006 ) to label / tag sentences .
Others+State - of - the - art tools for named entity recognition such as the Stanford NER Tagger ( Finkel et al. , 2005 ) compute semantic tags only for a small set of coarse - grained types : Person , Location , and Organization ( plus tags for non-entity phrases of type time , money , percent , and date ) .
Others+The first is named entity ( NE ) tags ( PERSON , ORGANIZATION and LOCATION ) returned by the Stanford NE recognition tool ( Finkel et al. , 2005 ) .
Others+• Token number ; • POS tags using the Brill tagger ( Brill , 1992 ) ; • Named Entities using the Stanford named entity recognizer recognizer ( Finkel et al. , 2005 ) ; • Chunks using the chunker by Phan ( 2006 ) ; • Syntactic tree using the Charniak parser ( Charniak , 2000 ) ; • Dependency tree derived from the syntactic
Others+Initially all sentences are pre-processed by the CoreNLP ( Finkel et al. , 2005 ; Toutanova et al. , 2003 ) suite of tools , a process that includes named entity recognition , normalization , part of speech tagging , lemmatization and stemming .
Others+For capturing and normalizing the above mentioned expressions , we make use of the Stanford NER Toolkit ( Finkel et al. , 2005 ) .
Others+Named entity recognition The Stanford Named Entity Recogniser ( Finkel et al. , 2005 ) was run over the BNC and any nouns that occur in the corpus with both named entity and non-named entity tags are extracted to form the list Stanford .
Others+Initially all sentences are pre-processed by the CoreNLP ( Finkel et al. , 2005 ; Toutanova et al. , 2003 ) suite of tools , a process that includes named entity recognition , normalization , part of speech tagging , lemmatization and stemming .
Others+We analysed the text in the metadata , performing lemmatization , PoS tagging , named entity recognition and classification ( NERC ) and date detection using Stanford CoreNLP ( Finkel et al. , 2005 ; Toutanova et al. , 2003 ) .
Others+In our experiments , we performed named entity recognition with the Stanford NER tool using the standard English model ( Finkel et al. , 2005 ) .
Others+One of the steps in the analysis of English is named entity recognition using Stanford Named Entity Recognizer ( Finkel et al. , 2005 ) .
Others+The results we obtained on the CoNLL03 test set were consistent with what was reported in ( Finkel et al. , 2005 ) .
Others+Use of global features for structured prediction problem has been explored by several NLP applications such as sequential labeling ( Finkel et al. , 2005 ; Krishnan and Manning , 2006 ; Kazama and Torisawa , 2007 ) and dependency parsing ( Nakagawa , 2007 ) with a great deal of success .
Others+2009 ) , and we preprocess the data using the Stanford named - entity recognizer ( Finkel et al. , 2005 ) .
Others+Finkel et al. ( 2005 ) used Gibbs Sampling to add non-local dependencies into linear - chain CRF model for information extraction .
Others+We use SYNERGY ( Shah et al. , 2010 ) , an ensemble NER system that combines the UIUC NER ( Ritanov and Roth , 2009 ) and Stanford NER ( Finkel et al. , 2005 ) systems , to produce GNM and ONM from G and O by selecting named mentions .
Others+The Stanford NER based on CRF The Stanford Named Entity Recognizer ( NER ) is based on the machine learning algorithm Conditional Random Fields ( Finkel et al. , 2005 ) and has been used extensively for identifying named entities in news text .
Others+Results of training with Stanford NER CRF As a first indication of whether it is possible to use the annotated consensus corpus for finding negation and speculation in clinical text , we trained the Stanford NER CRF , ( Finkel et al. , 2005 ) on the annotated data .
Others+These include several off - the - shelf statisical NLP tools such as the Stanford POS tagger ( Toutanova and Manning , 2000 ) , the Stanford named - entity recognizer ( NER ) ( Finkel et al. , 2005 ) and the Stanford Parser ( Klein and Manning , 2003 ) .
Others+We performed all pre-processing ( tokenization , part - of - speech ( POS ) tagging ) with the Stanford CoreNLP toolkit .2 For EMD we used the Stanford named entity recognizer ( Finkel et al. , 2005 ) .
Others+We use the Stanford MaxentTagger ( Toutanova et al. , 2003 ) for partof - speech tagging , and the Stanford Named Entity Recognizer ( Finkel et al. , 2005 ) for annotating named entities .
Others+We ran the Stanford Named Entity Recognition system ( Finkel et al. , 2005 ) to obtain a set of 25,000 candidate mentions which the system judged to be names of people .
Others+We do not undertake the problem of named entity recognition ( Tjong Kim Sang , 2002 ) , but rather apply an existing NER system as a preprocessing step ( Finkel et al. , 2005 ) .
Others+We use Stanford named entity recognizer7 to extract named entities from the texts ( Finkel et al. , 2005 ) .
Others+we use the TnT POS tagger ( Brants , 2000 ) , WordNet ( Fellbaum , 1998 ) , the YamCha chunker ( Kudo and Matsumoto , 2003 ) , the Stanford NERC ( Finkel et al. , 2005 ) , and an in - house temporal expressions recogniser .
Others+Both the blogs and the Wikipedia articles were tagged using the Stanford Named Entity Recognizer ( Finkel et al. , 2005 ) , which labels the entities according to these types : Time , Location , Organization , Person , Money , Percent , Date , and Miscellaneous .
Others+One successful and freely available named entity recognizer is the Stanford NER system ( Finkel et al. , 2005 ) , which provides an implementation of linear chain CRF sequence models , coupled with well - engineered feature extractors for NER , and trained with newswire documents .
Others+Regarding NER systems , we chose the Stanford NER system ( Finkel et al. , 2005 ) , which has reported successful results when detecting person names .
Others+The Stanford tools perform part of speech tagging ( Toutanova et al. , 2003 ) , constituent and dependency parsing ( Klein and Manning , 2003 ) , named entity recognition ( Finkel et al. , 2005 ) , and coreference resolution ( Lee et al. , 2011 ) .
Others+Our first experiment utilizes WN , VN , and the Stanford Parser ( de Marneffe et al. , 2006 ) and Named Entity Recognizer ( Finkel et al. , 2005 ) .
Others+We use Stanford NER ( Finkel et al. , 2005 ) for named entity recognition .
Others+We retrained the Stanford named entity recognizer20 ( Finkel et al. , 2005 ) on the OntoNotes data .
Others+To extract the extended targets , we capture named entities ( NE ) from the Wikipedia page of the debate topic ( fetched using jsoup java library ) using the Stanford Named Entity Recognizer ( Finkel et al. , 2005 ) and sort them based on their page occurrence count .
Others+A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; Gale and Church , 1991 ) to lexical methods ( Kay and RSscheisen , 1993 ; Chen , 1993 ) .
Others+A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; Gale and Church , 1991 ) to lexical methods ( Kay and RSscheisen , 1993 ; Chen , 1993 ) .
Others+Only a few such corpora exist , including the Hansard English - French corpus and the HKUST EnglishChinese corpus ( Wu , 1994 ) .
Others+English and Chinese words , as in ( Wu , 1994 ) .
Extends+Figure 2 illustrates a DSyntS from a meteorological application , MeteoCogent ( Kittredge and Lavoie , 1998 ) , represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework ( Lavoie and Rambow , 1997 ) .
Extends+The framework was originally developed for the realization of deep - syntactic structures in NLG ( Lavoie and Rambow , 1997 ) .
Extends+• Realization of English DSyntSs via SSyntS level for the domains of meteorology ( MeteoCogent ; Kittredge and Lavoie , 1998 ) and object modeling ( ModelExplainer ; Lavoie et al. , 1997 ) .
Extends+History of the Framework and Comparison with Other Systems The framework represents a generalization of several predecessor NLG systems based on Meaning - Text Theory : FoG ( Kittredge and Polgu ~ re , 1991 ) , LFS ( Iordanskaja et al. , 1992 ) , and JOYCE ( Rambow and Korelsky , 1992 ) .
Extends+A companion paper describes the evaluation process and results in further detail ( Chu - Carroll and Nickerson , 2000 ) .
Others+Since task initiative models contribution to domain / problemsolving goals , while dialogue initiative affects the cur5An alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results ( Litman et al. , 1998 ) , which we leave for future work .
Others+Previous research , on the other hand , has shown that changes in initiative strategies in human - human dialogues can be dynamically modeled in terms of characteristics of the user and of the on - going dialogue ( Chu - Carroll and Brown , 1998 ) and that adaptability of initiative strategies in dialogue systems leads to better system performance ( Litman and Pan , 1999 ) .
Others+The strategies employed when M I M I C has only dialogue initiative are similar to the mixed initiative dialogue strategies employed by many existing spoken dialogue systems ( e.g. , ( Bennacef et al. , 1996 ; Stent et al. , 1999 ) ) .
Others+3.2.3 S t r a t e g y Selection Previous work has argued that initiative affects the degree o f control an agent has in the dialogue interaction ( Whittaker and Stenton , 1988 ; Walker and Whittaker , 1990 ; Chu - Carroll and Brown , 1998 ) .
Others+3.2.3 S t r a t e g y Selection Previous work has argued that initiative affects the degree o f control an agent has in the dialogue interaction ( Whittaker and Stenton , 1988 ; Walker and Whittaker , 1990 ; Chu - Carroll and Brown , 1998 ) .
Others+The goal of the JAVOX toolkit is to speech - enable traditional desktop applications this is similar to the goals of the MELISSA project ( Schmidt et al. , 1998 ) .
Others+CommandTalk ( Moore et al. , 1997 ) , Circuit Fix - It Shop ( Smith , 1997 ) and TRAINS - 96 ( Traum and Allen , 1994 ; Tranm and Andersen , 1999 ) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents .
Others+The speech and language processing architecture is based on that of the SRI CommandTalk system ( Moore et al. , 1997 ; Stent et al. , 1999 ) .
Others+This method allows the efficient retrieval of arbitrary length n - grams ( Nagao and Mori , 94 ; Haruno et al. , 96 ; Ikehara et al. , 96 ; Shimohata et al. , 1997 ; Russell , 1998 ) .
Extends+SWIZZLEis a multilingual enhancement of COCKTAIL ( Harabagiu and Maiorano , 1999 ) , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information2 .
Others+Wacholder et al. , 1997 ) , which performs lemmatization , and discovers proper names and technical terms .
Others+S h a l l o w T e x t Processing Linguistic preprocessing of text documents is carried out by re-using sines , an information extraction core system for real - world German text processing ( Neumann et al. , 1997 ) .
Others+S h a l l o w T e x t Processing Linguistic preprocessing of text documents is carried out by re-using sines , an information extraction core system for real - world German text processing ( Neumann et al. , 1997 ) .
Others+Vv ' e use an in - house statistical tagger ( based on ( Church , 1988 ) ) to tag the text in which the unknown word occurs .
Others+Each component will return a confidence measure of the reliability of its prediction , c.f. ( Elworthy , 1998 ) .
Others+Research that is more similar in goal to that outlined in this paper is Vosse ( Vosse , 1992 ) .
Others+Research that is more similar in goal to that outlined in this paper is Vosse ( Vosse , 1992 ) .
Extends+The uses of this procedure include information retrieval ( Hearst and Plaunt , 1993 ; Hearst , 1994 ; Yaari , 1997 ; Reynar , 1999 ) , summarization ( Reynar , 1998 ) , text understanding , anaphora resolution ( Kozima , 1993 ) , language modelling ( Morris and Hirst , 1991 ; Beeferman et al. , 199717 ) and improving document navigation for the visually disabled ( Choi , 2000 ) .
Extends+hnplementations of this idea use word stem repetition ( Youmans , 1991 ; Reynar , 1994 ; Ponte and Croft , 1997 ) , context vectors ( Hearst , 1994 ; Yaari , 1997 ; Kaufmann , 1999 ; Eichmann et al. , 1999 ) , entity repetition ( Kan et al. , 1998 ) , semantic similarity ( Morris and Hirst , 1991 ; Kozima , 1993 ) , word
Others+The question is becoming increasingly important , though , as wide - coverage HPSG grammars are starting to be deployed in practical applications - for example for &apos; deep &apos; analysis in the VerbMobil speech - to - speech translation system ( Wahlster , 1997 ; Kiefer , Krieger , Carroll , & Malouf , 1999 ) .
Others+Although the approach may have potential , the shifting of complex accounting into the unification algorithm is at variance with the findings of Kiefer et al. ( 1999 ) , who report large speed - ups from the elimination of disjunction processing during unification .
Others+We perceive that these results can be extended to other language models that properly embed bilexical context - free grammars , as for instance the more general history - based models used in ( Ratnaparkhi , 1997 ) and ( Chelba and Jelinek , 1998 ) .
Others+We perceive that these results can be extended to other language models that properly embed bilexical context - free grammars , as for instance the more general history - based models used in ( Ratnaparkhi , 1997 ) and ( Chelba and Jelinek , 1998 ) .
Others+Although this is only true in cases where y occurs in an upward monotone context ( MacCartney and Manning , 2007 ) , in practice genuine contradictions between
Extends+We collect substring rationales for a sentiment classification task ( Pang and Lee , 2004 ) and use them to obtain significant accuracy improvements for each annotator .
Extends+We use the same set of binary features as in previous work on this dataset ( Pang et al. , 2002 ; Pang and Lee , 2004 ; Zaidan et al. , 2007 ) .
Others+We use the same set of binary features as in previous work on this dataset ( Pang et al. , 2002 ; Pang and Lee , 2004 ; Zaidan et al. , 2007 ) .
Others+More recently , however , Okanohara and Tsujii ( 2007 ) showed that a
Others+Unfortunately , as shown in ( Okanohara and Tsujii , 2007 ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large - margin classifier ( see section 3 for details ) .
Others+We measure this association using pointwise Mutual Information ( MI ) ( Church and Hanks , 1990 ) .
Others+We also made use of the person - name / instance pairs automatically extracted by Fleischman et al. ( 2003 ) .2 This data provides counts for pairs such as “ Edwin Moses , hurdler ” and “ William Farley , industrialist . ”
Others+Our training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models ( Erk , 2007 ; Keller and Lapata , 2003 ; Rooth et al. , 1999 ) .
Others+We gather similar words using Lin ( 1998a ) , mining similar verbs from a comparable - sized parsed corpus , and collecting similar nouns from a broader 10 GB corpus of English text .4 We also use Keller and Lapata ( 2003 ) ’s approach to obtaining web - counts .
Others+Our training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models ( Erk , 2007 ; Keller and Lapata , 2003 ; Rooth et al. , 1999 ) .
Others+We gather similar words using Lin ( 1998a ) , mining similar verbs from a comparable - sized parsed corpus , and collecting similar nouns from a broader 10 GB corpus of English text .4 We also use Keller and Lapata ( 2003 ) ’s approach to obtaining web - counts .
Others+Also , the Keller and Lapata ( 2003 ) approach will be undefined if the pair is unobserved on the web .
Others+Also , the Keller and Lapata ( 2003 ) approach will be undefined if the pair is unobserved on the web .
Others+Erk ( 2007 ) compared a number of techniques for creating similar - word sets and found that both the Jaccard coefficient and Lin ( 1998a ) ’s information - theoretic metric work best .
Others+lead 1.42 , rejoin 1.39 , form 1.34 , belong to 1.31 , found 1.31 , quit 1.29 , guide 1.19 , induct 1.19 , launch ( subj ) 1.18 , work at 1.14 give a better S IMS ( join ) for Equation ( 1 ) than the top similarities returned by ( Lin , 1998a ) : participate 0.164 , lead 0.150 , return to 0.148 , say 0.143 , rejoin 0.142 , sign 0.142 , meet 0.142 , include 0.141 , leave 0.140 , work 0.137 Other features are also weighted intuitively .
Others+We gather similar words using Lin ( 1998a ) , mining similar verbs from a comparable - sized parsed corpus , and collecting similar nouns from a broader 10 GB corpus of English text .4 We also use Keller and Lapata ( 2003 ) ’s approach to obtaining web - counts .
Others+Selectional Preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules ( Pantel et al. , 2007 ; Roberto et al. , 2007 ) .
Others+We follow Pantel et al. ( 2007 ) in using automatically - extracted semantic classes to help characterize plausible arguments .
Others+Usually , the classes are from WordNet ( Miller et al. , 1990 ) , although they can also be inferred from clustering ( Rooth et al. , 1999 ) .
Others+Our training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models ( Erk , 2007 ; Keller and Lapata , 2003 ; Rooth et al. , 1999 ) .
Others+Our training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models ( Erk , 2007 ; Keller and Lapata , 2003 ; Rooth et al. , 1999 ) .
Others+The advantage of tuning similarity to the application of interest has been shown previously by Weeds and Weir ( 2005 ) .
Others+Markov Logic Inference Rules H OLMES is given the following set of six domainindependent rules , which are similar to the upward monotone rules introduced by ( MacCartney and Manning , 2007 ) .
Others+While many approaches have addressed this problem , our work is most closely related to that of ( Raina et al. , 2005 ; MacCartney and Manning , 2007 ; Tatu and Moldovan , 2006 ; Braz et al. , 2005 ) , which convert the inputs into logical forms and then attempt to ‘ prove ’ H from T plus a set of axioms .
Extends+Syntax - based statistical machine translation ( SMT ) models ( Liu et al. , 2006 ; Galley et al. , 2006 ; Huang et al. , 2006 ) capture long distance reorderings by using rules with structural and linguistical information as translation knowledge .
Extends+In this paper , we incorporate the MERS model into a stateof - the - art linguistically syntax - based SMT model , the tree - to - string alignment template ( TAT ) model ( Liu et al. , 2006 ) .
Extends+Our baseline system is Lynx ( Liu et al. , 2006 ) , which is a linguistically syntax - based SMT system .
Extends+The features can be easily obtained by modifying the TAT extraction algorithm described in ( Liu et al. , 2006 ) .
Others+As do constraint relaxation ( Tromble and Eisner , 2006 ) and forest reranking ( Huang , 2008 ) .
Others+We could also introduce new variables , e.g. , nonterminal refinements ( Matsuzaki et al. , 2005 ) , or secondary links Mij ( not constrained by T REE / PT REE ) that augment the parse with representations of control , binding , etc. ( Sleator and Temperley , 1993 ; Buch - Kromann , 2006 ) .
Others+( The method is approximate because a first - order parser must equally penalize all parses containing e0 , even those that do not in fact contain e. ) This behavior is somewhat similar to parser stacking ( Nivre and McDonald , 2008 ; Martins et al. , 2008 ) , in which a first - order parser derives some of its input features from the full 1 - best output of another parser .
Extends+Experiments ( Section 5 ) show that forestbased extraction improves BLEU score by over 1 point on a state - of - the - art tree - to - string system ( Liu et al. , 2006 ; Mi et al. , 2008 ) , which is also 0.5 points better than ( and twice as fast as ) extracting on 30 - best parses .
Extends+Experiments ( Section 5 ) show that forestbased extraction improves BLEU score by over 1 point on a state - of - the - art tree - to - string system ( Liu et al. , 2006 ; Mi et al. , 2008 ) , which is also 0.5 points better than ( and twice as fast as ) extracting on 30 - best parses .
Extends+In other words , it becomes nondeterministic how to “ cut ” a forest into tree fragments , which is analogous to the non-deterministic pattern - match in forest - based decoding ( Mi et al. , 2008 ) .
Extends+The first direct application of parse forest in translation is our previous work ( Mi et al. , 2008 ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ) .
Extends+We refer readers to Mi et al. ( 2008 ) for details of the decoding algorithm .
Others+Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( Boas 2002 ) .
Others+Moschitti et al. ( 2005 ) has made some preliminary attempt on the idea of hierarchical semantic
Others+be found in figure 2 , which is similar with that in Moschitti et al. ( 2005 ) .
Others+Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( Boas 2002 ) .
Others+Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 ) , Xue and Palmer ( 2005 ) and Xue ( 2008 ) .
Others+The candidate feature templates include : Voice from Sun and Jurafsky ( 2004 ) .
Others+Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( Boas 2002 ) .
Others+After the PropBank ( Xue and Palmer 2003 ) was built , Xue and Palmer ( 2005 ) and Xue ( 2008 ) have produced more complete and systematic research on Chinese SRL .
Others+Xue and Palmer ( 2004 ) did very encouraging work on the feature calibration of semantic role labeling .
Others+Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 ) , Xue and Palmer ( 2005 ) and Xue ( 2008 ) .
Others+Experiments on Chinese SRL ( Xue and Palmer 2005 , Xue 2008 ) reassured these findings .
Others+Position , subcat frame , phrase type , first word , last word , subcat frame , predicate , path , head word and its POS , predicate head word , predicate phrase type , path to BA and BEI , verb class 3 , verb class head word , verb class phrase type , from Xue ( 2008 ) .
Others+We use the same data setting with Xue ( 2008 ) , however a bit different from Xue and Palmer ( 2005 ) .
Others+Yi et al. ( 2007 ) has made the first attempt working on the single semantic role level to make further improvement .
Others+Monte Carlo sampling methods and Variational Bayes are two kinds of approximate inference methods that have been applied to Bayesian inference of unsupervised HMM POS taggers ( Goldwater and Griffiths , 2007 ; Johnson , 2007 ) .
Others+In our experiments we set α and α0 to the uniform values ( i.e. , all components have the same value α or α0 ) , but it is possible to estimate these as well ( Goldwater and Griffiths , 2007 ) .
Others+The samplers that Goldwater and Griffiths ( 2007 ) and Johnson ( 2007 ) describe are pointwise collapsed Gibbs samplers .
Others+The studies presented by Goldwater and Griffiths ( 2007 ) and Johnson ( 2007 ) differed in the number of states that they used .
Others+On small data sets all of the Bayesian estimators strongly outperform EM ( and , to a lesser extent , VB ) with respect to all of our evaluation measures , confirming the results reported in Goldwater and Griffiths ( 2007 ) .
Others+In language modeling , monolingual trigger approaches have been presented ( Rosenfeld , 1996 ; Tillmann and Ney , 1997 ) as well as syntactical methods that parse the input and model long - range dependencies on the syntactic level by conditioning on the predecessing words and their corresponding parent nodes ( Chelba and Jelinek , 2000 ; Roark , 2001 ) .
Others+The resulting training procedure is analogous to the one presented in ( Brown et al. , 1993 ) and ( Tillmann and Ney , 1997 ) .
Others+Coming up - to - date , ( Blunsom et al. , 2008 ) attempt a related estimation problem to ( Marcu and Wong , 2002 ) , using the expanded phrase pair set of ( Chiang , 2005a ) , working with an exponential model and concentrating on marginalizing out the latent segmentation variable .
Others+This turns out crucial for improved results ( cfXXX ( Blunsom et al. , 2008 ) ) .
Others+Secondly , as ( Blunsom et al. , 2008 ) show , marginalizing out the different segmentations during decoding leads to improved performance .
Others+While this heuristic estimator gives good empirical results , it does not seem to optimize any intuitively reasonable objective function of the ( wordaligned ) parallel corpus ( see e.g. , ( DeNero et al. , 2006 ) ) The mounting number of efforts attacking this problem over the last few years ( DeNero et al. , 2006 ; Marcu and Wong , 2002 ; Birch et al. , 2006 ; Moore and Quirk , 2007 ; Zhang et al. , 2008 ) exhibits its difficulty .
Others+This is remarkable given the fact that comparable previous work ( DeNero et al. , 2006 ; Moore and Quirk , 2007 ) did not match the performance of the heuristic estimator using large training sets .
Others+More recently , ( Moore and Quirk , 2007 ) devise a estimator working with a model that does not include a hidden segmentation variable but works with a heuristic iterative procedure ( rather than MLE or EM ) .
Others+Based on this advise ( Moore and Quirk , 2007 ) exclude the latent segmentation variables and opt for a heuristic training procedure .
Others+In error - driven learning approaches , the background model is adjusted so as to minimize the ranking errors the model makes on the adaptation data ( Bacchiani et al. , 2004 ; Gao et al. 2006 ) .
Others+In this paper , we extend two classes of model adaptation methods ( i.e. , model interpolation and error - driven learning ) , which have been well studied in statistical language modeling for speech and natural language applications ( e.g. , Bacchiani et al. , 2004 ; Bellegarda , 2004 ; Gao et al. , 2006 ) , to ranking models for Web search applications .
Others+Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( Bagga and Baldwin , 1998 ; Gooi and Allan , 2004 ) .
Others+The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks : Word Sense Disambiguation ( WSD ) ( Agirre and Edmonds , 2006 ) and Cross-document Coreference ( CDC ) ( Bagga and Baldwin , 1998 ) .
Others+Some researchers ( Cucerzan , 2007 ; Nguyen and Cao , 2008 ) have explored the use of Wikipedia information to improve the disambiguation process .
Others+Saggion ( 2008 ) compared the performace of NEs versus BoW features .
Others+SP acquisition from undisambiguated corpus data is arguably challenging ( Brockmann and Lapata , 2003 ; Erk , 2007 ; Bergsma et al. , 2008 ) .
Others+Brockmann and Lapata ( 2003 ) have showed that WordNet - based approaches do not always outperform simple frequency - based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( Erk , 2007 ; Bergsma et al. , 2008 ) .
Others+SP acquisition from undisambiguated corpus data is arguably challenging ( Brockmann and Lapata , 2003 ; Erk , 2007 ; Bergsma et al. , 2008 ) .
Others+Brockmann and Lapata ( 2003 ) have showed that WordNet - based approaches do not always outperform simple frequency - based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( Erk , 2007 ; Bergsma et al. , 2008 ) .
Others+McClosky et al. ( 2006 ) effectively utilized unlabeled data to improve parsing accuracy on the standard WSJ training set , but they used a two - stage parser comprised of Charniak ’s lexicalized probabilistic parser with n - best parsing and a discriminative reranking parser ( Charniak and Johnson , 2005 ) , and thus it would be better categorized as “ co-training ” ( McClosky et al. , 2008 ) .
Others+Better results would be expected by combining the PCFG - LA parser with discriminative reranking approaches ( Charniak and Johnson , 2005 ; Huang , 2008 ) for self training .
Others+Better results would be expected by combining the PCFG - LA parser with discriminative reranking approaches ( Charniak and Johnson , 2005 ; Huang , 2008 ) for self training .
Others+Self - training should also benefit other discriminatively trained parsers with latent annotations ( Petrov and Klein , 2008 ) , although training would be much slower compared to using generative models , as in our case .
Others+Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet - based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .
Others+Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet - based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .
Others+Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet - based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .
Others+In line with ( Lapata and Lascarides , 2003 ) our approach is based on the assumption that predicates are omitted in a discourse when they are highly predictable from the semantics of their arguments .
Others+In line with ( Lapata and Lascarides , 2003 ) , our approach to extraction of argument - predicate ( AP ) relations is based on two assumptions : A1 : If predicates are highly predictable from the semantics of their arguments then they can be omitted in a discourse ; A2 : If a predicate frequently takes a word as an argument then it is highly predictable from the semantics of this word .
Others+As already mentioned in the literature , see for example ( Lapata and Lascarides , 2003 ) , knowledge about implicit predicates could be potentially useful for a variety of NLP tasks such as language generation , information extraction , question answering or machine translation .
Others+When we run our classifiers on resource - tight environments such as cell - phones , we can use a random feature mixing technique ( Ganchev and Dredze , 2008 ) or a memory - efficient trie implementation based on a succinct data structure ( Jacobson , 1989 ; Delpratt et al. , 2006 ) to reduce required memory usage .
Others+Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( Charniak and Johnson , 2005 ) and products of latent variable grammars ( Petrov , 2010 ) , despite being a single generative PCFG .
Others+Recently , Petrov ( 2010 ) showed that substantial differences between the learned grammars remain , even if the hierarchical splitting reduces the variance across independent runs of EM .
Others+Petrov ( 2010 ) showed that a simple randomization scheme produces widely varying grammars .
Others+Petrov ( 2010 ) attempted to use the second method to train individual grammars on either disjoint or overlapping subsets of the treebank , but observed a performance drop in individual grammars resulting from training on less data , as well as in the performance of the product model .
Others+Given a sentence s and a set of grammars G = { G1 , · · · , Gn } , recall that the decoding algorithm of the product model ( Petrov , 2010 ) searches for the best tree T such that the following objective function
Others+Petrov ( 2010 ) attempted to use the second method to train individual grammars on either disjoint or overlapping subsets of the treebank , but observed a performance drop in individual grammars resulting from training on less data , as well as in the performance of the product model .
Others+Our best single grammar achieves an accuracy that is only slightly worse ( 91.6 vs. 91.8 in F score ) than the product model in Petrov ( 2010 ) .
Others+Our best single grammar achieves an accuracy that is only slightly worse ( 91.6 vs. 91.8 in F score ) than the product model in Petrov ( 2010 ) .
Others+Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( Charniak and Johnson , 2005 ) and products of latent variable grammars ( Petrov , 2010 ) , despite being a single generative PCFG .
Others+Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set ( Zhang et al. , 2009 ) .
Others+In multi-party chat ( Elsner and Charniak , 2008 ) report an average of 2.75 discussions active at a time .
Others+Second , we adopt a set of metrics to measure the local and global structural similarity between two annotations from the work on multi-party chat disentanglement ( Elsner and Charniak , 2008 ) .
Others+We adopt the more appropriate metrics 1 - to - 1 , lock and m - to - 1 , introduced recently by ( Elsner and Charniak , 2008 ) .
Others+In multi-party discussion people usually mention each other ’s name for the purpose of disentanglement ( Elsner and Charniak , 2008 ) .
Others+In multi-party discussion people usually mention each other ’s name for the purpose of disentanglement ( Elsner and Charniak , 2008 ) .
Others+With respect to the focus on function words , our reordering model is closely related to the UALIGN system ( Hermjakob , 2009 ) .
Extends+To model o ( Li , S → T ) , o ( Ri , S → T ) , i.e. the reordering of the neighboring phrases of a function word , we employ the orientation model introduced by Setiawan et al. ( 2007 ) .
Extends+To model d ( F Wi −1 , S → T ) , d ( F Wi 1 , S → T ) , i.e. whether Li , S → T and Ri , S → T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of Setiawan et al. ( 2009 ) .
Others+We find that for virtually all measures and datasets , older systems using relatively simple models and algorithms ( Brown et al. , 1992 ; Clark , 2003 ) work as well or better than systems using newer and often far more sophisticated and time - consuming machine learning methods ( Goldwater and Griffiths , 2007 ; Johnson , 2007 ; Graca et al. , 2009 ; Berg - Kirkpatrick et al. , 2010 ) .
Others+The systems are as follows : 1 [ brown ] : Class - based n - grams ( Brown et al. , 1992 ) .
Others+We found that the oldest system ( Brown et al. , 1992 ) yielded the best prototypes , and that using these prototypes gave state - of - the - art performance on WSJ , as well as improvements on nearly all of the non-English corpora .
Extends+To address this limitation , our previous work ( Zhang and Chai , 2009 ) has initiated an investigation on the problem of conversation entailment .
Extends+To address this limitation , our previous work ( Zhang and Chai , 2009 ) has initiated an investigation on the problem of conversation entailment .
Extends+Overall Framework In our previous work ( Zhang and Chai , 2009 ) , conversation entailment is formulated as the following : given a conversation segment D which is represented by a set of clauses D = d1 ∧ . . . ∧ dm , and a hypothesis H represented by another set of clauses H = h1 ∧ . . . ∧ hn , the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 . . . dm as follows .
Extends+Using the implicit modeling of argument consistency , we follow the same approach as in our previous work ( Zhang and Chai , 2009 ) and trained a logistic regression model to predict verb alignment based on the features in Table 1 .
Extends+This alignment is obtained by following the same set of rules learned from the development dataset as in ( Zhang and Chai , 2009 ) .
Others+We use the structures previously used by Nguyen et al. ( 2009 ) , and propose one new structure .
Others+In the opinion mining context this question has been prominently investigated with respect to opinion polarity analysis ( sentiment analysis ) in previous research ( Aue and Gamon , 2005 ; Blitzer et al. , 2007 ) .
Others+Blitzer et al. ( 2007 ) introduce an extension to a structural correspondence learning algorithm , which was specifically designed to address the task of domain adaptation .
Others+Our results also confirm the insights gained by Blitzer et al. ( 2007 ) , who observed that in crossdomain polarity analysis adding more training data is not always beneficial .
Others+For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; Jiang and Zhai , 2007 ) , perform in comparison to our approach .
Others+Domain Adaptation in Opinion Mining The task of creating a supervised algorithm , which when trained on data from domain A , also performs well on data from another domain B , is a domain adaptation problem ( Daum ´ e III and Marcu , 2006 ; Jiang and Zhai , 2007 ) .
Others+For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; Jiang and Zhai , 2007 ) , perform in comparison to our approach .
Others+Though we could have used a further downstream measure like BLEU , METEOR has also been shown to directly correlate with translation quality ( Banerjee and Lavie , 2005 ) and is simpler to measure .
Others+like information extraction ( Yates and Etzioni , 2009 ) and textual entailment ( Berant et al. , 2010 ) .
Others+In most cases , the accuracy of parsers degrades when run on out - of - domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) .
Others+For some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( UAS ) and labeled attachment score ( LAS ) ( Buchholz and Marsi , 2006 ) .
Others+This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; Chang et al. , 2010 ) .
Others+The work that is most similar to ours is that of Chang et al. ( 2007 ) , who introduced the Constraint Driven Learning algorithm ( CODL ) .
Others+until converged { Return model θ } tence xi and an output yi ; and 2 ) a loss - function , L ( ˆ y , y ) , that measures the cost of predicting output yˆ relative to the gold standard y and is usually the 0/1 loss ( Collins , 2002 ) .
Others+Identical to the standard perceptron proof , e.g. , Collins ( 2002 ) , by inserting in loss - separability for normal separability .
Others+In most cases , the accuracy of parsers degrades when run on out - of - domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) .
Others+We use the non-projective k - best MST algorithm to generate k - best lists ( Hall , 2007 ) , where k = 8 for the experiments in this paper .
Others+A recent study by Katz - Brown et al. ( 2011 ) also investigates the task of training parsers to improve MT reordering .
Others+Liang et al. ( 2006 ) presented a perceptron - based algorithm for learning the phrase - translation parameters in a statistical machine translation system .
Others+In terms of treebank data , the primary training corpus is the Penn Wall Street Journal Treebank ( PTB ) ( Marcus et al. , 1993 ) .
Others+In most cases , the accuracy of parsers degrades when run on out - of - domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) .
Others+The method is called targeted self - training as it is similar in vein to self - training ( McClosky et al. , 2006 ) , with the exception that the new parse data is targeted to produce accurate word reorderings .
Others+Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( McDonald and Nivre , 2007 ) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks To test whether such weak information can significantly improve the parsing of questions , we trained an augmented - loss parser using the training set of the QTB stripped of all dependencies except the dependency from the root to the main verb of the sentence .
Others+We will show in the next section that our augmentedloss method is general and can be applied to any dependency parsing framework that can be trained by the perceptron algorithm , such as transition - based parsers ( Nivre , 2008 ; Zhang and Clark , 2008 ) and graph - based parsers ( McDonald et al. , 2005 ) .
Others+We will show in the next section that our augmentedloss method is general and can be applied to any dependency parsing framework that can be trained by the perceptron algorithm , such as transition - based parsers ( Nivre , 2008 ; Zhang and Clark , 2008 ) and graph - based parsers ( McDonald et al. , 2005 ) .
Others+This includes work on question answering ( Wang et al. , 2007 ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks .
Others+We will show in the next section that our augmentedloss method is general and can be applied to any dependency parsing framework that can be trained by the perceptron algorithm , such as transition - based parsers ( Nivre , 2008 ; Zhang and Clark , 2008 ) and graph - based parsers ( McDonald et al. , 2005 ) .
Others+transition - based dependency parsing framework ( Nivre , 2008 ) using an arc - eager transition strategy and are trained using the perceptron algorithm as in Zhang and Clark ( 2008 ) with a beam size of 8 .
Others+In most cases , the accuracy of parsers degrades when run on out - of - domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) .
Others+Petrov et al. ( 2010 ) observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank .
Others+criteria and data used in our experiments are based on the work of Talbot et al. ( 2011 ) .
Others+This includes work on question answering ( Wang et al. , 2007 ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks .
Others+This includes work on question answering ( Wang et al. , 2007 ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks .
Others+We will show in the next section that our augmentedloss method is general and can be applied to any dependency parsing framework that can be trained by the perceptron algorithm , such as transition - based parsers ( Nivre , 2008 ; Zhang and Clark , 2008 ) and graph - based parsers ( McDonald et al. , 2005 ) .
Others+transition - based dependency parsing framework ( Nivre , 2008 ) using an arc - eager transition strategy and are trained using the perceptron algorithm as in Zhang and Clark ( 2008 ) with a beam size of 8 .
Others+Previous work has already demonstrated the benefits of using a bi-text for a related resource - rich language to X ( e.g. , X = English ) to improve machine translation from a resource - poor language to X ( Nakov and Ng , 2009 ; Nakov and Ng , 2012 ) .
Others+For example , our previous work ( Nakov and Ng , 2009 ; Nakov and Ng , 2012 ) experimented with various techniques for combining a small bi-text for a resource - poor language ( Indonesian or Spanish , pretending that Spanish is resource - poor ) with a much larger bi-text for a related resource - rich language ( Malay or Portuguese ) ; the target language of all bi-texts was English .
Others+Finally , we experiment with a method for combining phrase tables proposed in ( Nakov and Ng , 2009 ; Nakov and Ng , 2012 ) .
Others+Other possibilities for combining the phrase tables include using alternative decoding paths ( Birch et al. , 2007 ) , simple linear interpolation , and direct phrase table merging with extra features ( CallisonBurch et al. , 2006 ) ; they were previously found inferior to the last two approaches above ( Nakov and Ng , 2009 ; Nakov and Ng , 2012 ) .
Others+We use an in - house developed hierarchical derive the following function : phrase - based translation ( Chiang , 2005 ) as our baseK
Others+Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .
Others+( Och and Ney , 2002 ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for MT. ( Och , 2003 ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .
Others+Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .
Others+( Watanabe et al. , 2007 ; Chiang et al. , 2008 ; Hopkins and May , 2011 ) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions .
Others+( Watanabe et al. , 2007 ; Chiang et al. , 2008 ; Hopkins and May , 2011 ) proposed other optimization objectives by introducing a margin - based and ranking - based indirect loss functions .
Others+The significance test ( Och , 2003 ; Smith and Eisner , 2006 ) , we approxiing is performed by paired bootstrap re-sampling mate the Error in ( 5 ) by the expected loss , and then ( Koehn , 2004 ) .
Others+Och and Ney ( 2002 ) introduced the log - linear model for statistical machine translation ( SMT ) , in which translation is considered as the following optimization problem : where f and e ( e0 ) are source and target sentences , respectively .
Others+( Och and Ney , 2002 ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for MT. ( Och , 2003 ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .
Others+Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .
Others+The significance test ( Och , 2003 ; Smith and Eisner , 2006 ) , we approxiing is performed by paired bootstrap re-sampling mate the Error in ( 5 ) by the expected loss , and then ( Koehn , 2004 ) .
Others+( Och and Ney , 2002 ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for MT. ( Och , 2003 ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .
Others+To retrieve translation examples for a test sentence , ( Watanabe and Sumita , 2003 ) defined a metric based on the combination of edit distance and TF - IDF ( Manning and Sch ¨ utze , 1999 ) as follows :
Others+Our method resorts to some translation examples , which is similar as example - based translation or translation memory ( Watanabe and Sumita , 2003 ; He et al. , 2010 ; Ma et al. , 2011 ) .
Others+Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .
Others+( Och and Ney , 2002 ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for MT. ( Och , 2003 ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .
Extends+We take some core ideas from our previous work on mining script information ( Regneri et al. , 2010 ) .
Extends+Commonly used ( candidate ) comparable corpora are news articles written by different news agencies within a limited time window ( Wang and Callison - Burch , 2011 ) .
Extends+Our own work ( Wang and Callison - Burch , 2011 ) extends the first idea to paraphrase fragment extraction on monolingual parallel and comparable corpora .
Extends+Provided with the candidate fragment elements , we previously ( Wang and Callison - Burch , 2011 ) used a chunker3 to finalize the output fragments , in order to follow the linguistic definition of a ( para - ) phrase .
Extends+How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work ( Liu et al. , 2012 ) .
Extends+Previous work has developed various approaches for grounded semantics mainly for the reference resolution task , i.e. , identifying visual objects in the environment given language descriptions ( Dhande , 2003 ; Gorniak and Roy , 2004 ; Tenbrink and Moratz , 2003 ; Siebert and Schlangen , 2008 ; Liu et al. , 2012 ) .
Extends+The data from ( Liu et al. , 2012 ; Liu et al. , 2013 ) shows that different strategies were used by conversation partners to produce referential descriptions .
Extends+Previous work has developed various approaches for grounded semantics mainly for the reference resolution task , i.e. , identifying visual objects in the environment given language descriptions ( Dhande , 2003 ; Gorniak and Roy , 2004 ; Tenbrink and Moratz , 2003 ; Siebert and Schlangen , 2008 ; Liu et al. , 2012 ) .
Others+It is frequently used in tasks like scene identification , and Deselaers and Ferrari ( 2011 ) shows that distance in GIST space correlates well with semantic distance in WordNet .
Others+Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .
Others+Bruni et al. ( 2012a ) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation .
Others+This seems to provide additional evidence of Bruni et al. ( 2012b ) ’s suggestion that something like a distributional hypothesis of images is plausible .
Others+Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .
Others+Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .
Others+The first work to do this with topic models is Feng and Lapata ( 2010b ) .
Others+Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .
Others+Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .
Others+The first work to do this with topic models is Feng and Lapata ( 2010b ) .
Others+Other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( Mathe et al. , 2008 ; Regneri et al. , 2013 ) .
Others+Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .
Others+Cognitive Modalities Association Norms ( AN ) is a collection of association norms collected by Schulte im Walde et al. ( 2012 ) .
Others+Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .
Others+Silberer and Lapata ( 2012 ) introduce a new method of multimodal integration based on Canonical Correlation Analysis , and performs a systematic comparison between their CCA - based model and others on association norm prediction , held out feature prediction , and word similarity .
Others+Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together ( Andrews et al. , 2009 ; Silberer and Lapata , 2012 ) .
Others+This result is consistent with other works using this model with these features ( Andrews et al. , 2009 ; Silberer and Lapata , 2012 ) .
Others+Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .
Others+More recently , Silberer et al. ( 2013 ) show that visual attribute classifiers , which have been immensely successful in object recognition ( Farhadi et al. , 2009 ) , act as excellent substitutes for feature norms .
Others+It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers ( Silberer et al. , 2013 ) .
Others+Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .
Others+Similar to ( Li et al. , 2013a ) , we use a pipeline summarization framework where multiple compression candidates are generated for each pre-selected important sentence , and then an ILPIn this paper , we focus on the problem of using sentence compression techniques to improve multi-document summarization .
Others+Previous methods include using integer linear programming ( ILP ) and submodular functions to solve the optimization problem ( Gillick et al. , 2009 ; Li et al. , 2013b ; Lin and Bilmes , 2010 ) .
Others+Similar to ( Li et al. , 2013a ) , our summarization system is , which consists of three key components : an initial sentence pre-selection module to select some important sentence candidates ; the above compression model to generate n - best compressions for each sentence ; and then an ILP summarization method to select the best summary sentences from the multiple compressed sentences .
Others+Our summarization framework is the same as ( Li et al. , 2013a ) , except they used a CRF - based compression model .
Others+The training data for the sentence compression module in the summarization system is summary guided compression corpus annotated by ( Li et al. , 2013a ) using TAC2010 data .
Others+For a comparison , we also include the results using the CRF - based compression model ( the one used in ( Nomoto , 2007 ; Li et al. , 2013a ) ) .
Others+The CRF - based compression model used in ( Li et al. , 2013a ) can not well model the grammar .
Others+Similar to ( Li et al. , 2013a ) , we use a pipeline summarization framework where multiple compression candidates are generated for each pre-selected important sentence , and then an ILPIn this paper , we focus on the problem of using sentence compression techniques to improve multi-document summarization .
Others+Previous methods include using integer linear programming ( ILP ) and submodular functions to solve the optimization problem ( Gillick et al. , 2009 ; Li et al. , 2013b ; Lin and Bilmes , 2010 ) .
Others+Similar to ( Li et al. , 2013a ) , our summarization system is , which consists of three key components : an initial sentence pre-selection module to select some important sentence candidates ; the above compression model to generate n - best compressions for each sentence ; and then an ILP summarization method to select the best summary sentences from the multiple compressed sentences .
Others+Our summarization framework is the same as ( Li et al. , 2013a ) , except they used a CRF - based compression model .
Others+The training data for the sentence compression module in the summarization system is summary guided compression corpus annotated by ( Li et al. , 2013a ) using TAC2010 data .
Others+For a comparison , we also include the results using the CRF - based compression model ( the one used in ( Nomoto , 2007 ; Li et al. , 2013a ) ) .
Others+The CRF - based compression model used in ( Li et al. , 2013a ) can not well model the grammar .
Extends+One such task is debate stance classification ( SC ) : given a post written for a two - sided topic discussed in an online debate forum , determine which of the two sides ( i.e. , for or against ) its author is taking ( Agrawal et al. , 2003 ; Thomas et al. , 2006 ; Bansal et al. , 2008 ; Somasundaran and Wiebe , 2009 ; Burfoot et al. , 2011 ; Hasan and Ng , 2013b ) .
Extends+Following our previous work on stance classification ( Hasan and Ng , 2013c ) , we employ three types of features computed based on the frame - semantic parse of each sentence in a post obtained from SEMAFOR ( Das et al. , 2010 ) .
Extends+This choice is motivated by an observation we made previously ( Hasan and Ng , 2013a ) : since each post in a sequence is a reply to the preceding post , we could exploit their dependencies by determining their stance labels together .3 As our sequence learner , we employ a maximum entropy Markov model ( MEMM ) ( McCallum et al. , 2000 ) .
Extends+One such task is debate stance classification ( SC ) : given a post written for a two - sided topic discussed in an online debate forum , determine which of the two sides ( i.e. , for or against ) its author is taking ( Agrawal et al. , 2003 ; Thomas et al. , 2006 ; Bansal et al. , 2008 ; Somasundaran and Wiebe , 2009 ; Burfoot et al. , 2011 ; Hasan and Ng , 2013b ) .
Extends+Following our previous work on stance classification ( Hasan and Ng , 2013c ) , we employ three types of features computed based on the frame - semantic parse of each sentence in a post obtained from SEMAFOR ( Das et al. , 2010 ) .
Extends+This choice is motivated by an observation we made previously ( Hasan and Ng , 2013a ) : since each post in a sequence is a reply to the preceding post , we could exploit their dependencies by determining their stance labels together .3 As our sequence learner , we employ a maximum entropy Markov model ( MEMM ) ( McCallum et al. , 2000 ) .
Extends+One such task is debate stance classification ( SC ) : given a post written for a two - sided topic discussed in an online debate forum , determine which of the two sides ( i.e. , for or against ) its author is taking ( Agrawal et al. , 2003 ; Thomas et al. , 2006 ; Bansal et al. , 2008 ; Somasundaran and Wiebe , 2009 ; Burfoot et al. , 2011 ; Hasan and Ng , 2013b ) .
Extends+Following our previous work on stance classification ( Hasan and Ng , 2013c ) , we employ three types of features computed based on the frame - semantic parse of each sentence in a post obtained from SEMAFOR ( Das et al. , 2010 ) .
Extends+This choice is motivated by an observation we made previously ( Hasan and Ng , 2013a ) : since each post in a sequence is a reply to the preceding post , we could exploit their dependencies by determining their stance labels together .3 As our sequence learner , we employ a maximum entropy Markov model ( MEMM ) ( McCallum et al. , 2000 ) .
Extends+We previously proposed a fast , online tuning algorithm ( Green et al. , 2013b ) based on AdaGrad ( Duchi et al. , 2011 ) .
Extends+We also hypothesize that the final interactive corrections might be more useful since suggestions prime translators ( Green et al. , 2013a ) , and the MT system was able to refine its suggestions .
Extends+Our experimental design with professional bilingual translators follows our previous work Green et al. ( 2013a ) comparing scratch translation to post-edit .
Others+In ( Prabhakaran et al. , 2014 ) , we explored this dimension and found that candidates with higher power introduce significantly more topics in the debates , but attempt to shift topics significantly less often while responding to a moderator .
Others+We also investigated the utility of topic shifting features described in ( Prabhakaran et al. , 2014 ) extracted using LDA based topic modeling .
Others+This is in line with our previous findings from ( Prabhakaran et al. , 2014 ) that candidates with higher power attempt to shift topics less often than others when responding to moderators .
Extends+Bridging or associative anaphora has been widely discussed in the linguistic literature ( Clark , 1975 ; Prince , 1981 ; Gundel et al. , 1993 ; L ¨ obner , 1998 ) .
Others+Our approach to the problem is more compatible with the empirical evidence we presented in our prior work ( Li et al. , 2014 ) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality .
Others+in history - based models ( Black et al. , 1993 ) , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ... , d,_1 , which is called the derivation history at step i .
Others+The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ) .
Others+Log - linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( Charniak , 2000 ) .
Others+Collins and Duffy ( 2002 ) define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features ( Collins , 2000 ) .
Others+For right - branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( Johnson , 1998 ) , as has conditioning on the left - corner child ( Roark and Johnson , 1999 ) .
Others+For right - branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( Johnson , 1998 ) , as has conditioning on the left - corner child ( Roark and Johnson , 1999 ) .
Others+We also compare the results with the output generated by the statistical translation system GIZA / ISI ReWrite Decoder ( AlOnaizan et al. , 1999 ; Och and Ney , 2000 ; Germann et al. , 2001 ) , trained on the same parallel corpus .
Others+financial news , we created a probabilistic CzechEnglish dictionary by running GIZA training ( translation models 1 - 4 , see Och and Ney ( 2000 ) ) on the training part of the English - Czech WSJ parallel corpus extended by the parallel corpus of entry / translation pairs from the manual dictionary .
Others+Most DOP models , such as in Bod ( 1993 ) , Goodman ( 1996 ) , Bonnema et al. ( 1997 ) , Sima'an ( 2000 ) and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .
Others+The importance of including nonheadwords has become uncontroversial ( e.g. Collins 1999 ; Charniak 2000 ; Goodman 1998 ) .
Others+Collins 1996 , Charniak 1997 , Collins 1999 and Charniak 2000 ) .
Others+This approach has now gained wide usage , as exemplified by the work of Collins ( 1996 , 1999 ) , Charniak ( 1996 , 1997 ) , Johnson ( 1998 ) , Chiang ( 2000 ) , and many others .
Others+Collins 1996 , Charniak 1997 , Collins 1999 and Charniak 2000 ) .
Others+And Collins ( 2000 ) argues for &quot; keeping track of counts of arbitrary fragments within parse trees &quot; , which has indeed been carried out in Collins and Duffy ( 2002 ) who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .
Others+Most DOP models , such as in Bod ( 1993 ) , Goodman ( 1996 ) , Bonnema et al. ( 1997 ) , Sima'an ( 2000 ) and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .
Others+Goodman ( 1996 , 1998 ) developed a polynomial time PCFG - reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar .
Others+Fortunately , there exists a compact PCFG - reduction of DOP1 that generates the same trees with the same probabilities , as shown by Goodman ( 1996 , 2002 ) .
Others+Goodman then shows by simple induction that subderivations headed by A with external nonterminals at the roots and leaves , internal nonterminals elsewhere have probability 1 / a. And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1 / a1 ( Goodman 1996 ) .
Others+Most DOP models , such as in Bod ( 1993 ) , Goodman ( 1996 ) , Bonnema et al. ( 1997 ) , Sima'an ( 2000 ) and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .
Others+For our experiments we used the standard division of the WSJ ( Marcus et al. 1993 ) , with sections 2 through 21 for training ( approx. 40,000 sentences ) and section 23 for testing ( 2416 sentences 100 words ) ; section 22 was used as development set .
Others+Most DOP models , such as in Bod ( 1993 ) , Goodman ( 1996 ) , Bonnema et al. ( 1997 ) , Sima'an ( 2000 ) and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .
Others+The maximum entropy approach ( Berger et al. , 1996 ) presents a powerful framework for the combination of several knowledge sources .
Others+For an introduction to maximum entropy modeling and training procedures , the reader is referred to the corresponding literature , for instance ( Berger et al. , 1996 ) or ( Ratnaparkhi , 1997 ) .
Others+The feature av is derived from unsupervised segmentation as in ( Zhao and Kit , 2008a ) , and the accessor variety ( AV ) ( Feng et al. , 2004 ) is adopted as the unsupervised segmentation criterion .
Others+Due to using a global model like CRFs , our previous work in ( Zhao et al. , 2006 ; Zhao and Kit , 2008c ) reported the best results over the evaluated corpora of Bakeoff - 2 until now7 .
Others+The feature av is derived from unsupervised segmentation as in ( Zhao and Kit , 2008a ) , and the accessor variety ( AV ) ( Feng et al. , 2004 ) is adopted as the unsupervised segmentation criterion .
Others+Due to using a global model like CRFs , our previous work in ( Zhao et al. , 2006 ; Zhao and Kit , 2008c ) reported the best results over the evaluated corpora of Bakeoff - 2 until now7 .
Others+Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of Avramidis and Koehn ( 2008 ) , Yeniterzi and Oflazer ( 2010 ) and others .
Others+Bojar and Kos ( 2010 ) improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ) .
Others+Virpioja et al. ( 2007 ) , Badr et al. ( 2008 ) , Luong et al. ( 2010 ) , Clifton and Sarkar ( 2011 ) , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word - formation .
Others+Fraser ( 2009 ) tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms .
Others+We prepare the training data by splitting compounds in two steps , following the technique of Fritzinger and Fraser ( 2010 ) .
Others+For compound splitting , we follow Fritzinger and Fraser ( 2010 ) , using linguistic knowledge encoded in a rule - based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies .
Others+Koehn and Hoang ( 2007 ) showed that the use of a POS factor only results in negligible BLEU improvements , but we need access to the POS in our inflection prediction models .
Others+Koehn and Hoang ( 2007 ) introduced factored SMT .
Others+Other approaches use less deep linguistic resources ( e.g. , POS - tags Stymne ( 2008 ) ) or are ( almost ) knowledge - free ( e.g. , Koehn and Knight ( 2003 ) ) .
Others+The key linguistic knowledge sources that we use are morphological analysis and generation of German based on SMOR , a morphological analyzer / generator of German ( Schmid et al. , 2004 ) and the BitPar parser , which is a state - of - the - art parser of German ( Schmid , 2004 ) .
Others+Following the work of Stymne and Cancedda ( 2011 ) , we implement a linear - chain CRF merging system using the following features : stemmed ( separated ) surface form , part - of - speech14 and frequencies from the training corpus for bigrams / merging of word and word 1 , word as true prefix , word 1 as true suffix , plus frequency comparisons of these .
Others+We follow Stymne and Cancedda ( 2011 ) , for compound merging .
Others+Williams and Koehn ( 2011 ) used unification in an SMT system to model some of the
Others+Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of Avramidis and Koehn ( 2008 ) , Yeniterzi and Oflazer ( 2010 ) and others .
Extends+In our previous work ( Agarwal and Rambow , 2010 ) , we presented results for the two Introduction Social network extraction from text has recently been gaining a considerable amount of attention ( Agarwal and Rambow , 2010 ; Elson et al. , 2010 ; Agarwal et al. , 2013a ; Agarwal et al. , 2013b ; He et al. , 2013 ) .
Extends+In Agarwal and Rambow ( 2010 ) , we explored a wide range of syntactic structures for the two tasks of social event detection ( SED ) and classification ( SEC ) .
Others+aSee ( King , 1994 ) for a discussion of the appropriateness of T ~ - £ : for HPSG and a comparison with other feature logic approaches designed for HPSG .
Others+As shown in ( Minnen , 1996 ) * The presented research was carried out at the University of Tfibingen , Germany , as part of the Sonderforschungsbereich 340 .
Others+We rephrase the m e t h o d of Grimley - Evans ( 1997 ) as follows : First , w e construct the approximating finite a u t o m a t o n according to the u n p a r a m e t e r i z e d RTN m e t h o d above .
Others+This idea was proposed by Krauwer and des Tombe ( 1981 ) , Langendoen and Langsam ( 1987 ) , and Pulman ( 1986 ) , and was rediscovered by Black ( 1989 ) and recently by Johnson ( 1998 ) .
Others+1 The representation in Mohri and Pereira ( 1998 ) is even more compact than ours for grammars that are not self - embedding .
Others+A very similar formulation , for another grammar transformation , is given in Nederhof ( 1998 ) .
Others+This m e t h o d can be generalized , inspired by Stolcke and Segal ( 1994 ) , w h o derive N - gram probabilities from stochastic context - free grammars .
Others+In the transducers p r o d u c e d b y the training m e t h o d described in this paper , the source and target positions are in the set { 1 , 0 , 1 } , t h o u g h we have also used handcoded transducers ( Alshawi and Xia 1997 ) and automatically trained transducers ( A1shawl and Douglas 2000 ) with a larger range of positions .
Others+This contrasts with one of the traditional approaches ( e.g. , Dorr 1994 ; Watanabe 1995 ) to posing the translation problem , i.e. , the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language .
Others+These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy &apos;s notion of restrictive ( i.e. , bottom - up ) planning ( Hovy 1988a , 1988c ) .
Others+The names given to the components vary ; they have been called &quot; strategic &quot; and &quot; tactical &quot; components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , &quot; planning &quot; and &quot; realization &quot; ( e.g. , McDonald 1983 ; Hovy 1988a ) , or simply &quot; what to say &quot; versus &quot; how to say it &quot; ( e.g. , Danlos 1987 ; Reithinger 1990 ) .
Others+These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy &apos;s notion of restrictive ( i.e. , bottom - up ) planning ( Hovy 1988a , 1988c ) .
Others+Hovy has described another text planner that builds similar plans ( Hovy 1988b ) .
Others+33 Something like this approach is in fact used in some systems ( e.g. , Elhadad and Robin 1992 ; PenMan 1989 ; Hovy 1988a ) .
Others+The names given to the components vary ; they have been called &quot; strategic &quot; and &quot; tactical &quot; components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , &quot; planning &quot; and &quot; realization &quot; ( e.g. , McDonald 1983 ; Hovy 1988a ) , or simply &quot; what to say &quot; versus &quot; how to say it &quot; ( e.g. , Danlos 1987 ; Reithinger 1990 ) .
Others+These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy &apos;s notion of restrictive ( i.e. , bottom - up ) planning ( Hovy 1988a , 1988c ) .
Others+Hovy has described another text planner that builds similar plans ( Hovy 1988b ) .
Others+33 Something like this approach is in fact used in some systems ( e.g. , Elhadad and Robin 1992 ; PenMan 1989 ; Hovy 1988a ) .
Others+These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy &apos;s notion of restrictive ( i.e. , bottom - up ) planning ( Hovy 1988a , 1988c ) .
Others+McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( Meteer 1994 ; Panaget 1994 ; Wanner 1994 ) .
Others+1 ° The b o d y of a plan can be an action or sequence of actions , a goal or sequence 9 Moore and Paris also note that &quot; a generation system must maintain the kinds of information outlined by Grosz and Sidner &quot; ( Moore and Paris 1989 , 203 ) .
Others+This approach has occasionally been taken , as in Kantrowitz and Bates ( 1992 ) and Danlos ( 1987 ) 4 and , at least implicitly , in Paris and Scott ( 1994 ) and Delin et al. ( 1994 ) ; however , under this approach , all of the flexibility and simplicity of m o d u l a r design is lost .
Others+Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( Reiter 1994 ) .
Others+In fact , Reiter has even argued in favor of this approach , claiming that the interactions are sufficiently minor to be ignored ( or at least handled on an ad hoc basis ) ( Reiter 1994 ) .
Others+McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( Meteer 1994 ; Panaget 1994 ; Wanner 1994 ) .
Others+See also the work of Byrd and Chodorow ( 1985 ) , which considers computer - based pronunciation by analogy but does not mention the possible application to text - to - speech synthesis .
Others+Typical letter - to - sound rule sets are those described by Ainsworth ( 1973 ) , McIlroy ( 1973 ) , Elovitz et al. ( 1976 ) , Hunnicutt ( 1976 ) , and Divay and Vitale ( 1997 ) .
Others+For instance , Divay and Vitale ( 1997 ) recently wrote : " To our knowledge , learning algorithms , although promising , have not ( yet ) reached the level of rule sets developed by humans " ( p. 520 ) .
Others+In the models of Brown , Della Pietra , Della Pietra , and Mercer ( 1993 ) , reviewed in Section 4.3 ,
Others+There are many plausible representations , such as pairs of trees from synchronous tree adjoining grammars ( Abeill6 et al. 1990 ; Shieber 1994 ; Candito 1998 ) , lexical conceptual structures ( Dorr 1992 ) and WordNet synsets ( Fellbaum 1998 ; Vossen 1998 ) .
Others+• cross-language information retrieval ( e.g. , McCarley 1999 ) , • multilingual document filtering ( e.g. , Oard 1997 ) , • computer - assisted language learning ( e.g. , Nerbonne et al. 1997 ) , • certain machine - assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , • concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ) , Melamed Models of Translational Equivalence • corpus linguistics ( e.g. , Svartvik 1992 ) , • &quot; crummy &quot; machine translation ( e.g. , Church and Hovy 1992 ; Resnik
Extends+Many researchers have proposed greedy algorithms for estimating nonprobabilistic word - to - word translation models , also known as translation lexicons ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ; Fung 1995 ; Kumano and Hirakawa 1994 ; Melamed 1995 ; Wu and Xia 1994 ) .
Extends+Many researchers have proposed greedy algorithms for estimating nonprobabilistic word - to - word translation models , also known as translation lexicons ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ; Fung 1995 ; Kumano and Hirakawa 1994 ; Melamed 1995 ; Wu and Xia 1994 ) .
Others+• cross-language information retrieval ( e.g. , McCarley 1999 ) , • multilingual document filtering ( e.g. , Oard 1997 ) , • computer - assisted language learning ( e.g. , Nerbonne et al. 1997 ) , • certain machine - assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , • concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ) , Melamed Models of Translational Equivalence • corpus linguistics ( e.g. , Svartvik 1992 ) , • &quot; crummy &quot; machine translation ( e.g. , Church and Hovy 1992 ; Resnik
Others+If each w o r d &apos; s translation is treated as a sense tag ( Resnik and Yarowsky 1997 ) , then &quot; translational &quot; collocations have the unique p r o p e r t y that the collocate and the w o r d sense are one and the same !
Others+Many other such cases are described in Danlos &apos;s book ( Danlos 1987 ) .
Others+This may be because pipelines have many engineering advantages , and in practice the sort of problems pointed out by Danlos and other pipeline critics do not seem to be a major problem in current applied NLG systems ( Mittal et al. 1998 ) .
Others+The lexicon is used to mediate and map between a language - independent domain model and a language - dependent ontology widely used in NLG , the Upper Model ( Bateman 1990 ) .
Others+Shortly after the publication of The Sound Pattern of English ( Chomsky and Halle 1968 ) , Kornai points out , " Johnson ( 1970 ) demonstrated that the context - sensitive machinery of SPE ... [ could ] be replaced by a much simpler one , based on finite - state transducers ( FSTs ) ; the same conclusion was reached independently by Kaplan and Kay , whose work remained an underground classic until it was finally published in Kaplan and Kay ( 1994 ) . "
Others+' Whether this is always possible under an appropriate definition of " simple constraints " ( e.g. , Eisner 1997b ) is of course an empirical question .
Others+But typical OT grammars offer much richer finite - state models of left context ( Eisner 1997a ) than provided by the traditional HMM finite - state topologies .
Others+Samuelsson and Voutilainen ( 1997 ) report excellent part - of - speech tagging results using a handcrafted approach that is close to OT .
Others+An approach ( also based on regulation of the succession of rule application ) to the associated problem of spurious ambiguity is given in Hepple and Morrill ( 1989 ) but again , to our knowledge , there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in
Others+Surveys and articles on the topic include Lamarche and Retor ¢ ( 1996 ) , de Groote and Retor ¢ ( 1996 ) , and Morrill ( 1999 ) .
Others+The combination of likelihood and prior modeling , HMMs , and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( Bahl , Jelinek , and Mercer 1983 ) and tagging ( Church 1988 ) .
Others+It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct ( Dermatas and Kokkinakis 1995 ) .
Others+For example , Radzinsky ( 1991 ) proves that Chinese numerals such as w u zhao zhao zhao zhao zhao w u zhao zhao zhao zhao w u zhao zhao zhao w u zhao zhao w u zhao , for the number 5000000000000000005000000000000005000000000005000000005000 , are not context - free , which implies that Chinese is not a context - free language and thus might parse in exponential worst - case time .
Others+Discriminant analysis has been employed by researchers in automatic text genre detection ( Biber 1993b ; Karlgren and Cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables .
Others+For example , some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes ( Strzalkowski 1994 ) .
Others+We a s s u m e that e v e r y d e t e r m i n e r has its o w n equivalence , w h i c h resolves it as a quantifier : s o m e t i m e s this can be quite a complicated matter , as w i t h any ( Alshawi 1990 ) , w h i c h will resolve in different w a y s d e p e n d i n g on its linguistic context , b u t here w e a v o i d this complexity .
Others+The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in Alshawi ( 1990 , 1992 ) , and implemented in SRI &apos;s Core Language Engine ( CLE ) .
Others+The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs , either individually or in a &quot; packed &quot; structure ( Alshawi and Carter 1994 ) , with the resolution process as described here .
Others+We then go on to compare the current approach with that of some other theories with similar aims : the &quot; standard &quot; version of quasi-logical form implemented in the Core Language Engine , as rationally reconstructed by Alshawi and Crouch ( 1992 ) and Crouch and Pulman ( 1994 ) ; underspecified Discourse Representation Theory ( Reyle 1993 ) ; and the &quot; glue language &quot; approach of Dalrymple et al. ( 1996 ) .
Others+These constructs correspond as directly as possible to properties of the linguistic structure that express them and are , to as small an extent as possible , dependent on the requirements of contextual resolution ( unlike , say , the metavariables of standard QLFs [ Alshawi and Crouch 1992 ] , or the labels of UDRS [ Reyle 1996 ] , which are motivated entirely by the mechanisms that operate on them after grammatical processing ) .
Others+Volume 26 , Number 4 tionally reconstructed by Alshawi and Crouch ( 1992 ) and Crouch and Pulman ( 1994 ) , the context - independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules .
Others+A third problem arises with the approach to the semantics of QLFs that this notion of the relationship between QLF and RQLF encourages one to adopt : it is that taken by Alshawi and Crouch ( 1992 ) .
Others+only the available five relative scopings of the quantifiers are produced ( Hobbs and Shieber 1987 , 47 ) , but without the need for a free variable constraint -- the HOU algorithm will not produce any solutions in which a previously bound variable becomes free ; • the equivalences are reversible , and thus the above sentences can be generated from scoped logical forms ; • partial scopings are permitted ( see Reyle [ 1996 ] ) • scoping can be freely interleaved with other types of reference resolution ; • unscoped or partially scoped forms are available for inference or for generation at every stage .
Others+This equivalence is doing essentially the same job as Pereira &apos;s p r o n o u n abstraction schema in Pereira ( 1990 ) .
Others+The version p r o p o s e d here c o m b i n e s a basic insight f r o m Lewin ( 1990 ) w i t h higher - order unification to give an analysis that has a strong resemblance to that p r o p o s e d in Pereira ( 1990 , 1991 ) , w i t h s o m e differences that are c o m m e n t e d on below .
Others+It is interesting to compare this analysis with that described in Dalrymple , Shieber , and Pereira ( 1991 ) and Pereira ( 1990 , 1991 ) .
Others+But the general outlines are reasonably clear , and we can adapt some of the UDRS ( Reyle 1995 ) work to our own framework .
Others+A number of proposals in the 1990s deliberately limited the extent to which they relied on domain a n d / o r linguistic knowledge and reported promising results in knowledge - poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , 1998b ) .
Others+A number of proposals in the 1990s deliberately limited the extent to which they relied on domain a n d / o r linguistic knowledge and reported promising results in knowledge - poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , 1998b ) .
Others+A number of proposals in the 1990s deliberately limited the extent to which they relied on domain a n d / o r linguistic knowledge and reported promising results in knowledge - poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , 1998b ) .
Others+Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .
Others+For instance , the Alembic workbench ( Aberdeen et al. 1995 ) contains a sentence - splitting module that employs over 100 regular - expression rules written in Flex .
Others+The description of the EAGLE workbench for linguistic engineering ( Baldwin et al. 1997 ) mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower - cased in the same document .
Others+We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill ’s [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens .
Others+We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill ’s [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens .
Others+As Church ( 1988 ) rightly pointed out , however , “ Proper nouns and capitalized words are particularly problematic : some capitalized words are proper nouns and some are not .
Others+Since then this idea has been applied to several tasks , including word sense disambiguation ( Yarowsky 1995 ) and named - entity recognition ( Cucerzan and Yarowsky 1999 ) .
Others+This is implemented as a cascade of simple strategies , which were briefly described in Mikheev ( 1999 ) .
Others+Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum - entropy modeling ( Reynar and Ratnaparkhi 1997 ) .
Others+Park and Byrd ( 2001 ) recently described a hybrid method for finding abbreviations and their definitions .
Others+We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill ’s [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens .
Others+Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum - entropy modeling ( Reynar and Ratnaparkhi 1997 ) .
Others+This is similar to “ one sense per collocation ” idea of Yarowsky ( 1993 ) .
Others+Since then this idea has been applied to several tasks , including word sense disambiguation ( Yarowsky 1995 ) and named - entity recognition ( Cucerzan and Yarowsky 1999 ) .
Others+More specifically , the notion of the phrasal lexicon ( used first by Becker 1975 ) has been used successfully in a number of areas :
Others+Other similar approaches include those of Cicekli and Guvenir ¨ ( 1996 ) , McTait and Trujillo ( 1999 ) , Carl ( 1999 ) , and Brown ( 2000 ) , inter alia .
Others+Other similar approaches include those of Cicekli and Guvenir ¨ ( 1996 ) , McTait and Trujillo ( 1999 ) , Carl ( 1999 ) , and Brown ( 2000 ) , inter alia .
Others+Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; Veale and Way 1997 ; Carl 1999 ) .7 As an example , consider the translation into French of the house collapsed .
Others+Fung and McKeown ( 1997 ) attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora .
Others+Other similar approaches include those of Cicekli and Guvenir ¨ ( 1996 ) , McTait and Trujillo ( 1999 ) , Carl ( 1999 ) , and Brown ( 2000 ) , inter alia .
Others+Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; Veale and Way 1997 ; Carl 1999 ) .7 As an example , consider the translation into French of the house collapsed .
Others+More recently , Simard and Langlais ( 2001 ) have proposed the exploitation of TMs at a subsentential level , while Carl , Way , and Sch ¨ aler ( 2002 ) and Sch ¨ aler , Way , and Carl ( 2003 , pages 108 – 109 ) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment .
Others+Watanabe ( 1993 ) combines lexical and dependency mappings to form his generalizations .
Others+Some examples include text categorization ( Lewis and Catlett 1994 ) , base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part - of - speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill 2001 ) , and word sense disambiguation ( Fujii et al. 1998 ) .
Others+Some well - known approaches include rule - based models ( Brill and Resnik 1994 ) , backed - off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( Ratnaparkhi 1998 ) .
Others+Some well - known approaches include rule - based models ( Brill and Resnik 1994 ) , backed - off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( Ratnaparkhi 1998 ) .
Others+Some examples include text categorization ( Lewis and Catlett 1994 ) , base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part - of - speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill 2001 ) , and word sense disambiguation ( Fujii et al. 1998 ) .
Others+For example , while it is difficult to induce a grammar with raw text alone , the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data ( Pereira and Schabes 1992 ) .
Others+Some well - known approaches include rule - based models ( Brill and Resnik 1994 ) , backed - off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( Ratnaparkhi 1998 ) .
Others+The work of Sarkar ( 2001 ) and Steedman , Osborne , et al. ( 2003 ) suggests that co-training can be helpful for statistical parsing .
Others+Briscoe and Carroll ( 1997 ) predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection .
Others+Brent ( 1993 ) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames .
Others+The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following Brent ( 1993 ) .
Others+Briscoe and Carroll ( 1997 ) predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection .
Others+Briscoe and Carroll ( 1997 ) , by comparison , employ 163 distinct predefined frames .
Others+Aside from the extraction of theory - neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( Chen and Vijay - Shanker 2000 ; Xia 1999 ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .
Others+Chen and Vijay - Shanker ( 2000 ) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn - II Treebank with the aim of constructing a statistical model for parsing .
Others+The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman ( 1994 ) and Collins ( 1997 ) .
Others+Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head / argument / modifier distinctions are made for each node in the tree based on Magerman ( 1994 ) and Collins ( 1997 ) ;
Others+The extraction procedure utilizes a head percolation table as introduced by Magerman ( 1995 ) in combination with a variation of Collins ’s ( 1997 ) approach to the differentiation between complement and adjunct .
Others+Manning ( 1993 ) argues that , aside from missing domain - specific complementation trends , dictionaries produced by hand will tend to lag behind real language use because of their static nature .
Others+Manning ( 1993 ) attempts to improve on the approach of Brent ( 1993 ) by passing raw text through a stochastic tagger and a finite - state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur .
Others+However , more recent work ( Cahill et al. 2002 ; Cahill , McCarthy , et al. 2004 ) has presented efforts in evolving and scaling up annotation techniques to the Penn - II Treebank ( Marcus et al. 1994 ) , containing more than 1,000,000 words and 49,000 sentences .
Others+As noted above , it is well documented ( Roland and Jurafsky 1998 ) that subcategorization frames ( and their frequencies ) vary across domains .
Others+Sarkar and Zeman ( 2000 ) present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic
Others+A previous work along this line is Sproat et al. ( 1996 ) , which is based on weighted finite - state transducers ( FSTs ) .
Others+The Chinese person - name model is a modified version of that described in Sproat et al. ( 1996 ) .
Others+A previous work along this line is Sproat et al. ( 1996 ) , which is based on weighted finite - state transducers ( FSTs ) .
Others+NLG has to do more than select a distinguishing description ( i.e. , one that unambiguously denotes its referent ; Dale 1989 ) : The selected expression should also be felicitous .
Others+For example , consider a relational description ( cfXXX , Dale and Haddock 1991 ) involving a gradable adjective , as in the dog in the large shed .
Others+The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( Kyburg and Morreau 2000 ; DeVault and Stone 2004 ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .
Others+One area of current interest concerns the left - to - right arrangement of premodifying adjectives within an NP ( e.g. , Shaw and Hatzivassiloglou 1999 ; Malouf 2000 ) .
Others+A more flexible approach is used by Reiter and Sripada ( 2002 ) , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm / h , as heavy above 20 mm / h , and so on .
Others+A more flexible approach is used by Reiter and Sripada ( 2002 ) , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm / h , as heavy above 20 mm / h , and so on .
Others+One area of current interest concerns the left - to - right arrangement of premodifying adjectives within an NP ( e.g. , Shaw and Hatzivassiloglou 1999 ; Malouf 2000 ) .
Others+The algorithm we implemented is inspired by the work of Yarowsky ( 1995 ) on word sense disambiguation .
Others+Based on examples of this sort , recent approaches have formalized the problem of disambiguating PP attachments as a binary choice , distinguishing between attachment of a PP to a given verb or to the verb ’s direct object ( Hindle and Rooth 1993 ; Ratnaparkhi , Reynar , and Roukos 1994 ; Collins and Brooks 1995 ; Merlo , Crocker , and Berthouzoz 1997 ; Stetina and Nagao 1997 ; Ratnaparkhi 1997 ; Zhao and Lin 2004 ) .
Others+The changes made were inspired by those described in Stetina and Nagao ( 1997 , page 75 ) .
Others+Perhaps some variation of multi-level bulleted lists , appropriately integrated with interface elements for expanding and hiding items , might provide physicians a better overview of the information landscape ; see , for example , Demner - Fushman and Lin ( 2006 ) .
Others+Perhaps some variation of multi-level bulleted lists , appropriately integrated with interface elements for expanding and hiding items , might provide physicians a better overview of the information landscape ; see , for example , Demner - Fushman and Lin ( 2006 ) .
Others+We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization ( Lin and Demner - Fushman 2005b ) , but these features are also beyond the capabilities of current summarization systems .
Others+A number of studies ( e.g. , Hildebrandt , Katz , and Lin 2004 ) have pointed out shortcomings of the original nugget scoring model , although a number of these issues have been recently addressed ( Lin and Demner - Fushman 2005a , 2006b ) .
Others+We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization ( Lin and Demner - Fushman 2005b ) , but these features are also beyond the capabilities of current summarization systems .
Others+A number of studies ( e.g. , Hildebrandt , Katz , and Lin 2004 ) have pointed out shortcomings of the original nugget scoring model , although a number of these issues have been recently addressed ( Lin and Demner - Fushman 2005a , 2006b ) .
Others+In addition , there has been much work on the application of linguistic and semantic knowledge to information retrieval ; see Lin and Demner - Fushman ( 2006a ) for a brief overview .
Others+For example , McKnight and Srinivasan ( 2003 ) describe a machine learning approach to automatically label sentences as belonging to introduction , methods , results , or conclusion using structured abstracts as training data ( see also Lin et al. 2006 ) .
Others+The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by Niu and Hirst ( 2004 ) .
Others+Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanﬁlippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al. 2003 ; Preiss 2003 ; Kaplan et al. 2004 ; Miyao and Tsujii 2004 ) .
Others+TAG - based approaches convert 1 Our use of the terms “ shallow ” and “ deep ” parsers / grammars follows Kaplan et al. ( 2004 ) where a “ shallow parser ” does not relate strings to meaning representations .
Others+Two recent papers ( Preiss 2003 ; Kaplan et al. 2004 ) have started tying together the research strands just sketched : They use dependency - based parser evaluation to compare wide - coverage parsing systems using hand - crafted , deep , constraint - based grammars with systems based on a simple version of treebank - based deep grammar acquisition technology in the conversion paradigm .
Others+Her results show that the hand - crafted deep uniﬁcation parser ( Briscoe and Carroll 1993 ) outperforms the machine - learned parsers ( Collins 1997 ; Charniak 2000 ) on the f - score derived from weighted precision and recall on grammatical relations .4 Kaplan et al. ( 2004 ) compare their deep , handcrafted , LFG - based XLE parsing system ( Riezler et al. 2002 ) with Collins ’s ( 1999 ) model 3 using a simple conversion - based approach , capturing dependencies from the tree output of the machine - learned parser , and evaluating both parsers against the PARC 700 Dependency Bank ( King et al. 2003 ) .
Others+Two recent papers ( Preiss 2003 ; Kaplan et al. 2004 ) have started tying together the research strands just sketched : They use dependency - based parser evaluation to compare wide - coverage parsing systems using hand - crafted , deep , constraint - based grammars with systems based on a simple version of treebank - based deep grammar acquisition technology in the conversion paradigm .
Others+In the ﬁrst experiment ( Section 5.1 ) , we evaluate the f - structure annotation algorithm and Bikel retrained parser - based LFG system against the hand - crafted , wide - coverage LFG and XLE parsing system ( Riezler et al. 2002 ; Kaplan et al. 2004 ) on the PARC 700 Dependency Bank ( King et al. 2003 ) .
Others+Following the experimental setup of Kaplan et al. ( 2004 ) , we mark up multi-word expression predicates based on the gold - standard PARC 700 Dependency Bank .
Others+Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanﬁlippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al. 2003 ; Preiss 2003 ; Kaplan et al. 2004 ; Miyao and Tsujii 2004 ) .
Others+Her results show that the hand - crafted deep uniﬁcation parser ( Briscoe and Carroll 1993 ) outperforms the machine - learned parsers ( Collins 1997 ; Charniak 2000 ) on the f - score derived from weighted precision and recall on grammatical relations .4 Kaplan et al. ( 2004 ) compare their deep , handcrafted , LFG - based XLE parsing system ( Riezler et al. 2002 ) with Collins ’s ( 1999 ) model 3 using a simple conversion - based approach , capturing dependencies from the tree output of the machine - learned parser , and evaluating both parsers against the PARC 700 Dependency Bank ( King et al. 2003 ) .
Others+Following the experimental setup in Kaplan et al. ( 2004 ) , we use the Penn - II Section 23based PARC 700 Dependency Bank ( King et al. 2003 ) to evaluate the treebank - induced LFG resources against the hand - crafted XLE grammar and parsing system of Riezler et al. ( 2002 ) and Kaplan et al. .
Others+In the ﬁrst experiment ( Section 5.1 ) , we evaluate the f - structure annotation algorithm and Bikel retrained parser - based LFG system against the hand - crafted , wide - coverage LFG and XLE parsing system ( Riezler et al. 2002 ; Kaplan et al. 2004 ) on the PARC 700 Dependency Bank ( King et al. 2003 ) .
Others+Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanﬁlippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al. 2003 ; Preiss 2003 ; Kaplan et al. 2004 ; Miyao and Tsujii 2004 ) .
Others+Two recent papers ( Preiss 2003 ; Kaplan et al. 2004 ) have started tying together the research strands just sketched : They use dependency - based parser evaluation to compare wide - coverage parsing systems using hand - crafted , deep , constraint - based grammars with systems based on a simple version of treebank - based deep grammar acquisition technology in the conversion paradigm .
Others+Both Preiss ( 2003 ) and Kaplan et al. ( 2004 ) emphasize that they use rather basic versions of the conversion - based deep grammar acquisition technology outlined herein .
Others+Following Preiss ( 2003 ) , we use the SUSANNE Based CBS 500 Dependency Bank ( Carroll , Briscoe , and Sanﬁlippo 1998 ) to evaluate the treebankinduced LFG resources against the hand - crafted RASP grammar and parsing system ( Carroll and Briscoe 2002 ) as well as against the XLE system ( Riezler et al. 2002 ) .
Others+We used a sophisticated method for automatically producing deep dependency relations from Penn - II - style CFG - trees ( Cahill et al. 2002b , 2004 ) to compare shallow parser output at the level of dependency relation and revisit experiments carried out by Preiss ( 2003 ) and Kaplan et al. ( 2004 ) .
Extends+Information ordering has been investigated by substantial recent work in text - totext generation ( Barzilay , Elhadad , and McKeown 2002 ; Lapata 2003 ; Barzilay and Lee 2004 ; Barzilay and Lapata 2005 ; Bollegala , Okazaki , and Ishizuka 2006 ; Ji and Pulman 2006 ; Siddharthan 2006 ; Soricut and Marcu 2006 ; Madnani et al. 2007 , among others ) as well as concept - to - text generation ( particularly Kan and McKeown [ 2002 ] and Dimitromanolaki and Androutsopoulos 2003 ) .2 We added to this work by presenting approaches to information ordering based on a genetic algorithm ( Karamanis and Manurung 2002 ) and linear programming ( Althaus , Karamanis , and Koller 2004 ) which can be applied to both concept - to - text and text - to - text generation .
Extends+This became known as the principle of CONTINUITY ( Karamanis and Manurung 2002 ) .
Extends+Following our previous work ( Karamanis and Manurung 2002 ; Althaus , Karamanis , and Koller 2004 ) , the input to information ordering is an unordered set of informationbearing items represented as CF lists .
Others+Recent work that looks into the acquisition of the distinctive properties of idioms has been limited , both in scope and in the evaluation of the methods proposed ( Lin 1999 ; Evert , Heid , and Spranger 2004 ) .
Others+Speciﬁcally , we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity , an approach inspired by Lin ( 1999 ) .
Others+In his work , Lin ( 1999 ) assumes that a target expression is non-compositional if and only if its PMI value is signiﬁcantly different from that of all the variants .
Others+In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ; Barzilay and McKeown 2005 ) .
Others+Only an automatic evaluation was performed , which relied on having model responses ( Berger and Mittal 2000 ; Berger et al. 2000 ) .
Others+Two applications that , like help - desk , deal with question – answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ; Shrestha and McKeown 2004 ) , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000 ;
Others+In FAQs , Berger and Mittal ( 2000 ) employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence , and the questions and answers are embedded in an FAQ document .
Others+Because the judges do not evaluate the same cases , we could not employ standard inter-annotator agreement measures ( Carletta 1996 ) .
Others+The question answering system developed by Chu - Carroll et al. ( 2003 ) belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke ’s cascade sub-category ) .
Others+After calculating the raw score of each sentence , we use a modiﬁed version of the Adaptive Greedy Algorithm by Filatova and Hatzivassiloglou ( 2004 ) to penalize redundant sentences in cohesive clusters .
Others+In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ; Barzilay and McKeown 2005 ) .
Others+A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was signiﬁcantly smaller than ours ( Feng et al. 2006 ; Leuski et al. 2006 ) .
Others+number of syntactic phrases , grammatical mood , and grammatical person ( Marom and Zukerman 2006 ) , but the simple binary bag - of - lemmas representation yielded similar results .
Others+Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( Lapalme and Kosseim 2003 ; Roy and Subramaniam 2006 ) .
Others+Two applications that , like help - desk , deal with question – answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ; Shrestha and McKeown 2004 ) , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000 ;
Others+Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ Galley et al. 2004 ; Chiang et al. 2005 ] ) as well as for ∗ INESC - ID Lisboa , Spoken Language Systems Lab , R. Alves Redol 9 , 1000 - 029 LISBOA , Portugal .
Others+This heuristic is called soft union ( DeNero and Klein 2007 ) .
Others+In the context of word alignment , Deng and Byrne ( 2005 ) use a state - duration HMM in order to model word - to - phrase translations .
Others+This is the approach taken by IBM Models 4 ( Brown et al. 1993b ; Och and Ney 2003 ) , and more recently by the LEAF model ( Fraser and Marcu 2007 ) .
Others+Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ Galley et al. 2004 ; Chiang et al. 2005 ] ) as well as for ∗ INESC - ID Lisboa , Spoken Language Systems Lab , R. Alves Redol 9 , 1000 - 029 LISBOA , Portugal .
Others+tested three values of the threshold ( 0.2 , 0.4 , 0.6 ) which try to capture different tradeoffs 5 The open source Moses ( Hoang et al. 2007 ) toolkit from www.statmt.org/moses/ .
Others+results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En → Es results are based on a corpus of parliamentary proceedings ( Koehn 2005 ) .
Others+Another possibility that often works better is to use Minimum Bayes - Risk ( MBR ) decoding ( Kumar and Byrne 2002 ; Liang , Taskar , and Klein 2006 ; Grac ¸ a , Ganchev , and Taskar 2007 ) .
Others+Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations ( Och and Ney 2003 ) .
Others+This is the approach taken by IBM Models 4 ( Brown et al. 1993b ; Och and Ney 2003 ) , and more recently by the LEAF model ( Fraser and Marcu 2007 ) .
Others+Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations ( Och and Ney 2003 ) .
Others+For MT the most commonly used heuristic is called grow diagonal ﬁnal ( Och and Ney 2003 ) .
Others+For the task of unsupervised dependency parsing , Smith and Eisner ( 2006 ) add a constraint of the form “ the average length of dependencies should be X ” to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing .
Others+For the joint segmentation and POS - tagging task , we present a novel solution using the framework in this article , and show that it gives comparable accuracies to our previous work ( Zhang and Clark 2008a ) , while being more than an order of magnitude faster .
Others+In Zhang and Clark ( 2008a ) we proposed a joint word segmentor and POS - tagger using a multiple - beam decoder , and showed that it outperformed a pipelined baseline .
Others+In Zhang and Clark ( 2008a ) we proposed a joint word segmentor and POS - tagger using a multiple - beam decoder , and showed that it outperformed a pipelined baseline .
Others+We use several pruning methods from Zhang and Clark ( 2008a ) , most of which serve to improve the accuracy by removing irrelevant candidates from the beam .
Others+The threshold value is taken from Zhang and Clark ( 2008a ) , and we did not adjust it during development .
Others+From these four pruning methods , only the third was not used by our multiplebeam system ( Zhang and Clark 2008a ) .
Others+4.3.1 The Multiple - Beam System of Zhang and Clark ( 2008a ) .
Others+As we pointed out in Zhang and Clark ( 2008a ) , it is very difficult to score partial words properly when they are compared with full words , although such comparison is necessary for incremental decoding with a single - beam .
Others+The multiple - beam search decoder we proposed in Zhang and Clark ( 2008a ) can be seen as a means to ensure that the three characters always have a chance to be considered as a single word .
Others+The joint segmentor and tagger systems outperformed the baseline consistently , while the single beam - search decoder in this article gave comparable accuracies to our multiple - beam algorithm of Zhang and Clark ( 2008a ) .
Others+In this article , we use an in - house system which provides functional gender , number , and rationality features ( Alkuhlani and Habash 2012 ) .
Others+In this article , we use an in - house system which provides functional gender , number , and rationality features ( Alkuhlani and Habash 2012 ) .
Others+As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .
Others+For example , modeling CASE in Czech improves Czech parsing ( Collins et al. 1999 ) : CASE is relevant , not redundant , and can be predicted with sufficient accuracy .
Others+The result holds for both the MaltParser ( Nivre 2008 ) and the Easy - First Parser ( Goldberg and Elhadad 2010 ) .
Others+Furthermore , we demonstrate that our results carry over successfully to another parser , the Easy - First Parser ( Goldberg and Elhadad 2010 ) ( Section 6 ) .
Others+As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .
Others+inconsistency in the correspondence between inflectional features and morphemes , and inspired by Smrž ( 2007 ) , we distinguish between two types of inflectional features : formbased ( a.k.a. surface , or illusory ) features and functional features .6 Most available Arabic NLP tools and resources model morphology using formbased ( “ surface ” ) inflectional features , and do not mark rationality ; this includes the Penn Arabic Treebank ( PATB ) ( Maamouri et al. 2004 ) , the Buckwalter morphological analyzer ( Buckwalter 2004 ) , and tools using them such as the Morphological Analysis and Disambiguation for Arabic ( MADA ) toolkit ( Habash and Rambow 2005 ; Habash , Rambow , and Roth 2012 ) .
Others+inconsistency in the correspondence between inflectional features and morphemes , and inspired by Smrž ( 2007 ) , we distinguish between two types of inflectional features : formbased ( a.k.a. surface , or illusory ) features and functional features .6 Most available Arabic NLP tools and resources model morphology using formbased ( “ surface ” ) inflectional features , and do not mark rationality ; this includes the Penn Arabic Treebank ( PATB ) ( Maamouri et al. 2004 ) , the Buckwalter morphological analyzer ( Buckwalter 2004 ) , and tools using them such as the Morphological Analysis and Disambiguation for Arabic ( MADA ) toolkit ( Habash and Rambow 2005 ; Habash , Rambow , and Roth 2012 ) .
Others+2.5 Corpus , CATiB Format , and the CATIB 6 POS Tag Set We use the Columbia Arabic Treebank ( CATiB ) ( Habash and Roth 2009 ) .
Others+For more information on CATiB , see Habash and Roth ( 2009 ) and Habash , Faraj , and Roth ( 2009 ) .
Others+The following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE 44 and the newly introduced CORE 12 ; ( b ) CATiB Treebank tag set ( CATIB 6 ) ( Habash and Roth 2009 ) and its newly introduced extension of CATIB E X created using simple regular expressions on word form , indicating particular morphemes such as the wn ; this tag set is the best - performing tag set for Arabic prefix Al or the suffix on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW ) ( Buckwalter 2004 ) ; and two extensions of the PATB reduced tag set ( P ENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and Jurafsky 2004 ] ) , both outperforming it : ( d ) Kulick , Gabbard , and Marcus ( 2006 ) ’s tag set ( K ULICK ) , size 43 , one of whose most important extensions is the marking of the definite article clitic , and ( e ) Diab and Benajiba ’s ( in preparation ) E XTENDED RTS tag set ( ERTS ) , which marks gender , number , and definiteness , size 134 .
Others+As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .
Others+Hohensee and Bender ( 2012 ) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .
Others+In all our experiments , we contrasted the results obtained using machine - predicted input with the results obtained using gold input ( the 9 We do not relate to specific results in their study because it has been brought to our attention that Hohensee and Bender ( 2012 ) are in the process of rechecking their code for errors , and rerunning their experiments ( personal communication ) .
Others+For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( Nivre 2003 , 2008 ; Kübler , McDonald , and Nivre 2009 ) , a transition - based parser with an input buffer and a stack , which uses SVM classifiers 10 We use the term “ dev set ” to denote a non-blind test set , used for model development ( feature selection and feature engineering ) .
Others+The result holds for both the MaltParser ( Nivre 2008 ) and the Easy - First Parser ( Goldberg and Elhadad 2010 ) .
Others+As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .
Others+For statistical significance , we use McNemar ’s test on non-gold L AS , as implemented by Nilsson and Nivre ( 2008 ) .
Others+For statistical significance , we use McNemar ’s test on non-gold L AS , as implemented by Nilsson and Nivre ( 2008 ) .
Extends+To the best of our knowledge , the framework for solving event coreference presented in this article , extending the approach reported in Bejan and colleagues ( Bejan et al. 2009 ; Bejan and Harabagiu 2010 ) , is the only line of research on event coreference resolution that uses fully unsupervised methods and is based on Bayesian models .
Extends+HDP has been used in a wide variety of applications such as maneuvering target tracking ( Fox , Sudderth , and Willsky 2007 ) , visual scene analysis ( Sudderth et al. 2008 ) , information retrieval ( Cowans 2004 ) , entity coreference resolution ( Haghighi and Klein 2007 ; Ng 2008 ) , event coreference resolution ( Bejan et al. 2009 ; Bejan and Harabagiu 2010 ) , word segmentation ( Goldwater , Griffiths , and Johnson 2006 ) , and construction of stochastic context - free grammars ( Finkel , Grenager , and Manning 2007 ; Liang et al. 2007 ) .
Extends+Examples of applications that utilized these models are : identification of protein complexes ( Chu et al. 2006 ) , modeling of dyadic data ( Meeds et al. 2006 ) , ¨ ur , ¨ J ¨ akel , and Rasmussen 2006 ) , and event coreference modeling of choice behavior ( Gor resolution ( Bejan et al. 2009 ; Bejan and Harabagiu 2010 ) .
Extends+This article represents an extension of our previous work on unsupervised event coreference resolution ( Bejan et al. 2009 ; Bejan and Harabagiu 2010 ) .
Others+CCGBank ( Hockenmaier and Steedman 2007 ) is used to train the model .
Others+Such a type - raised category can then combine with a transitive verb type using the rule of forward composition : S / ( S \ NP ) ( S \ NP ) / NP ⇒ S / NP Following Fowler and Penn ( 2010 ) , we extract the grammar by reading rule instances directly from the derivations in CCGbank ( Hockenmaier and Steedman 2007 ) , rather than defining the combinatory rule schema manually as in Clark and Curran
Others+We use CCGBank ( Hockenmaier and Steedman 2007 ) and the Penn Treebank ( Marcus , Santorini , and Marcinkiewicz 1993 ) for CCG and dependency data , respectively .
Others+This article is based on , and significantly extends , three conference papers ( Zhang and Clark 2011 ; Zhang , Blackwood , and Clark 2012 ; Zhang 2013 ) .
Others+In our previous papers ( Zhang and Clark 2011 ; Zhang , Blackwood , and Clark 2012 ) , we applied a set of beams to this structure , which makes it similar to the data structure used for phrase - based MT decoding ( Koehn 2010 ) .
Others+In our previous papers ( Zhang and Clark 2011 ; Zhang , Blackwood , and Clark 2012 ) , we proposed an approximate online training algorithm , which forces positive examples to be kept in the hypothesis space without being discarded , and prevents the expansion of negative examples during the training process ( so that the hypothesis space does not get too large ) .
Others+In our previous papers ( Zhang and Clark 2011 ; Zhang , Blackwood , and Clark 2012 ) , we proposed an approximate online training algorithm , which forces positive examples to be kept in the hypothesis space without being discarded , and prevents the expansion of negative examples during the training process ( so that the hypothesis space does not get too large ) .
Others+In addition , the definitions of U PDATE PARAMETERS are different for the perceptron training algorithm ( Zhang and Clark 2011 ) , the large - margin training algorithm ( Zhang , Blackwood , and Clark 2012 ) , and the large - margin algorithm of this article , as explained earlier .
Others+The three curves represent the scaled model of this article , the online large - margin model from Zhang , Blackwood , and Clark ( 2012 ) , and the perceptron model from Zhang and Clark ( 2011 ) , respectively .
Others+The three curves represent the scaled model of this article , the online large - margin model from Zhang , Blackwood , and Clark ( 2012 ) , and the perceptron model from Zhang and Clark ( 2011 ) , respectively .
Others+The three curves represent the scaled model of this article , the online large - margin model from Zhang , Blackwood , and Clark ( 2012 ) , and the perceptron model from Zhang and Clark ( 2011 ) , respectively .
Others+The three curves represent the scaled model of this article , the online large - margin model from Zhang , Blackwood , and Clark ( 2012 ) , and the perceptron model from Zhang and Clark ( 2011 ) , respectively .
Others+The three curves represent the scaled model of this article , the online large - margin model from Zhang , Blackwood , and Clark ( 2012 ) , and the perceptron model from Zhang and Clark ( 2011 ) , respectively .
Others+More recently , a dictionary server , of the kind described by Kay ( 1984b ) , was implemented and installed as a background process on a Xerox workstation networked together with the rest of the equipment dedicated to natural language processing applications ( Boguraev et al. , 1987 ) .
Others+In addition , a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic , lexical , syntactic , and semantic information ( Boguraev et al. , 1987 ) .
Others+However , each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see Calzolari ( 1984 ) for further discussion ) .
Others+Our task was made possible by the fact that while far from being a database in the accepted sense of the word , the LDOCE typesetting tape is the only truly computerised dictionary of English ( Michiels , 1983 ) .
Others+The research described below is taking place in the context of three collaborative projects ( Boguraev , 1987 ; Russell et al. , 1986 ; Phillips and Thompson , 1986 ) to develop a general - purpose , wide coverage morphological and syntactic analyser for English .
Others+No attempt has been made to map any closed class entries from LDOCE , as a 3,000 word lexicon containing most closed class items has been developed independently by one of the groups collaborating with us to develop the general purpose morphological and syntactic analyser ( see the Introduction and Russell et al. , 1986 ) .
Others+Two exceptions to this generalisation are the Linguistic String Project ( Sager , 1981 ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; Byrd , 1983 ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources .
Others+Recent developments in linguistics , and especially on grammatical theory for example , Generalised Phrase Structure Grammar ( GPSG ) ( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG ) ( Kaplan and Bresnan , 1982 ) and on natural language parsing frameworks for example , Functional Unification Grammar ( FUG ) ( Kay , 1984a ) , PATR - II ( Shieber , 1984 ) make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .
Others+To demonstrate that this is possible we have implemented a system which constructs dictionary entries for the PATR - II system ( Shieber , 1984 and references therein ) .
Others+Hirschberg and Litman ( 1987 ) and Litman and Hirschberg ( 1990 ) also examine the relation between discourse and prosodic phrasing .
Others+The reader may consult recent papers on this subject ( e.g. Moens and Steedman 1987 ; Webber 1987 ) to see what a formal interpretation of events in time might look like .
Others+The reader may consult recent papers on this subject ( e.g. Moens and Steedman 1987 ; Webber 1987 ) to see what a formal interpretation of events in time might look like .
Others+Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and Young ( 1989 ) .
Others+This approach resembles the work by Grishman et al. ( 1986 ) and Hirschman et al. ( 1975 ) on selectional restrictions .
Others+Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and Young ( 1989 ) .
Others+27 Briscoe and Copestake ( 1996 ) argue that semi-productivity of lexical rules , which can be understood as a generalization of exceptions to lexical rules , can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry .
Others+The powerful mechanism of lexical rules ( Carpenter 1991 ) has been used in many natural language processing systems .
Others+A similar method is included in PATR - II ( Shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the CUF system ( Dbrre and Eisele 1991 ; D6rre and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; Emele 1994 ) .
Others+32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( Maxwell and Kaplan 1989 ; Eisele and D6rre 1990 ; Griffith 1996 ) can be used to circumvent constraint propagation .
Others+15 Hinrichs and Nakazawa ( 1996 ) show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention .
Others+32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( Maxwell and Kaplan 1989 ; Eisele and D6rre 1990 ; Griffith 1996 ) can be used to circumvent constraint propagation .
Others+Covariation Approach to HPSG Lexical Rules Riehemann 1993 ; Oliva 1994 ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .
Others+In practice , perceptron - type algorithms are often applied in a batch learning scenario , i.e. , the algorithm is applied for K epochs to a training sample of size T and then used for prediction on an unseen test set ( Freund and Schapire , 1999 ; Collins , 2002 ) .
Others+To prepare SMT outputs for post-editing , the creators of the corpus used their own WMT10 system ( Potet et al. , 2010 ) , based on the Moses phrase - based decoder ( Koehn et al. , 2007 ) with dense features .
Others+In fact , most of the features3 implemented in existing coreference resolution systems rely solely on mention heads ( Bengtson and Roth , 2008 ) .
Others+For this mention - pair coreference model φ ( u , v ) , we use the same set of features used in Bengtson and Roth ( 2008 ) .
Others+We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( Culotta et al. , 2007 ; Bengtson and Roth , 2008 ) .
Others+Cardie , 2002 ; Bengtson and Roth , 2008 ; Soon et al. , 2001 ) .
Others+Our work is inspired by the latent left - linking model in Chang et al. ( 2013 ) and the ILP formulation from Chang et al. ( 2011 ) .
Others+Our work is inspired by the latent left - linking model in Chang et al. ( 2013 ) and the ILP formulation from Chang et al. ( 2011 ) .
Others+Developed Systems Our developed system is built on the work by Chang et al. ( 2013 ) , using Constrained Latent Left - Linking Model ( CL3 M ) as our mention - pair coreference model in the joint framework10 .
Others+In this paper , we use the Constrained Latent Left - Linking Model ( CL3 M ) described in Chang et al. ( 2013 ) in our experiments .
Others+We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( Culotta et al. , 2007 ; Bengtson and Roth , 2008 ) .
Others+Baseline Systems We choose three publicly available state - of - the - art end - to - end coreference systems as our baselines : Stanford system ( Lee et al. , 2011 ) , Berkeley system ( Durrett and Klein , 2014 ) and HOTCoref system ( Bj ¨ orkelund and Kuhn , 2014 ) .
Others+For Berkeley system , we use the reported results from Durrett and Klein ( 2014 ) .
Others+Lee et al. ( 2012 ) model entity coreference and event coreference jointly ; Durrett and Klein ( 2014 ) consider joint coreference and entity - linking .
Others+We present experiments on the two standard coreference resolution datasets , ACE - 2004 ( NIST , 2004 ) and OntoNotes - 5.0 ( Hovy et al. , 2006 ) .
Others+The OntoNotes - 5.0 dataset , which is released for the CoNLL - 2012 Shared Task ( Pradhan et al. , 2012 ) , contains 3,145 annotated documents .
Others+Based on this assumption , the problem of identifying mention heads is a sequential phrase identification problem , and we choose to employ the BILOU - representation as it has advantages over traditional BIO-representation , as shown , e.g. in Ratinov and Roth ( 2009 ) .
Others+Ratinov and Roth ( 2009 ) present detailed studies on the task of named entity recognition , which discusses and compares different methods on multiple aspects including chunk representation , inference method , utility of non-local features , and integration of external knowledge .
Others+We further add rules for combining with punctuation to the left and right and allow for the merge rule X → X X of Clark and Curran ( 2007 ) .
Others+Terminal production prior means We employ the same procedure as our previous work for setting the terminal production prior distributions θtTERM -0 ( w ) by estimating word - givencategory relationships from the weak supervision : the tag dictionary and raw corpus ( Garrette and Baldridge , 2012 ; Garrette et al. , 2015 ) .4 This procedure attempts to automatically estimate the frequency of each word / tag combination by dividing the number of raw - corpus occurrences of each word in the dictionary evenly across all of its associated tags .
Extends+We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger ( Garrette et al. , 2014 ) .
Extends+We can define PCAT using a probabilistic grammar ( Garrette et al. , 2014 ) .
Extends+The right - side context of a non-terminal category — the probability of generating a category to the right of the current constituent ’s category — corresponds directly to the category transitions used for the HMM supertagger of Garrette et al. ( 2014 ) .
Extends+We use the same splits as Garrette et al. ( 2014 ) .
Others+In order to estimate the parameters of our model , we develop a blocked sampler based on that of Johnson et al. ( 2007 ) to sample parse trees for sentences in the raw training corpus according to their posterior probabilities .
Others+Our strategy is based on the approach presented by Johnson et al. ( 2007 ) .
Others+To sample from our proposal distribution , we use a blocked Gibbs sampler based on the one proposed by Goodman ( 1998 ) and used by Johnson et al. ( 2007 ) that samples entire parse trees .
Others+One important example is the constituentcontext model ( CCM ) of Klein and Manning ( 2002 ) , which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear .
Others+Since we are not generating from the model , this does not introduce difficulties ( Klein and Manning , 2002 ) .
Others+Klein and Manning ( 2002 ) ’s CCM is an unlabeled bracketing model that generates the span of part - of - speech tags that make up each constituent and the pair of tags surrounding each constituent span ( as well as the spans and contexts of each non-constituent ) .
Others+We follow Lewis and Steedman ( 2014 ) in allowing a small set of generic , linguistically - plausible unary and binary grammar rules .
Others+We evaluated on the English CCGBank ( Hockenmaier and Steedman , 2007 ) , which is a transformation of the Penn Treebank ( Marcus et al. , 1993 ) ; the CTBCCG ( Tse and Curran , 2010 ) transformation of the Penn Chinese Treebank ( Xue et al. , 2005 ) ; and the CCG - TUT corpus ( Bos et al. , 2009 ) , built from the TUT corpus of Italian text ( Bosco et al. , 2000 ) .
Others+This is similar to the “ deletion ” strategy employed by Zettlemoyer and Collins ( 2007 ) , but we do it directly in the grammar .
Others+These operations are not domain - specific and are similar to those of previous aggregation components ( Rambow and Korelsky , 1992 ; Shaw , 1998 ; Danlos , 2000 ) , although the various M ERGE operations are , to our knowledge , novel in this form .
Others+Previous work in sentence planning in the natural language generation ( NLG ) community uses hand - written rules to approximate the distribution of linguistic phenomena in a corpus ( see ( Shaw , 1998 ) for a recent example with further references ) .
Others+The data used in the experiment was selected from the Penn Treebank Wall Street Journal , and is the same used by Brill and Wu ( 1998 ) .
Others+The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation-based learner , and the ICA system ( Hepple , 2000 ) .
Others+The ICA system ( Hepple , 2000 ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance .
Others+2The algorithm was implemented by the the authors , following the description in Hepple ( 2000 ) .
Others+â¢ The regular TBL , as described in section 2 ; â¢ An improved version of TBL , which makes extensive use of indexes to speed up the rules ' update ; â¢ The FastTBL algorithm ; â¢ The ICA algorithm ( Hepple , 2000 ) .
Extends+It was originally proposed in ( Li and Abe , 1998 ) , and then adopted in our previous method for automatically extracting systematic polysemy ( Tomuro , 2000 ) .
Others+We have presented an ensemble approach to word sense disambiguation ( Pedersen , 2000 ) where multiple Naive Bayesian classi ers , each based on co { occurrence features from varying sized windows of context , is shown to perform well on the widely studied nouns interest and line .
Extends+Since our approach falls into this category ( expanding upon our earlier approach ( Schone and Jurafsky , 2000 ) ) , we describe work in this area in more detail .
Extends+2.3.3 Schone and Jurafsky : induced semantics In our earlier work , we ( Schone and Jurafsky ( 2000 ) ) generated a list of N candidate suffixes and used this list to identify word pairs which share the same stem but conclude with distinct candidate suffixes .
Extends+Using this final lexicon , we can now seek for suffixes in a manner equivalent to what we had done before ( Schone and Jurafsky , 2000 ) .
Extends+In order to obtain semantic representations of each word , we apply our previous strategy ( Schone and Jurafsky ( 2000 ) ) .
Extends+To correlate these semantic vectors , we use normalized cosine scores ( NCSs ) as we had illustrated before ( Schone and Jurafsky ( 2000 ) ) .
Extends+We compare this improved algorithm to our former algorithm ( Schone and Jurafsky ( 2000 ) ) as well as to Goldsmith &apos;s Linguistica ( 2000 ) .
Others+This approach has its roots in Fillmore ’s Case Grammar ( 1968 ) , and serves as the foundation for two current large - scale semantic annotation projects : FrameNet ( Baker et al. , 1998 ) and PropBank ( Kingsbury et al. , 2002 ) .
Others+Due to advances in statistical syntactic parsing techniques ( Collins , 1997 ; Charniak , 2001 ) , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences .
Others+The system is in the form of an agenda - driven chart - based parser whose foundation is similar to previous formalizations of Chomsky ’s Minimalist Program ( Stabler , 1997 ; Harkema , 2000 ; Niyogi , 2001 ) .
Extends+The system is in the form of an agenda - driven chart - based parser whose foundation is similar to previous formalizations of Chomsky ’s Minimalist Program ( Stabler , 1997 ; Harkema , 2000 ; Niyogi , 2001 ) .
Extends+Niyogi ( 2001 ) has developed an agenda - driven chart parser for the feature - driven formalism described above ; please refer to his paper for a description of the parsing algorithm .
Others+There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure — representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991b ; Rappaport Hovav and Levin , 1998 ) .
Others+This framework , where the “ semantic load ” is spread more evenly throughout the lexicon to lexical categories not typically thought to bear semantic content , is essentially the model advocated by Pustejovsky ( 1991a ) , among many others .
Others+Barzilay and McKeown ( 2001 ) also note that the applicability of paraphrases is strongly influenced by context .
Others+The paraphrase dictionary that we use was generated for us by Chris Callison - Burch , using the technique described in Callison - Burch ( 2008 ) , which exploits a parallel corpus and methods developed for statistical machine translation .
Others+The paraphrase dictionary that we use was generated for us by Chris Callison - Burch , using the technique described in Callison - Burch ( 2008 ) , which exploits a parallel corpus and methods developed for statistical machine translation .
Others+We use the Clark and Curran ( 2007 ) CCG parser to analyse the sentence before and after paraphrasing .
Extends+Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( Forbes - Riley and Litman , 2011a ; Drummond and Litman , 2011 ) , we have also experimented with other features , including state - of - theart acoustic - prosodic features used in the last Interspeech Challenges ( Schuller et al. , 2010 ; Schuller et al. , 2009b ) and made freely available in the openSMILE Toolkit ( Florian et al. , 2010 ) .
Others+We use the open - source Moses toolkit ( Koehn et al. , 2007 ) to build a phrase - based SMT system trained on mostly MSA data ( 64M words on the Arabic side ) obtained from several LDC corpora including some limited DA data .
Others+The parallel corpus is word - aligned using GIZA ( Och and Ney , 2003 ) .
Others+The number and sophistication of morphological analysis and disambiguation tools in DA is very limited in comparison to MSA ( Duh and Kirchhoff , 2005 ; Habash and Rambow , 2006 ; Abo Bakr et al. , 2008 ; Habash , 2010 ; Salloum and Habash , 2011 ; Habash et al. , 2012 ; Habash et al. , 2013 ) .
Others+In previous work , we presented a rule - based DA - MSA system to improve DA - to - English MT ( Salloum and Habash , 2011 ; Salloum and Habash , 2012 ) .
Others+In previous work , we presented a rule - based DA - MSA system to improve DA - to - English MT ( Salloum and Habash , 2011 ; Salloum and Habash , 2012 ) .
Others+c. Morphology - based word selection : E LISSA uses ADAM ( Salloum and Habash , 2011 ) to select words that have DA analyses only ( DIAONLY ) or DA / MSA analyses ( DIAMSA ) .
Others+dialectal morphological analysis step uses ADAM ( Salloum and Habash , 2011 ) to get a list of dialectal analyses .
Others+We use the same development ( dev ) and test sets used by Salloum and Habash ( 2011 ) ( we will call them speech - dev and speech - test , respectively ) and we compare to them in the next sections .
Others+This is a similar conclusion to our previous work in Salloum and Habash ( 2011 ) .
Others+Few approaches to parsing have tried to handle disfluent utterances ( notable exceptions are Core & Schubert , 1999 ; Hindle , 1983 ; Nakatani & Hirschberg , 1994 ; Shriberg , Bear , & Dowding , 1992 ) .
Others+One approach to this more general problem , taken by the ‘ Nitrogen ’ generator ( Langkilde and Knight , 1998a ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model .
Others+One approach to this more general problem , taken by the ‘ Nitrogen ’ generator ( Langkilde and Knight , 1998a ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model .
Others+Shaw and Hatzivassiloglou ( 1999 ) propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation ≺ .
Others+To quantify the relative strengths of these transitive inferences , Shaw and Hatzivassiloglou ( 1999 ) propose to assign a weight to each link .
Others+Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy - channel decoding ,1 including classic models for speech recognition ( Pereira and Riley , 1997 ) and machine translation ( Knight and Al - Onaizan , 1998 ) .
Others+For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules ( Mohri and Sproat , 1996 ) , ( 3 ) by compilation of decision trees ( Sproat and Riley , 1996 ) , ( 4 ) as a relation that performs contextual left - to - right replacement of input substrings by a smaller conditional relation ( Gerdemann and van Noord , 1999 ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below .
Others+A more subtle example is weighted FSAs that approximate PCFGs ( Nederhof , 2000 ; Mohri and Nederhof , 2001 ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .
Others+For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules ( Mohri and Sproat , 1996 ) , ( 3 ) by compilation of decision trees ( Sproat and Riley , 1996 ) , ( 4 ) as a relation that performs contextual left - to - right replacement of input substrings by a smaller conditional relation ( Gerdemann and van Noord , 1999 ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below .
Others+Similar to our previous work ( Chan and Ng , 2005b ) , we used the supervised WSD approach described in ( Lee and Ng , 2002 ) for our experiments , using the naive Bayes algorithm as our classifier .
Others+As a result , researchers have re-adopted the once - popular knowledge - rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .
Others+2 ( 6 ) INDUCED CLASS : Since the first - sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , Hearst ( 1992 ) ) .
Others+As a result , researchers have re-adopted the once - popular knowledge - rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .
Others+More importantly , the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution : many of them evaluate coreference performance on perfect mentions ( e.g. , Luo et al. ( 2004 ) ) ; and for those that do report performance on automatically extracted mentions , they do not explain whether or how the induced SC information is used in their coreference algorithms .
Others+However , learning - based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , Soon et al. ( 2001 ) , Markert and Nissim ( 2005 ) ) .
Others+Each instance is represented by 33 lexical , grammatical , semantic , and positional features that have been employed by highperforming resolvers such as Ng and Cardie ( 2002 ) and Yang et al. ( 2003 ) , as described below .
Others+As a result , researchers have re-adopted the once - popular knowledge - rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .
Others+Following Ponzetto and Strube ( 2006 ) , we consider an anaphoric reference , NPi , correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition .
Others+However , learning - based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , Soon et al. ( 2001 ) , Markert and Nissim ( 2005 ) ) .
Others+Motivated by Soon et al. ( 2001 ) , we have a semantic feature that tests whether one NP is a name alias or acronym of the other .
Others+Following previous work ( e.g. , Soon et al. ( 2001 ) and Ponzetto and Strube ( 2006 ) ) , we generate training instances as follows : a positive instance is created for each anaphoric NP , NPj , and its closest antecedent , NPi ; and a negative instance is created for NPj paired with each of the intervening NPs , NPi 1 , NPi 2 , . . . , NPj −1 .
Others+Motivated by Soon et al. ( 2001 ) , we have a semantic feature that tests whether one NP is a name alias or acronym of the other .
Others+We report performance in terms of two metrics : ( 1 ) the Fmeasure score as computed by the commonly - used MUC scorer ( Vilain et al. , 1995 ) , and ( 2 ) the accuracy on the anaphoric references , computed as the fraction of anaphoric references correctly resolved .
Others+Each instance is represented by 33 lexical , grammatical , semantic , and positional features that have been employed by highperforming resolvers such as Ng and Cardie ( 2002 ) and Yang et al. ( 2003 ) , as described below .
Others+Ng and Low ( 2004 ) mapped the joint segmentation and POS tagging task into a single character sequence tagging problem .
Others+Ng and Low ( 2004 ) and Shi and Wang ( 2007 ) were described in the Introduction .
Others+Word information is used to process known - words , and character information is used for unknown words in a similar way to Ng and Low ( 2004 ) .
Others+Ng and Low ( 2004 ) mapped the joint segmentation and POS tagging task into a single character sequence tagging problem .
Extends+We use baseline system to refer to the system which performs segmentation first , followed by POS tagging ( using the single - best segmentation ) ; baseline segmentor to refer to the segmentor from ( Zhang and Clark , 2007 ) which performs segmentation only ; and baseline POS tagger to refer to the Collins tagger which performs POS tagging only ( given segmentation ) .
Extends+Experiments with the standard beam - search decoder described in ( Zhang and Clark , 2007 ) resulted in low accuracy .
Extends+The Baseline System We built a two - stage baseline system , using the perceptron segmentation model from our previous work ( Zhang and Clark , 2007 ) and the perceptron POS tagging model from Collins ( 2002 ) .
Others+Our HDP extension is also inspired from the Bayesian model proposed by Haghighi and Klein ( 2007 ) .
Others+These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work ( He et al. , 2008 ; Gimpel and Smith , 2008 ; Marton and Resnik , 2008 ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1 .
Others+( Marton and Resnik , 2008 ; Xiong et al. , 2009 ) explore the source syntactic information to reward exact matching structure rules or punish crossing structure rules .
Others+These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work ( He et al. , 2008 ; Gimpel and Smith , 2008 ; Marton and Resnik , 2008 ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1 .
Others+More recently , an alignment selection approach was proposed in ( Huang , 2009 ) , which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand - picked threshold .
Others+Similar to ( Huang , 2009 ) , we define the confidence of aijk as Stemming Pashto is one of the morphologically rich languages .
Others+The feature of head word trigger which we apply to the log - linear model is motivated by the trigger - based approach ( Hasan and Ney , 2009 ) .
Others+Sridhar et al. ( 2009 ) obtain promising results in dialogue act tagging of the Switchboard - DAMSL corpus using lexical , syntactic and prosodic cues , while Gravano and Hirschberg ( 2009 ) examine the relation between particular acoustic and prosodic turn - yielding cues and turn taking in a large corpus of task - oriented dialogues .
Others+We use the TRIPS dialogue parser ( Allen et al. , 2007 ) to parse the utterances .
Others+The contextual interpreter then uses a reference resolution approach similar to Byron ( 2002 ) , and an ontology mapping mechanism ( Dzikovska et al. , 2008a ) to produce a domain - specific semantic representation of the student ’s output .
Others+At present , the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers .1 In addition to a remediation policy , the tutorial planner implements an error recovery policy ( Dzikovska et al. , 2009 ) .
Others+The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example , using better terminology ) but does n’t explicitly explain the reason why different terminology is needed ( Dzikovska et al. , 2010 ) .
Others+These include , just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .
Others+Some previous works ( Bannard and Callison - Burch , 2005 ; Zhao et al. , 2009 ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .
Others+One of the proposed methods to extract paraphrases relies on a pivot - based approach using phrase alignments in a bilingual parallel corpus ( Bannard and Callison - Burch , 2005 ) .
Others+These include , just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .
Others+They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) .
Others+They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) .
Others+They are widely used in MT as a way to figure out how to translate input in one language into output in another language ( Koehn et al. , 2003 ) .
Others+Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( Koehn et al. , 2007 ) .
Others+Introduction Cross-lingual Textual Entailment ( CLTE ) has been proposed by ( Mehdad et al. , 2010 ) as an extension of Textual Entailment ( Dagan and Glickman , 2004 ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T .
Others+Using the basic solution proposed by ( Mehdad et al. , 2010 ) as a term of comparison , we experiment with different sources of multilingual lexical knowledge to address the following questions : ( 1 ) What is the potential of the existing multilingual lexical resources to approach CLTE ?
Others+These include , just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .
Others+For the sake of completeness , we report in this section also the results obtained adopting the “ basic solution ” proposed by ( Mehdad et al. , 2010 ) .
Others+Introduction Cross-lingual Textual Entailment ( CLTE ) has been proposed by ( Mehdad et al. , 2010 ) as an extension of Textual Entailment ( Dagan and Glickman , 2004 ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T .
Others+Using the basic solution proposed by ( Mehdad et al. , 2010 ) as a term of comparison , we experiment with different sources of multilingual lexical knowledge to address the following questions : ( 1 ) What is the potential of the existing multilingual lexical resources to approach CLTE ?
Others+These include , just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .
Others+For the sake of completeness , we report in this section also the results obtained adopting the “ basic solution ” proposed by ( Mehdad et al. , 2010 ) .
Others+the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( Negri and Mehdad , 2010 ) .
Others+We run TreeTagger ( Schmid , 1994 ) for tokenization , and used the Giza ( Och and Ney , 2003 ) to align the tokenized corpora at the word level .
Others+Lexical resources for TE and CLTE All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 ) , or applying logical inference ( Tatu and Moldovan , 2005 ) , or adopting transformation - based techniques ( Kouleykov and Magnini , 2005 ; Bar - Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference .
Others+After the extraction , pruning techniques ( Snover et al. , 2009 ) can be applied to increase the precision of the extracted paraphrases .
Others+One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( Yamada and Knight , 2001 ) .
Others+It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; Chen , 1996 ) .
Others+for their models ( Brown et al. , 1993b ) .
Others+for their models ( Brown et al. , 1993b ) .
Others+The co-occurrence relation can also be based on distance in a bitext space , which is a more general representations of bitext correspondence ( Dagan et al. , 1993 ; Resnik & Melamed , 1997 ) , or it can be restricted to words pairs that satisfy some matching predicate , which can be extrinsic to the model ( Melamed , 1995 ; Melamed , 1997 ) .
Others+Models of translational equivalence that are ignorant of indirect associations have " a tendency ... to be confused by collocates " ( Dagan et al. , 1993 ) .
Others+For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency n ( u , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) z , following ( Dunning , 1993 ) 2 .
Others+Co-occurrence With the exception of ( Fung , 1998b ) , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts ( Gale & Church , 1991 ; Kumano & Hirakawa , 1994 ; Fung , 1998a ; Melamed , 1995 ) .
Others+2We could just as easily use other symmetric " association " measures , such as ¢ 2 ( Gale & Church , 1991 ) or the Dice coefficient ( Smadja , 1992 ) .
Others+We induced a two - class word - to - word model of translational equivalence from 13 million words of the Canadian Hansards , aligned using the method in ( Gale & Church , 1991 ) .
Others+The co-occurrence relation can also be based on distance in a bitext space , which is a more general representations of bitext correspondence ( Dagan et al. , 1993 ; Resnik & Melamed , 1997 ) , or it can be restricted to words pairs that satisfy some matching predicate , which can be extrinsic to the model ( Melamed , 1995 ; Melamed , 1997 ) .
Others+Table look - up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including " crummy " MT on the World Wide Web ( Church & I - Iovy , 1993 ) , certain machine - assisted translation tools ( e.g. ( Macklovitch , 1994 ; Melamed , 1996b ) ) ,
Others+Fortunately , indirect associations are usually not difficult to identify , because they tend to be weaker than the direct associations on which they are based ( Melamed , 1996c ) .
Others+Table look - up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including " crummy " MT on the World Wide Web ( Church & I - Iovy , 1993 ) , certain machine - assisted translation tools ( e.g. ( Macklovitch , 1994 ; Melamed , 1996b ) ) ,
Others+Fortunately , indirect associations are usually not difficult to identify , because they tend to be weaker than the direct associations on which they are based ( Melamed , 1996c ) .
Others+The co-occurrence relation can also be based on distance in a bitext space , which is a more general representations of bitext correspondence ( Dagan et al. , 1993 ; Resnik & Melamed , 1997 ) , or it can be restricted to words pairs that satisfy some matching predicate , which can be extrinsic to the model ( Melamed , 1995 ; Melamed , 1997 ) .
Others+For example , frequent words are translated less consistently than rare words ( Melamed , 1997 ) .
Others+The co-occurrence relation can also be based on distance in a bitext space , which is a more general representations of bitext correspondence ( Dagan et al. , 1993 ; Resnik & Melamed , 1997 ) , or it can be restricted to words pairs that satisfy some matching predicate , which can be extrinsic to the model ( Melamed , 1995 ; Melamed , 1997 ) .
Others+For example , frequent words are translated less consistently than rare words ( Melamed , 1997 ) .
Others+The co-occurrence relation can also be based on distance in a bitext space , which is a more general representations of bitext correspondence ( Dagan et al. , 1993 ; Resnik & Melamed , 1997 ) , or it can be restricted to words pairs that satisfy some matching predicate , which can be extrinsic to the model ( Melamed , 1995 ; Melamed , 1997 ) .
Others+In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( Blunsom et al. 2009 ) 5 .
Others+In ( Blunsom et al. , 2009 ) , for Chinese - to - English translation , the Bayesian SCFG grammar only outperform the heuristic SCFG grammar by 0.1 BLEU points on NIST MT 2004 and 0.6 BLEU points on NIST MT 2005 in the NEWS domain .
Others+Burkett and Klein ( 2008 ) and Burkett et al. ( 2010 ) focused on joint parsing and alignment .
Others+Burkett and Klein ( 2008 ) and Burkett et al. ( 2010 ) focused on joint parsing and alignment .
Others+Burkett and Klein ( 2012 ) utilized a transformation - based method to learn a sequence of monolingual tree transformations for translation .
Others+The obtained SCFG is further used in a phrase - based and hierarchical phrase - based system ( Chiang , 2007 ) .
Others+Cohn and Blunsom ( 2009 ) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees .
Others+Because each rule r consists of a target tree fragment frag and a source string str in the model , we follow Cohn and Blunsom ( 2009 ) and decompose the prior probability P0 ( r | N ) into two factors as follows :
Others+Moreover , we will further conduct experiments to compare our methods with other relevant works , such as ( Cohn and Blunsom , 2009 ) and ( Burkett and Klein , 2012 ) .
Others+DeNero and Klein ( 2007 ) proposed this statement , and Cohn and Blunsom ( 2009 ) has verified it in their experiments with parse trees .
Others+Differently , Cohn and Blunsom ( 2009 ) designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment .
Others+Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree - based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .
Others+Using the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules 7 in terms of frontier nodes ( Galley et al. , 2004 ) .
Others+We only use the minimal GHKM rules ( Galley et al. , 2004 ) here to reduce the complexity of the sampler .
Others+Using the GHKM algorithm ( Galley et al. 2004 ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment .
Others+In the system , we extract both the minimal GHKM rules ( Galley et al. , 2004 ) , and the rules of SPMT Model 1 ( Galley et al. , 2006 ) with phrases up to length L = 5 on the source side .
Others+The system is implemented based on ( Galley et al. , 2006 ) and ( Marcu et al. 2006 ) .
Others+The statistical significance test is performed by the re-sampling approach ( Koehn , 2004 ) .
Others+Liu et al. ( 2012 ) re-trained the linguistic parsers bilingually based on word alignment .
Others+Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree - based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .
Others+Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree - based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .
Others+The system is implemented based on ( Galley et al. , 2006 ) and ( Marcu et al. 2006 ) .
Others+To build the above s2t system , we first use the parse tree , which is generated by parsing the English side of the bilingual data with the Berkeley parser ( Petrov et al. , 2006 ) .
Others+DeNero and Klein ( 2007 ) proposed this statement , and Cohn and Blunsom ( 2009 ) has verified it in their experiments with parse trees .
Others+This indicates that parse trees are usually not the optimal choice for training tree - based translation models ( Wang et al. , 2010 ) .
Others+Our previous work ( Zhai et al. , 2012 ) designed an EMbased method to construct unsupervised trees for tree - based translation models .
Others+Zollmann and Venugopal ( 2006 ) substituted the non-terminal X in hierarchical phrase - based model by extended syntactic categories .
Others+To create the baseline system , we use the opensource Joshua 4.0 system ( Ganitkevitch et al. , 2012 ) to build a hierarchical phrase - based ( HPB ) system , and a syntax - augmented MT ( SAMT ) 11 system ( Zollmann and Venugopal , 2006 ) respectively .
Others+Zollmann and Vogel ( 2011 ) further labeled the SCFG rules with POS tags and unsupervised word classes .
Others+From ( Zollmann and Vogel , 2011 ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags .
Others+The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ) .
Others+For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( Nakano et al. , 1994 ; Nakano and Shimazu , 1999 ) .
Extends+WIT features an incremental understanding method ( Nakano et al. , 1999b ) that makes it possible to build a robust and real - time system .
Extends+The priorities are used for disambiguating interpretation in the incremental understanding method ( Nakano et al. , 1999b ) .
Extends+Since the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( Nakano et al. , 1999a ) .
Extends+The priorities are used for disambiguating interpretation in the incremental understanding method ( Nakano et al. , 1999b ) .
Extends+WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( Nakano et al. , 1999b ) , a video - recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather information system ( Dohsaka et al. , 2000 ) .
Others+Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( Nakano , 1991 ) .
Others+The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ) .
Others+Another technique is automatic discovery of translations from parallel or non-parallel corpora ( Fung and Mckeown , 1997 ) .
Others+Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns — syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .
Others+Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns — syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .
Others+Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns — syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .
Others+Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns — syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .
Others+For the full parser , we use the one developed by Michael Collins ( Collins , 1996 ; Collins , 1997 ) — one of the most accurate full parsers around .
Others+Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns — syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .
Others+However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; Collins , 1997 ; Ratnaparkhi , 1997 ) .
Others+For the full parser , we use the one developed by Michael Collins ( Collins , 1996 ; Collins , 1997 ) — one of the most accurate full parsers around .
Others+Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; Greffenstette , 1993 ) .
Others+First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large - scale language processing applications including information extraction and text summarization ( Grishman , 1995 ; Appelt et al. , 1993 ) .
Others+would be chunked as follows ( Tjong Kim Sang and Buchholz , 2000 ) : [ NP He ] [ VP reckons ] [ NP the current account deficit ] [ VP will narrow ] [ PP This research is supported by NSF grants IIS - 9801638 , ITR - IIS - 0085836 and an ONR MURI Award .
Others+The first is the one used in the chunking competition in CoNLL - 2000 ( Tjong Kim Sang and Buchholz , 2000 ) .
Others+Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL - 2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .
Others+Table 2 shows the results on identifying all phrases — chunking in CoNLL2000 ( Tjong Kim Sang and Buchholz , 2000 ) terminology .
Others+Training was done on the Penn Treebank ( Marcus et al. , 1993 ) Wall Street Journal data , sections 02 - 21 .
Extends+Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns — syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .
Extends+The shallow parser used is the SNoW - based CSCL parser ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) .
Extends+Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL - 2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .
Others+Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns — syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .
Others+Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns — syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .
Others+However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; Collins , 1997 ; Ratnaparkhi , 1997 ) .
Others+Other works ( Kasper et al. , 1995 ; Becker and Lopez , 2000 ) convert HPSG grammars into LTAG grammars .
Others+There have been many studies on parsing techniques ( Poller and Becker , 1998 ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming / grammar - development environ1 In this paper , we use the term LTAG to refer to FBLTAG , if not confusing .
Others+Our group has developed a wide - coverage HPSG grammar for Japanese ( Mitsuishi et al. , 1998 ) , which is used in a high - accuracy Japanese dependency analyzer ( Kanayama et al. , 2000 ) .
Others+Our group has developed a wide - coverage HPSG grammar for Japanese ( Mitsuishi et al. , 1998 ) , which is used in a high - accuracy Japanese dependency analyzer ( Kanayama et al. , 2000 ) .
Others+There have been many studies on parsing techniques ( Poller and Becker , 1998 ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming / grammar - development environ1 In this paper , we use the term LTAG to refer to FBLTAG , if not confusing .
Others+Tateisi et al. also translated LTAG into HPSG ( Tateisi et al. , 1998 ) .
Others+Introduction This paper describes an approach for sharing resources in various grammar formalisms such as Feature - Based Lexicalized Tree Adjoining Grammar ( FB - LTAG1 ) ( Vijay - Shanker , 1987 ; Vijay - Shanker and Joshi , 1988 ) and Head - Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .
Others+FBLTAG ( Vijay - Shanker , 1987 ; Vijay - Shanker and Joshi , 1988 ) is an extension of the LTAG formalism .
Others+For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example - base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al - Adhaileh & Tang , 1999 ) .
Others+Al - Adhaileh and Tang ( 2001 ) presented an approach for constructing a BKB based on the S - SSTC .
Others+In this paper , a flexible annotation schema called Structured String - Tree Correspondence ( SSTC ) ( Boitet & Zaharin , 1988 ) will be introduced to capture a natural language text , its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two .
Others+The SSTC is a general structure that can associate an arbitrary tree structure to string in a language as desired by the annotator to be the interpretation structure of the string , and more importantly is the facility to specify the correspondence between the string and the associated tree which can be nonprojective ( Boitet & Zaharin , 1988 ) .
Others+Towards this aim , a flexible annotation structure called Structured String - Tree Correspondence ( SSTC ) was introduced in Boitet & Zaharin ( 1988 ) to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the sub-correspondences recorded as part of a SSTC .
Others+Towards this aim , a flexible annotation structure called Structured String - Tree Correspondence ( SSTC ) was introduced in Boitet & Zaharin ( 1988 ) to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the sub-correspondences recorded as part of a SSTC .
Others+For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example - base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al - Adhaileh & Tang , 1999 ) .
Others+For instance , when building translation units in EBMT approaches ( Richardson et al. , 2001 ) , ( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) , ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Sadler & Vendelmans , 1990 ) , etc. , where S - SSTC can be used to represent the entries of the BKB or when S - SSTC used as an annotation schema to find the translation correspondences ( lexical and structural correspondences ) for transferrules ’ extraction from parallel parsed corpus ( Menezes & Richardson , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) and ( kaji et al. , 1992 ) .
Others+Figure 10 shows an example from Menezes and Richardson ( 2001 ) , the logical form for the SpanishEnglish pair : ( “ En Información del hipervínculo , haga clic en la dirección del hipervínculo ” , “ Under Hyperlink Information , click the hyperlink address ” ) .
Others+For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example - base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al - Adhaileh & Tang , 1999 ) .
Others+For instance , when building translation units in EBMT approaches ( Richardson et al. , 2001 ) , ( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) , ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Sadler & Vendelmans , 1990 ) , etc. , where S - SSTC can be used to represent the entries of the BKB or when S - SSTC used as an annotation schema to find the translation correspondences ( lexical and structural correspondences ) for transferrules ’ extraction from parallel parsed corpus ( Menezes & Richardson , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) and ( kaji et al. , 1992 ) .
Others+For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example - base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al - Adhaileh & Tang , 1999 ) .
Others+For instance , when building translation units in EBMT approaches ( Richardson et al. , 2001 ) , ( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) , ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Sadler & Vendelmans , 1990 ) , etc. , where S - SSTC can be used to represent the entries of the BKB or when S - SSTC used as an annotation schema to find the translation correspondences ( lexical and structural correspondences ) for transferrules ’ extraction from parallel parsed corpus ( Menezes & Richardson , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) and ( kaji et al. , 1992 ) .
Others+Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( Rambow & Satta , 1996 ) .
Others+Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( Rambow & Satta , 1996 ) , such as the relation between syntax and semantic .
Others+For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example - base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al - Adhaileh & Tang , 1999 ) .
Others+For instance , when building translation units in EBMT approaches ( Richardson et al. , 2001 ) , ( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) , ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Sadler & Vendelmans , 1990 ) , etc. , where S - SSTC can be used to represent the entries of the BKB or when S - SSTC used as an annotation schema to find the translation correspondences ( lexical and structural correspondences ) for transferrules ’ extraction from parallel parsed corpus ( Menezes & Richardson , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) and ( kaji et al. , 1992 ) .
Others+For instance , when building translation units in EBMT approaches ( Richardson et al. , 2001 ) , ( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) , ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Sadler & Vendelmans , 1990 ) , etc. , where S - SSTC can be used to represent the entries of the BKB or when S - SSTC used as an annotation schema to find the translation correspondences ( lexical and structural correspondences ) for transferrules ’ extraction from parallel parsed corpus ( Menezes & Richardson , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) and ( kaji et al. , 1992 ) .
Others+For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example - base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al - Adhaileh & Tang , 1999 ) .
Others+For instance , when building translation units in EBMT approaches ( Richardson et al. , 2001 ) , ( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) , ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Sadler & Vendelmans , 1990 ) , etc. , where S - SSTC can be used to represent the entries of the BKB or when S - SSTC used as an annotation schema to find the translation correspondences ( lexical and structural correspondences ) for transferrules ’ extraction from parallel parsed corpus ( Menezes & Richardson , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) and ( kaji et al. , 1992 ) .
Others+Recent work ( Banko and Brill , 2001 ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data .
Others+There have already been several attempts to develop distributed NLP systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( Hacioglu and Pellom , 2003 ) .
Others+The T N T POS tagger ( Brants , 2000 ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .
Others+The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ) .
Others+The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( Curran and Clark , 2003 ; Clark et al. , 2003 ) .
Others+The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ) .
Others+The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( Curran and Clark , 2003 ; Clark et al. , 2003 ) .
Others+Recent work ( Banko and Brill , 2001 ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data .
Others+The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ) .
Others+Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( Cunningham et al. , 1997 ) and the Alembic Workbench ( Day et al. , 1997 ) ) as well as NLP tools and resources that can be manipulated from the GUI .
Others+For example , the suite of LT tools ( Mikheev et al. , 1999 ; Grover et al. , 2000 ) perform tokenization , tagging and chunking on XML marked - up text directly .
Others+For example , 10 million words of the American National Corpus ( Ide et al. , 2002 ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( Marcus et al. , 1993 ) , currently used for training POS taggers .
Extends+Finally , the Natural Language Toolkit ( NLTK ) is a package of NLP components implemented in Python ( Loper and Bird , 2002 ) .
Extends+It has already been used to implement a framework for teaching NLP ( Loper and Bird , 2002 ) .
Others+An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ) .
Others+For example , 10 million words of the American National Corpus ( Ide et al. , 2002 ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( Marcus et al. , 1993 ) , currently used for training POS taggers .
Others+Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( Ngai and Florian , 2001 ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( Roche and Schabes , 1997 ) .
Others+Similarly , ( Barzilay and Lee , 2003 ) and ( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .
Others+While corpus driven efforts along the PAR SEVAL lines ( Black et al. , 1991 ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation .
Others+Thus for instance , ( Copestake and Flickinger , 2000 ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .
Others+Thus for instance , ( Copestake and Flickinger , 2000 ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .
Others+The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; Copestake et al. , 2001 ) .
Others+Semantic construction proceeds from the derived tree ( Gardent and Kallmeyer , 2003 ) rather than – as is more common in TAG – from the derivation tree .
Others+For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( Pereira et al. , 1993 ; Lin , 1998 ) .
Others+For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( Pereira et al. , 1993 ; Lin , 1998 ) .
Others+results for other acquisition tasks when compared to existing statistical techniques ( Bouillon et al. , 2002 ) .
Others+However , most strategies are based on internal or external methods ( Grabar and Zweigenbaum , 2002 ) , i.e. methods that rely on the form of terms or on the information gathered from contexts .
Others+More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : Daille ( 2003 ) uses derivational morphology ; Grabar and Zweigenbaum ( 2002 ) use , as a starting point , a number of identical characters .
Others+This approach , which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( Habert et al. , 1996 , for example ) , does not specify the relationship itself .
Others+Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( Pearce , 2002 ) for a review ) , these patterns allow :
Others+Both systems are built around from the maximum - entropy technique ( Berger et al. , 1996 ) .
Others+The principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ) , the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( Berger et al. , 1996 ) .
Others+where mk is one mention in entity e , and the basic model building block PˆL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( Berger et al. , 1996 ) .
Others+Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( Luo et al. , 2004 ) .
Others+The context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( Florian et al. , 2004 ) .
Others+Semantic similarity is typically defined via the lexical relations of synonymy ( automobile – car ) and hypernymy ( vehicle – car ) , while semantic relatedness ( SR ) is defined to cover any kind of lexical or functional association that may exist between two words ( Gurevych , 2005 ) .3 Dissimilar words can be semantically related , e.g. via functional relationships ( night – dark ) or when they are antonyms ( high – low ) .
Others+Gurevych ( 2005 ) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .
Others+We used the revised experimental setup ( Gurevych , 2005 ) , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs .
Others+Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary - based ( Lesk , 1986 ) , ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .
Others+Morris and Hirst ( 2004 ) pointed out that many relations between words in a text are non-classical ( i.e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity .
Others+In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im Walde and Melinger , 2005 ) .
Others+Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary - based ( Lesk , 1986 ) , ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .
Others+Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary - based ( Lesk , 1986 ) , ontology - based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information - based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .
Others+As has been previously observed and exploited in the NLP literature ( Pang and Lee , 2004 ; Agarwal and Bhattacharyya , 2005 ; Barzilay and Lapata , 2005 ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .
Others+Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( Daelemans and Hoste , 2002 ; Munson et al. , 2005 ) .
Others+More sophisticated approaches have been proposed ( Hillard et al. , 2003 ) , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments ( Galley et al. , 2004 ) .
Others+A few others incorporate various measures of inter-document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) .
Others+Previous sentiment - analysis work in different domains has considered inter-document similarity ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) or explicit
Others+More sophisticated approaches have been proposed ( Hillard et al. , 2003 ) , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments ( Galley et al. , 2004 ) .
Others+Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( Daelemans and Hoste , 2002 ; Munson et al. , 2005 ) .
Others+As has been previously observed and exploited in the NLP literature ( Pang and Lee , 2004 ; Agarwal and Bhattacharyya , 2005 ; Barzilay and Lapata , 2005 ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .
Others+A few others incorporate various measures of inter-document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) .
Others+Previous sentiment - analysis work in different domains has considered inter-document similarity ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) or explicit
Others+In particular , since we treat each individual speech within a debate as a single “ document ” , we are considering a version of document - level sentiment - polarity classification , namely , automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; Turney , 2002 ; Dave et al. , 2003 ) .
Others+In our experiments , we employed the well - known classifier SVMlight to obtain individual - document classification scores , treating Y as the positive class and using plain unigrams as features .5 Following standard practice in sentiment analysis ( Pang et al. , 2002 ) , the input to SVMlight consisted of normalized presence - of - feature ( rather than frequency - of - feature ) vectors .
Others+Also relevant is work on the general problems of dialog - act tagging ( Stolcke et al. , 2000 ) , citation analysis ( Lehnert et al. , 1990 ) , and computational rhetorical analysis ( Marcu , 2000 ; Teufel and Moens , 2002 ) .
Others+In particular , since we treat each individual speech within a debate as a single “ document ” , we are considering a version of document - level sentiment - polarity classification , namely , automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; Turney , 2002 ; Dave et al. , 2003 ) .
Others+Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , Hearst ( 1992 ) , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .
Others+Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion - oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , Hearst ( 1992 ) , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .
Others+Baroni and Bernardini ( 2004 ) built a corpus by iteratively searching Google for a small set of seed terms .
Others+Curran ( 2003 )
Others+Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by Fletcher ( 2004b ) and shingling techniques described by Chakrabarti ( 2002 ) .
Others+In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( Kennedy , 1998 : 56 ) unless the web is used as a corpus ( Kilgarriff and Grefenstette , 2003 ) .
Others+The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .
Others+• History - based feature models for predicting the next parser action ( Black et al. , 1992 ) .
Others+• Support vector machines for mapping histories to parser actions ( Kudo and Matsumoto , 2002 ) .
Others+• Graph transformations for recovering nonprojective structures ( Nivre and Nilsson , 2005 ) .
Others+The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by Nivre ( 2003 ) and extended to labeled dependency parsing by Nivre et al. ( 2004 ) .
Others+All experiments have been performed using MaltParser ( Nivre et al. , 2006 ) , version 0.4 , which is made available together with the suite of programs used for preand post - processing .1
Others+Building on the work of Ruch et al. ( 2003 ) in the same domain , we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models ( HMMs ) ; cfXXX ( Barzilay and Lee , 2004 ) .
Others+Following Ruch et al. ( 2003 ) and Barzilay and Lee ( 2004 ) , we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts .
Others+An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors ( log probabilities of observing entire sentences based on our language models ) , as opposed to sequences of terms , as done in ( Barzilay and Lee , 2004 ) .
Others+Although not the first to employ a generative approach to directly model content , the seminal work of Barzilay and Lee ( 2004 ) is a noteworthy point of reference and comparison .
Others+Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( McKeown , 1985 ; Marcu and Echihabi , 2002 ) .
Others+Our task is closer to the work of Teufel and Moens ( 2000 ) , who looked at the problem of intellectual attribution in scientific texts .
Others+role labelling ( Baker et al. , 1998 ; Kipper et al. , 2000 ; Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ; Atserias et al. , 2001 ; Shi and Mihalcea , 2005 ) .
Others+Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts ( Baker et al. , 1998 ) .
Others+Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .
Others+Most approaches rely on VerbNet ( Kipper et al. , 2000 ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; Shi and Mihalcea , 2005 ) .
Others+In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( Rosario and Hearst , 2001 ) or the system ( Gomez , 1998 ) .
Others+Encouraged by the success of chunk - based verb reordering lattices on ArabicEnglish ( Bisazza and Federico , 2010 ) , we tried to adapt the same approach to the German - English language pair .
Others+The flexible architecture we have presented enables interesting future research : ( i ) a straightforward improvement is the use of lexical similarity to reduce data sparseness , e.g. ( Basili et al. , 2005 ; Basili et al. , 2006 ; Bloehdorn et al. , 2006 ) .
Others+Future research should apply the work of Blunsom et al. ( 2008 ) and Blunsom and Osborne ( 2008 ) , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .
Others+As in this work English is the main source language , and as we have dealt with it as a target language already in Rapp & Zock ( 2010 ) , we do not use the respective English versions of these corpora here .
Others+Having dealt with this problem in Rapp & Zock ( 2010 ) we will not elaborate on it here , rather we will suggest a workaround .
Others+As suggested in Rapp & Zock ( 2010 ) this can be done by looking up the ranks of each of the four given words ( i.e. the words occurring in a particular word equation ) within the association vector of a translation candidate , and by multiplying these ranks .
Others+This contrasts with the findings described in Rapp & Zock ( 2010 ) where significant improvements could be achieved by increasing the number of source languages .
Others+Whereas Rapp & Zock ( 2010 ) dealt only with an English corpus , the current work shows that this methodology is applicable to a wide range of languages and corpora .
Others+These efforts include Tsuruoka and Tsujii ( 2003 ) , utilising dictionaries for protein detection by considering each dictionary entry using a novel distance measure , and Sasaki et al. ( 2008 ) , applying dictionaries to restrain the contexts in which proteins appear in text .
Others+A possible future direction would be to compare the query string to retrieved results using a method similar to that of Tsuruoka and Tsujii ( 2003 ) .
Extends+In our prior work ( Xiong and Litman , 2011 ) , we examined whether techniques used for predicting the helpfulness of product reviews ( Kim et al. , 2006 ) could be tailored to our peer - review domain , where the definition of helpfulness is largely influenced by the educational context of peer review .
Extends+( Details of how the average - expert model performs can be found in our prior work ( Xiong and Litman , 2011 ) . )
Extends+This work has mostly been for English , and there are issues , such as greater morphological complexity , in moving to other languages ( see , e.g. , de Ilarraza et al. , 2008 ; Dickinson et al. , 2010 ) .
Extends+Using a machine learner has the advantage of being able to perform well without a researcher having to specify rules , especially with the complex set of linguistic relationships motivating particle selection .2 We build from Dickinson et al. ( 2010 ) in two main ways : first , we implement a presence - selection pipeline that has proven effective for English preposition error detection ( cfXXX Gamon et al. , 2008 ) .
Extends+To acquire this , we use webbased corpora , as this allows us to find data similar to learner language , and using web as corpus ( WaC ) tools allows us to adjust parameters for new data ( Dickinson et al. , 2010 ) .
Extends+We follow our previous work ( Dickinson et al. , 2010 ) in our feature choices , using a fiveword window that includes the target stem and two words on either side for context ( see also Tetreault and Chodorow , 2008 ) .
Others+This process produces a hierarchical clustering of the word types in the corpus , and these clusterings have been found useful in many applications ( Ratinov and Roth , 2009 ; Koo et al. , 2008 ; Miller et al. , 2004 ) .
Others+Following Ratinov and Roth ( 2009 ) , we also compare the performance of our system with a system using features based on the Brown clusters of the word types in a document .
Others+It is inspired by the system described in Ratinov and Roth ( 2009 ) .
Others+This choice is inspired by recent work on learning syntactic categories ( Yatbaz et al. , 2012 ) , which successfully utilized such language models to represent word window contexts of target words .
Others+In the preposition model , priors for preposition preferences are learned from the shared task training data ( Rozovskaya and Roth , 2011 ) .
Others+Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced .
Others+Our motivation for generation of material for language education exists in work such as Sumita et al. ( 2005 ) and Mostow and Jang ( 2012 ) , which deal with automatic generation of classic fill in the blank questions .
Others+Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced .
Others+Another extension would be to incorporate the rough communicative goal of response to a previous sentence as in Wu et al. ( 2013 ) and attempt to produce in - vocab dialogs such as are ubiquitous in language education textbooks .
Others+Argument mining of online interactions , however , is still in its infancy ( Abbott et al. , 2011 ; Biran and Rambow , 2011 ; Yin et al. , 2012 ; Andreas et al. , 2012 ; Misra and Walker , 2013 ) .
Others+Another line of research that is correlated with ours is recognition of agreement / disagreement ( Misra and Walker , 2013 ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; Galley et al. , 2004 ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums .
Extends+And , yet , others assume that important sentences and clauses are derivable from a discourse representation of texts ( Ono et al. , 1994 ; Marcu , 1997a ; Marcu , 1997c ) .
Extends+For example , when the text shown in ( 1 ) , below , is given as input to the rhetorical parsing algorithm that is discussed in detail by Marcu ( 1997b ; 1997c ) , it is broken into ten elementary units ( those surrounded by square brackets ) .
Extends+And , yet , others assume that important sentences and clauses are derivable from a discourse representation of texts ( Ono et al. , 1994 ; Marcu , 1997a ; Marcu , 1997c ) .
Extends+The only disambiguation metric that we used in our previous work ( Marcu , 1997b ) was the shape - based metric , according to which the " best " trees are those that are skewed to the right .
Extends+The only disambiguation metric that we used in our previous work ( Marcu , 1997b ) was the shape - based metric , according to which the " best " trees are those that are skewed to the right .
Others+Arguably the most widely used is the mutual information ( Hindle , 1990 ; Church and Hanks , 1990 ; Dagan et al. , 1995 ; Luk , 1995 ; D. Lin , 1998a ) .
Others+Arguably the most widely used is the mutual information ( Hindle , 1990 ; Church and Hanks , 1990 ; Dagan et al. , 1995 ; Luk , 1995 ; D. Lin , 1998a ) .
Others+Hamming - type metrics ( Cardie , 1993 ; Zavrel and Daelemans , 1997 ) are intended for data with symbolic features , since they count feature label mismatches , whereas we are dealing feature Values that are probabilities .
Others+Variations of the value difference metric ( Stanfill and Waltz , 1986 ) have been employed for supervised disambiguation ( Ng and H.B. Lee , 1996 ; Ng , 1997 ) ; but it is not reasonable in language modeling to expect training data tagged with correct probabilities .
Others+The Dice coej ~ cient ( Smadja et al. , 1996 ; D. Lin , 1998a , 1998b ) is monotonic in Jaccard &apos;s coefficient ( van Rijsbergen , 1979 ) , so its inclusion in our experiments would be redundant .
Others+Smadja et al. ( 1996 ) observe that for two potential mutual translations X and Y , the fact that X occurs with translation Y indicates association ; X &apos; s occurring with a translation other t h a n Y decreases one &apos;s belief in their association ; but the absence of both X and Y yields no information .
Others+D. Lin ( 1997 ; 1998a ) takes an axiomatic approach to determining the characteristics of a good similarity measure .
Others+Recent comparisons of approaches that can be trained on corpora ( van Halteren et al. , 1998 ; Volk and Schneider , 1998 ) have shown that in most cases statistical aproaches ( Cutting et al. , 1992 ; Schmid , 1995 ; Ratnaparkhi , 1996 ) yield better results than finite - state , rule - based , or memory - based taggers ( Brill , 1993 ; Daelemans et al. , 1996 ) .
Others+Recent comparisons of approaches that can be trained on corpora ( van Halteren et al. , 1998 ; Volk and Schneider , 1998 ) have shown that in most cases statistical aproaches ( Cutting et al. , 1992 ; Schmid , 1995 ; Ratnaparkhi , 1996 ) yield better results than finite - state , rule - based , or memory - based taggers ( Brill , 1993 ; Daelemans et al. , 1996 ) .
Others+Additionally , we present results of the tagger on the NEGRA corpus ( Brants et al. , 1999 ) and the Penn Treebank ( Marcus et al. , 1993 ) .
Others+The annotation consists of four parts : 1 ) a context - free structure augmented with traces to mark movement and discontinuous constituents , 2 ) phrasal categories that are annotated as node labels , 3 ) a small set of grammatical functions that are annotated as extensions to the node labels , and 4 ) part - of - speech tags ( Marcus et al. , 1993 ) .
Others+The uses of this procedure include information retrieval ( Hearst and Plaunt , 1993 ; Hearst , 1994 ; Yaari , 1997 ; Reynar , 1999 ) , summarization ( Reynar , 1998 ) , text understanding , anaphora resolution ( Kozima , 1993 ) , language modelling ( Morris and Hirst , 1991 ; Beeferman et al. , 199717 ) and improving document navigation for the visually disabled ( Choi , 2000 ) .
Others+Methods for finding the topic boundaries include sliding window ( Hearst , 1994 ) , lexical chains ( Morris , 1988 ; Kan et al. , 1998 ) , dynamic programming ( Ponte and Croft , 1997 ; Heinonen , 1998 ) , agglomerative clustering ( Yaari , 1997 ) and divisive clustering ( Reynar , 1994 ) .
Others+Other performanc ( ; measures include the popular precision and recall metric ( PR ) ( Hearst , 1994 ) , fuzzy PR ( Reynar , 1998 ) and edit distance ( Ponte and Croft , 1997 ) .
Others+The uses of this procedure include information retrieval ( Hearst and Plaunt , 1993 ; Hearst , 1994 ; Yaari , 1997 ; Reynar , 1999 ) , summarization ( Reynar , 1998 ) , text understanding , anaphora resolution ( Kozima , 1993 ) , language modelling ( Morris and Hirst , 1991 ; Beeferman et al. , 199717 ) and improving document navigation for the visually disabled ( Choi , 2000 ) .
Others+The uses of this procedure include information retrieval ( Hearst and Plaunt , 1993 ; Hearst , 1994 ; Yaari , 1997 ; Reynar , 1999 ) , summarization ( Reynar , 1998 ) , text understanding , anaphora resolution ( Kozima , 1993 ) , language modelling ( Morris and Hirst , 1991 ; Beeferman et al. , 199717 ) and improving document navigation for the visually disabled ( Choi , 2000 ) .
Others+The uses of this procedure include information retrieval ( Hearst and Plaunt , 1993 ; Hearst , 1994 ; Yaari , 1997 ; Reynar , 1999 ) , summarization ( Reynar , 1998 ) , text understanding , anaphora resolution ( Kozima , 1993 ) , language modelling ( Morris and Hirst , 1991 ; Beeferman et al. , 199717 ) and improving document navigation for the visually disabled ( Choi , 2000 ) .
Extends+We present a new algorithm that builds on previous work by Reynar ( Reynar , 1998 ; Reynar , 1994 ) .
Extends+Methods for finding the topic boundaries include sliding window ( Hearst , 1994 ) , lexical chains ( Morris , 1988 ; Kan et al. , 1998 ) , dynamic programming ( Ponte and Croft , 1997 ; Heinonen , 1998 ) , agglomerative clustering ( Yaari , 1997 ) and divisive clustering ( Reynar , 1994 ) .
Others+hnplementations of this idea use word stem repetition ( Youmans , 1991 ; Reynar , 1994 ; Ponte and Croft , 1997 ) , context vectors ( Hearst , 1994 ; Yaari , 1997 ; Kaufmann , 1999 ; Eichmann et al. , 1999 ) , entity repetition ( Kan et al. , 1998 ) , semantic similarity ( Morris and Hirst , 1991 ; Kozima , 1993 ) , word
Others+Methods for finding the topic boundaries include sliding window ( Hearst , 1994 ) , lexical chains ( Morris , 1988 ; Kan et al. , 1998 ) , dynamic programming ( Ponte and Croft , 1997 ; Heinonen , 1998 ) , agglomerative clustering ( Yaari , 1997 ) and divisive clustering ( Reynar , 1994 ) .
Others+Multi-source methods combine lexical cohesion with other indicators of topic shift such as cue phrases , prosodic features , reference , syntax and lexical attraction ( Beeferman et al. , 1997a ) using decision trees ( Miike et al. , 1994 ; Kurohashi and Nagao , 1994 ; Litman and Passonneau , 1995 ) and probabilistic models ( Beeferman et al. , 1997b ; Hajime et al. , 1998 ; Reynar , 1998 ) .
Others+Multi-source methods combine lexical cohesion with other indicators of topic shift such as cue phrases , prosodic features , reference , syntax and lexical attraction ( Beeferman et al. , 1997a ) using decision trees ( Miike et al. , 1994 ; Kurohashi and Nagao , 1994 ; Litman and Passonneau , 1995 ) and probabilistic models ( Beeferman et al. , 1997b ; Hajime et al. , 1998 ; Reynar , 1998 ) .
Others+A tokenizer ( Grefenstette and Tapanainen , 1994 ) and a sentence boundary disambiguation algorithm ( Palmer and Hearst , 1994 ; Reynar and Ratnaparkhi , 1997 ) or EAGLE ( Reynar et al. , 1997 ) may be used to convert a plain text document into the acceptable input format .
Others+A tokenizer ( Grefenstette and Tapanainen , 1994 ) and a sentence boundary disambiguation algorithm ( Palmer and Hearst , 1994 ; Reynar and Ratnaparkhi , 1997 ) or EAGLE ( Reynar et al. , 1997 ) may be used to convert a plain text document into the acceptable input format .
Others+A tokenizer ( Grefenstette and Tapanainen , 1994 ) and a sentence boundary disambiguation algorithm ( Palmer and Hearst , 1994 ; Reynar and Ratnaparkhi , 1997 ) or EAGLE ( Reynar et al. , 1997 ) may be used to convert a plain text document into the acceptable input format .
Extends+The method is based on Reynar &apos;s maximisation algorithm ( Reynar , 1998 ; Helfman , 1996 ; Church , 1993 ; Church and Helfman , 1993 ) .
Others+Despite making such an assumption , this proves to be among the most accurate techniques in comparative studies of corpus - based word sense disambiguation methodologies ( e.g. , ( Leacock et al. , 1993 ) , ( Mooney , 1996 ) , ( Ng and Lee , 1996 ) , ( Pealersen and Bruce , 1997 ) ) .
Others+A similar finding has emerged in word sense disambiguation , where a number of comparative studies have all reported that no method achieves significantly greater accuracy than the Naive Bayesian classifier ( e.g. , ( Leacock et al. , 1993 ) , ( Mooney , 1996 ) , ( Ng and Lee , 1996 ) , ( Pedersen and Bruce , 1997 ) ) .
Others+Despite making such an assumption , this proves to be among the most accurate techniques in comparative studies of corpus - based word sense disambiguation methodologies ( e.g. , ( Leacock et al. , 1993 ) , ( Mooney , 1996 ) , ( Ng and Lee , 1996 ) , ( Pealersen and Bruce , 1997 ) ) .
Others+A similar finding has emerged in word sense disambiguation , where a number of comparative studies have all reported that no method achieves significantly greater accuracy than the Naive Bayesian classifier ( e.g. , ( Leacock et al. , 1993 ) , ( Mooney , 1996 ) , ( Ng and Lee , 1996 ) , ( Pedersen and Bruce , 1997 ) ) .
Others+Despite making such an assumption , this proves to be among the most accurate techniques in comparative studies of corpus - based word sense disambiguation methodologies ( e.g. , ( Leacock et al. , 1993 ) , ( Mooney , 1996 ) , ( Ng and Lee , 1996 ) , ( Pealersen and Bruce , 1997 ) ) .
Others+A similar finding has emerged in word sense disambiguation , where a number of comparative studies have all reported that no method achieves significantly greater accuracy than the Naive Bayesian classifier ( e.g. , ( Leacock et al. , 1993 ) , ( Mooney , 1996 ) , ( Ng and Lee , 1996 ) , ( Pedersen and Bruce , 1997 ) ) .
Others+In natural language processing , ensemble techniques have been successfully applied to p a r t of - speech tagging ( e.g. , ( Brill and Wu , 1998 ) ) and parsing ( e.g. , ( Henderson and Brill , 1999 ) ) .
Others+An alternative representation for baseNPs has been put forward by ( Ramshaw and Marcus , 1995 ) .
Others+1This ( Ramshaw and Marcus , 1995 ) baseNP data set is available via ftp://ftp.cis.upenn.edu/pub/chunker/ 2Software for generating the data is available from http://lcg-www.uia.ac.be/conl199/npb/
Others+They have used the ( Ramshaw and Marcus , 1995 ) representation as well ( IOB1 ) .
Others+Both ( Mufioz et al. , 1999 ) and ( Tjong Kim Sang and Veenstra , 1999 ) have shown how classifiers can process bracket structures .
Others+Their results can be converted to baseNPs by making pairs of open and close brackets with large probability scores ( Mufioz et al. , 1999 ) or by regarding only the shortest phrases between open and close brackets as baseNPs ( Tjong Kim Sang and Veenstra , 1999 ) .
Others+( Argamon et al. , 1998 ) use Memory - Based Sequence Learning for recognizing both NP chunks and VP chunks .
Others+( Cardie and Pierce , 1998 ) store POS tag sequences that make up complete chunks and use these sequences as rules for classifying unseen data .
Others+Using finite - state methods , it has been possible to describe both word formation and the concomitant phonological modifications in many languages , ranging from straightforward concatenative combination ( Koskenniemi , 1983 ) over Semitic - style nonconcatenative intercalation ( Beesley ( 1996 ) , Kiraz ( 1994 ) ) to circumfixional long - distance dependencies ( Beesley , 1998 ) .
Others+While it is possible to transfer much of the present proposal to the transducer - based setting that is often preferred nowadays , the monostratal approach still offers an attractive alternative due to its easy blend with monostratal grammars such as HPSG and the good prospects for machine learning of its surfacetrue constraints ( Ellison ( 1992 ) , Belz ( 1998 ) ) .
Others+Similar advances have been made in machine translation ( Frederking and Nirenburg , 1994 ) , speech recognition ( Fiscus , 1997 ) and named entity recognition ( Borthwick et al. , 1998 ) .
Others+Similar advances have been made in machine translation ( Frederking and Nirenburg , 1994 ) , speech recognition ( Fiscus , 1997 ) and named entity recognition ( Borthwick et al. , 1998 ) .
Others+The corpus - based statistical parsing community has many fast and accurate automated parsing systems , including systems produced by Collins ( 1997 ) , Charniak ( 1997 ) and Ratnaparkhi ( 1997 ) .
Others+The corpus - based statistical parsing community has many fast and accurate automated parsing systems , including systems produced by Collins ( 1997 ) , Charniak ( 1997 ) and Ratnaparkhi ( 1997 ) .
Others+These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus ( Marcus et al. , 1993 ) .
Others+The work of Efron and Tibshirani ( 1993 ) enabled Breiman &apos;s refinement and application of their techniques for machine learning ( Breiman , 1996 ) .
Others+The training set for these experiments was sections 01 - 21 of the Penn Treebank ( Marcus et al. , 1993 ) .
Others+The parser induction algorithm used in all of the experiments in this paper was a distribution of Collins &apos;s model 2 parser ( Collins , 1997 ) .
Others+Haruno et al. ( 1998 ) used boosting to produce more accurate classifiers which were embedded as control
Others+Abney et al. ( 1999 ) performed part - of - speech tagging and prepositional phrase attachment using AdaBoost as a core component .
Others+Abney et al. ( 1999 ) showed a similar corpus analysis technique for part of speech tagging and prepositional phrase tagging , but for parsing we must remove errors introduced by the parser as we did in Section 3.3.2 before questioning the corpus quality .
Others+CommandTalk ( Moore et al. , 1997 ) , Circuit Fix - It Shop ( Smith , 1997 ) and Tl : tAINS - 96 ( Traum and Allen , 1994 ; Traum and Andersen , 1999 ) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents .
Others+CommandTalk ( Moore et al. , 1997 ) , Circuit Fix - It Shop ( Smith , 1997 ) and Tl : tAINS - 96 ( Traum and Allen , 1994 ; Traum and Andersen , 1999 ) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents .
Extends+CommandTalk ( Moore et al. , 1997 ) , Circuit Fix - It Shop ( Smith , 1997 ) and Tl : tAINS - 96 ( Traum and Allen , 1994 ; Traum and Andersen , 1999 ) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents .
Extends+The speech and language processing architecture is based on that of the SRI CommandTalk system ( Moore et al. , 1997 ; Stent et al. , 1999 ) .
Extends+The speech and language processing architecture is based on that of the SRI CommandTalk system ( Moore et al. , 1997 ; Stent et al. , 1999 ) .
Others+Initial language processing is carried out Using the SRI Gemini system ( Dowding et al. , 1993 ) , using a domain - independent unification .
Others+There are more sophisticated surface generation packages , such as FUF / SURGE ( Elhadad and Robin , 1996 ) , KPML ( Bateman , 1996 ) , MUMBLE ( Meteer et al. , 1987 ) , and RealPro ( Lavoie and Rambow , 1997 ) , which produce natural language text from an abstract semantic representation .
Others+There are more sophisticated surface generation packages , such as FUF / SURGE ( Elhadad and Robin , 1996 ) , KPML ( Bateman , 1996 ) , MUMBLE ( Meteer et al. , 1987 ) , and RealPro ( Lavoie and Rambow , 1997 ) , which produce natural language text from an abstract semantic representation .
Others+The only trainable approaches ( known to the author ) to surface generation are the purely statistical machine translation ( MT ) systems such as ( Berger et al. , 1996 ) and the corpus - based generation system described in ( Langkilde and Knight , 1998 ) .
Others+Unless the desired set of GRs matches the set already annotated in some large training corpus ( e.g. , the Buchholz et al. ( 1999 ) GR finder used the GRs annotated in the Penn Treebank ( Marcus et al. , 1993 ) ) , one will have to either manually write rules to find the GRs or annotate a training corpus for the desired set .
Others+Ferro et al. ( 1999 ) and Buchholz et al. ( 1999 ) both describe learning systems to find GRs .
Others+One can not directly compare the two systems from the descriptions given in Ferro et al. ( 1999 ) and Buchholz et al. ( 1999 ) , as the results in the descriptions were based on different data sets and on different assumptions of what is known and what needs to be found .
Others+Unless the desired set of GRs matches the set already annotated in some large training corpus ( e.g. , the Buchholz et al. ( 1999 ) GR finder used the GRs annotated in the Penn Treebank ( Marcus et al. , 1993 ) ) , one will have to either manually write rules to find the GRs or annotate a training corpus for the desired set .
Others+The former ( TR ) uses transformation - based error - driven learning ( Brill and Resnik , 1994 ) and the latter ( MB ) uses memory - based learning ( Daelemans et al. , 1999 ) .
Others+These tests have previously used on such functions during the “ message understanding ” ( MUC ) evaluations ( Chinchor et al. , 1993 ) .
Others+Church and Mercer ( 1993 ) give some examples of dependence between test set instances in natural language .
Others+Two baseNP data sets have been put forward by Ramshaw and Marcus ( 1995 ) .
Others+An alternative representation for baseNPs has been put forward by Ramshaw and Marcus ( 1995 ) .
Others+The main data set consist of four sections of the Wall Street Journal ( WSJ ) part of the Penn Treebank ( Marcus et al. , 1993 ) as training material ( sections 15 - 18 , 211727 tokens ) and one section as test material ( section 20 , 47377 tokens ) 1 .
Others+For further processing steps we have to introduce the concept of alignment ( Brown et al. , 1990 ) .
Extends+Then we add the ( Samuelsson , 1997 ) .
Others+The grammars were induced from sections 2 - 21 of the Penn Wall St. Journal Treebank ( Marcus et al. , 1993 ) , and tested on section 23 .
Others+As Resnik ( 1997 ) and Abney and Light ( 1999 ) have found , the main problem these systems face is the presence of ambiguous words in the training data .
Others+As Resnik ( 1997 ) and Abney and Light ( 1999 ) have found , the main problem these systems face is the presence of ambiguous words in the training data .
Others+Abney and Light 's approach Abney and Light ( 1999 ) pointed out that the distribution of senses of an ambiguous word is not uniform .
Others+Because the emission probability of a PCFG production can be regarded as an annotation on a CFG production , the left - corner transform can produce a CFG with weighted productions which assigns the same probabilities to strings and transformed trees as the original grammar ( Abney et al. , 1999 ) .
Others+This may be responsible for the observation that exhaustive left - corner parsing is less ecient than top - down parsing ( Covington , 1994 ) .
Others+After this paper was accepted for publication we learnt of Moore ( 2000 ) , which addresses the issue of grammar size using very similar techniques to those proposed here .
Others+B 2 L Moore ( 2000 ) introduces a version of the leftcorner transform called LCLR , which applies only to productions with left - recursive parent and left child categories .
Others+Moore ( 2000 ) suggests an additional constraint on nonterminals D { X that can appear in useful productions of LC L ( G ) : D must either be the start symbol of G or else appear in a production A !
Others+lective left - corner transform is followed by a onestep - removal transform ( i.e. , composition or partial evaluation of schema 1b with respect to schema 1d ( Johnson , 1998a ; Abney and Johnson , 1991 ; Resnik , 1992 ) ) , each top - down production from G appears unchanged in the nal grammar .
Others+A stochastic top - down parser using the PCFG estimated from the trees produced by TL simulates a stochastic generalized left - corner parser , which is a generalization of a standard stochastic left - corner parser that permits productions to be recognized top - down as well as left - corner ( Manning and Carpenter , 1997 ) .
Others+The state of a left - corner parser does capture some linguistic generalizations ( Manning and Carpenter , 1997 ; Roark and Johnson , 1999 ) , but one might still expect sparse - data problems .
Others+As reported previously , the standard left - corner grammar embeds sucient non-local information in its productions to signi cantly improve the labelled precision and recall of its MLPs with respect to MLPs of the PCFG estimated from the untransformed trees ( Manning and Carpenter , 1997 ; Roark and Johnson , 1999 ) .
Others+Ultimately , however , it seems that a more complex approach incorporating back - o and smoothing is necessary in order to achieve the parsing accuracy achieved by Charniak ( 1997 ) and Collins ( 1997 ) .
Others+Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; Greffenstette , 1993 ) .
Others+These problems formulations are similar to those studied in ( Ramshaw and Marcus , 1995 ) and ( Church , 1988 ; Argamon et al. , 1998 ) , respectively .
Others+These problems formulations are similar to those studied in ( Ramshaw and Marcus , 1995 ) and ( Church , 1988 ; Argamon et al. , 1998 ) , respectively .
Others+This approach has been studied in ( Church , 1988 ; Argamon et al. , 1998 ) .
Others+Of the several slightly different definitions of a base NP in the literature we use for the purposes of this work the definition presented in ( Ramshaw and Marcus , 1995 ) and used also by ( Argamon et al. , 1 9 9 8 ) a n d others .
Others+These problems formulations are similar to those studied in ( Ramshaw and Marcus , 1995 ) and ( Church , 1988 ; Argamon et al. , 1998 ) , respectively .
Others+Of the several slightly different definitions of a base NP in the literature we use for the purposes of this work the definition presented in ( Ramshaw and Marcus , 1995 ) and used also by ( Argamon et al. , 1 9 9 8 ) a n d others .
Others+This approach has been studied in ( Ramshaw and Marcus , 1995 ) .
Others+In order to be able to compare our results with the results obtained by other researchers , we worked with the same data sets already used by ( Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ) for NP and SV detection .
Others+We compared this other results reported using the Inside / Outside with the same experiment , only this time withmethod ( Ramshaw and Marcus , 1995 ) ( see Taout incorporating the features from open brackble 7 .
Others+Perhaps this was not observed earlier since ( Ramshaw and Marcus , 1995 ) studied only base NPs , most of which are short .
Others+SV phrases , following the definition suggested in ( Argamon et al. , 1998 ) , are word phrases starting with the subject of the sentence and ending with the first verb , excluding modal 2Notice that according to this definition the identified verb may not correspond to the subject , but this phrase still contains meaningful information ; in any case , the learning method presented is independent of the specific
Others+These problems formulations are similar to those studied in ( Ramshaw and Marcus , 1995 ) and ( Church , 1988 ; Argamon et al. , 1998 ) , respectively .
Others+SV phrases , following the definition suggested in ( Argamon et al. , 1998 ) , are word phrases starting with the subject of the sentence and ending with the first verb , excluding modal 2Notice that according to this definition the identified verb may not correspond to the subject , but this phrase still contains meaningful information ; in any case , the learning method presented is independent of the specific
Others+This approach has been studied in ( Church , 1988 ; Argamon et al. , 1998 ) .
Others+In order to be able to compare our results with the results obtained by other researchers , we worked with the same data sets already used by ( Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ) for NP and SV detection .
Others+As the overall accuracy for the close bracket prereported in ( Argamon et al. , 1998 ) , most base dictor is very high .
Others+Extracting semantic information from word co-occurrence statistics has been effective , particularly for sense disambiguation ( Schiitze , 1992 ; Gale et al. , 1992 ; Yarowsky , 1995 ) .
Others+In Riloff and Shepherd ( 1997 ) , noun co-occurrence statistics were used to indicate nominal category membership , for the purpose of aiding in the construction of semantic lexicons .
Others+To identify conjunctions , lists , and appositives , we first parsed the corpus , using an efficient statistical parser ( Charniak et al. , 1998 ) , trMned on the Penn Wall Street Journal Treebank ( Marcus et al. , 1993 ) .
Others+For the final ranking , we chose the log likelihood statistic outlined in Dunning ( 1993 ) , which is based upon the co-occurrence counts of all nouns ( see Dunning for details ) .
Others+This is done , in the spirit of the Dependency Model of Lauer ( 1995 ) , by selecting the noun to its right in the compound with the highest probability of occuring with the word in question when occurring in a noun compound .
Others+However , the best performing statistical approaches to lexical ambiguity resolution themselves rely on complex information sources such as lemmas , inected forms , parts of speech and arbitrary word classes [ : : : ] local and distant collocations , trigram sequences , and predicate argument association ( Yarowsky ( 1995 ) , p. 190 ) or large context - windows up to 1000 neighboring words ( Schütze , 1992 ) .
Others+We evaluated this simple method on a large number of real - world translations and got results comparable to related approaches such as that of Dagan and Itai ( 1994 ) where much more selectional information is used .
Others+Machine learning based classifiers and maximum entropy models which , in principle , are not restricted to features of these forms have used them nevertheless , perhaps under the influence of probabilistic methods ( Brill , 1995 ; Yarowsky , 1994 ; Ratnaparkhi et al. , 1994 ) .
Others+Machine learning based classifiers and maximum entropy models which , in principle , are not restricted to features of these forms have used them nevertheless , perhaps under the influence of probabilistic methods ( Brill , 1995 ; Yarowsky , 1994 ; Ratnaparkhi et al. , 1994 ) .
Others+Machine learning based classifiers and maximum entropy models which , in principle , are not restricted to features of these forms have used them nevertheless , perhaps under the influence of probabilistic methods ( Brill , 1995 ; Yarowsky , 1994 ; Ratnaparkhi et al. , 1994 ) .
Others+Efforts in this directions consists of ( 1 ) directly adding syntactic information , as in ( Chelba and Jelinek , 1998 ; Rosenfeld , 1996 ) , and ( 2 ) indirectly adding syntactic and semantic information , via similarity models ; in this case n - gram type features are used whenever possible , and when they can not be used ( due to data sparsity ) , additional information compiled into a similarity measure is used ( Dagan et al. , 1999 ) .
Others+This approach was used by Palmer ( 1997 ) for word segmentation .
Others+Our in Miller , who organized ... , this system is work adds an existing system to improve the trained to indicate that who is the subject rules learned , while Palmer ( 1997 ) adds rules of organized , while the Ferro et al. ( 1999 ) to improve an existing system &apos;s performance .
Others+We Hwa ( 1999 ) describes a somewhat similar apFerro et al. ( 1999 ) proach for nding parse brackets which comlearner on a small annotated training set by bines a fully annotated related training data using an existing system to provide initial set and a large but incompletely annotated GR annotations .
Others+Both these works deal on with just one ( word boundary ) or two ( start are reported in Buchholz et al. ( 1999 ) and and end parse bracket ) annotation label types Carroll et al. ( 1999 ) , respectively .
Others+The TSNLP project ( Lehmann and Oepen , 1996 ) and its successor DiET ( Netter et al. , 1998 ) , which built large multilingual testsuites , likewise fall into this category .
Others+For example , the work reported in ( Rayner and Samuelsson , 1994 ; Samuelsson , 1994 ) di ers from the one presented below in several aspects : They induce a grammar from a treebank , while I propose to annotate the grammar based on all solutions it produces .
Others+cooccurrence smoothing is given by Finally , while we have used our similarity model only for missing bigrams in a back - off scheme , Essen and Steinbiss ( 1992 ) used linear interpolation for all bigrams to combine the cooccurrence smoothing model with MLE models of bigrams and unigrams .
Others+cooccurrence smoothing is given by Finally , while we have used our similarity model only for missing bigrams in a back - off scheme , Essen and Steinbiss ( 1992 ) used linear interpolation for all bigrams to combine the cooccurrence smoothing model with MLE models of bigrams and unigrams .
Others+4This presentation corresponds to model 2 - B in Essen and Steinbiss ( 1992 ) .
Others+This m a y require an adjustment of the similarity based estimate , possibly along the lines of ( Rosenfeld and Huang , 1992 ) .
Others+Other types of conditional cooccurrence probabilities have been used in probabilistic parsing ( Black et al. , 1993 ) .
Others+It has been noted in previous work that the felicity of certain forms of ellipsis is dependent on the type of coherence relationship extant between the antecedent and elided clauses ( Levin and Prince , 1982 ; Kehler , 1993b ) .
Others+In a previous paper ( Kehler , 1993b ) , five contexts for VP - ellipsis were examined to determine whether the representations retrieved are syntactic or semantic in nature .
Others+4It has been noted that in gapping constructions , contrastive accent is generally placed on parallel elements in 2These examples have been taken or adapted from Kehler ( 1993b ) .
Others+The analysis accounts for the range of data given in Kehler ( 1993b ) , although one point of departure exists between that account and the current one with respect to clauses conjoined with but .
Others+lZThese examples have been adapted from several in Kehler ( 1993b ) .
Others+We illustrate the relevant syntactic and semantic properties of these forms using the version of Categorial Semantics described in Pereira ( 1990 ) .
Others+This process is reliant on performing comparison and generalization operations on the corresponding representations ( Scha and Polanyi , 1988 ; Hobbs , 1990 ; Priist , 1992 ; Asher , 1993 ) .
Others+The parsers create parse forests ( Tomita , 1987 ) that incorporate subtree sharing ( in which identical sub-analyses are shared between differing superordinate analyses ) and node packing ( where subanalyses covering the same portion of input whose root categories are in a subsumption relationship are merged into a single node ) .
Others+THE The three parsers in this study are : a bottomup left - corner parser , a ( non-deterministic ) LR parser , and an LR - like parser based on an algorithm devised by Schabes ( 1991 ) .
Others+The Compiled - Earley ( CE ) parser is based on a predictive chart - based CF parsing algorithm devised by Schabes ( 1991 ) which is driven by a table compiling out the predictive component of Earley 's ( 1970 ) parser .
Others+Although Schabes ( 1991:107 ) claims that the problem of exponential grammar complexity " is particularly acute for natural language processing since in this context the input length is typically small ( 10 - 20 words ) and the grammar size very large ( hundreds or thousands of rules and symbols ) " , the experiments indicate that , with a widecoverage NL grammar , inputs of this length can be parsed quite quickly ; however , longer inputs ( of more than about 30 words in length ) -- which occur relatively frequently in written text -- are a problem .
Others+However , in the unification versions , on each reduce action the daughters of the rule involved have to be unified with every possible alternative sequence of the sub-analyses that are being consumed by the rule The grammar - dependent complexity of the LR parser makes it also appear intractable : Johnson ( 1989 ) shows that the number of LR ( 0 ) states for certain ( pathological ) grammars is exponentially related to the size of the grammar , and that there are some inputs which force an LR parser to visit all of these states in the course of a parse .
Others+Hindle ( 1990 ) proposed dealing with the
Others+More recently , we have constructed similar tables with the help of a statistical part - of - speech tagger ( Church , 1988 ) and of tools for regular expression pattern matching on tagged corpora ( Yarowsky , 1992 ) .
Others+Such approaches are not new , witness the statistical approach to machine translation suggested by Weaver ( 1955 ) , but the current level of interest is largely due to the success of applying hidden Markov models and N - gram language models in speech recognition .
Others+Second , there is a class of techniques for learning rules from text , a recent example being Brill 1993 .
Others+' l'lmse alignment relations are similar in some respects to the alignments used by Brown et al. ( 1990 ) in their surface translation model .
Others+Chang and Su 1993 ) :
Others+Although we have not provided a denotatio.al semantics for sets of relation edges , we anticipate that this will be possible along the lines developed in m ( motonic semantics ( Alshawi and Crouch 1992 ) .
Others+This is mainly due to the clear theoretical and practical advantages of bidirectional grammar use ( see , among others , Appelt , 1987 ) .
Others+Shieber ( 1988 ) gave the first use of Earley &apos;s algorithm for generation , but this algorithm does not * The p r e s e n t e d research was s p o n s o r e d by &apos; l ~ eilprojekt B4 &quot; Constraints on Grammar for Efficient Generation &quot; of the Sonderforschungsbereich 340 &quot; Sprachtheoretische Grundlagen fiir die Computerllnguistik &quot; of the Deutsche Forschungsgemeinschaft .
Others+Shieber ( 1988 ) gave the first use of Earley &apos;s algorithm for generation , but this algorithm does not * The p r e s e n t e d research was s p o n s o r e d by &apos; l ~ eilprojekt B4 &quot; Constraints on Grammar for Efficient Generation &quot; of the Sonderforschungsbereich 340 &quot; Sprachtheoretische Grundlagen fiir die Computerllnguistik &quot; of the Deutsche Forschungsgemeinschaft .
Others+However , both approaches can just as well take a declarative grammar specification as input to produce generator and / or parser - oriented grammars as in Dymetman et al. ( 1990 ) .
Others+This testgrammar is based on the implementation of an analysis of partial vP topicalization in German ( Hinrichs et al. , 1994 ) in the Troll system ( Gerdemann and King , 1994 ) .
Others+The interaction of morphological analysis with spelling correction ( Carter , 1992 ; Oflazer , 1994 ; Bowden , 1995 ) is another possibly fruitful area of work .
Others+SRs may have different applications in NLP , specifically , they may help a parser with Word Sense Selection ( WSS , as in ( Hirst , 1987 ) ) , with preferring certain structures out of several g r a m m a t i c a l ones ( Whittemore et al. , 1990 ) and finally with deciding the semantic role played by a syntactic complement ( Basili et al. , 1992 ) .
Others+SRs may have different applications in NLP , specifically , they may help a parser with Word Sense Selection ( WSS , as in ( Hirst , 1987 ) ) , with preferring certain structures out of several g r a m m a t i c a l ones ( Whittemore et al. , 1990 ) and finally with deciding the semantic role played by a syntactic complement ( Basili et al. , 1992 ) .
Others+SRs may have different applications in NLP , specifically , they may help a parser with Word Sense Selection ( WSS , as in ( Hirst , 1987 ) ) , with preferring certain structures out of several g r a m m a t i c a l ones ( Whittemore et al. , 1990 ) and finally with deciding the semantic role played by a syntactic complement ( Basili et al. , 1992 ) .
Others+Ribas ( 1994a ) * This research has been made in the framework of the Acquilex - II Esprit Project ( 7315 ) , and has been supported by a grant of Departament d'Ensenyament , Generalitat de Catalunya , 91 - DOGC - 1491 .
Others+Thus , although suit was used in the WSJ articles only in the sense of < legal_action > , the algorithm not only considered the other senses as well ( < suit , suing > , < aResnik ( 1992 ) and Ribas ( 1994a ) used equation 1 without introducing normalization .
Others+Analyzing the results obtained from different experimental evaluation methods , Ribas ( 1994a ) drew up some conclusions : ( to atypical subjects as <suit_of_clothes> ) as SRs on the subject in contrast to Assoc. T h e second advantage is that as long as the prior probabilities , p ( c ) , involve simpler events than those used in Assoc , p ( cls ) , the estimation is easier and more accurate ( ameliorating d a t a sparseness ) .
Others+This scheme seems to present two problematic features ( see ( Ribas , 1994b ) for more details ) .
Others+Finally it would be a powerful tool for detecting flaws of a particular technique ( e.g , ( Ribas , 1994a ) analysis ) .
Others+Ribas ( 1994a ) * This research has been made in the framework of the Acquilex - II Esprit Project ( 7315 ) , and has been supported by a grant of Departament d'Ensenyament , Generalitat de Catalunya , 91 - DOGC - 1491 .
Others+Thus , although suit was used in the WSJ articles only in the sense of < legal_action > , the algorithm not only considered the other senses as well ( < suit , suing > , < aResnik ( 1992 ) and Ribas ( 1994a ) used equation 1 without introducing normalization .
Others+Analyzing the results obtained from different experimental evaluation methods , Ribas ( 1994a ) drew up some conclusions : ( to atypical subjects as <suit_of_clothes> ) as SRs on the subject in contrast to Assoc. T h e second advantage is that as long as the prior probabilities , p ( c ) , involve simpler events than those used in Assoc , p ( cls ) , the estimation is easier and more accurate ( ameliorating d a t a sparseness ) .
Others+This scheme seems to present two problematic features ( see ( Ribas , 1994b ) for more details ) .
Others+Finally it would be a powerful tool for detecting flaws of a particular technique ( e.g , ( Ribas , 1994a ) analysis ) .
Others+Other statistical measures to score SRs In this section we propose the application of other measures apart from Assoc for learning SRs : loglikelihood ratio ( Dunning , 1993 ) , relative entropy ( Cover and Thomas , 1991 ) , mutual information ratio ( Church and Hanks , 1990 ) , ¢ 2 ( Gale and Church , 1991 ) .
Others+Other statistical measures to score SRs In this section we propose the application of other measures apart from Assoc for learning SRs : loglikelihood ratio ( Dunning , 1993 ) , relative entropy ( Cover and Thomas , 1991 ) , mutual information ratio ( Church and Hanks , 1990 ) , ¢ 2 ( Gale and Church , 1991 ) .
Others+Other statistical measures to score SRs In this section we propose the application of other measures apart from Assoc for learning SRs : loglikelihood ratio ( Dunning , 1993 ) , relative entropy ( Cover and Thomas , 1991 ) , mutual information ratio ( Church and Hanks , 1990 ) , ¢ 2 ( Gale and Church , 1991 ) .
Others+Other statistical measures to score SRs In this section we propose the application of other measures apart from Assoc for learning SRs : loglikelihood ratio ( Dunning , 1993 ) , relative entropy ( Cover and Thomas , 1991 ) , mutual information ratio ( Church and Hanks , 1990 ) , ¢ 2 ( Gale and Church , 1991 ) .
Others+Other statistical measures to score SRs In this section we propose the application of other measures apart from Assoc for learning SRs : loglikelihood ratio ( Dunning , 1993 ) , relative entropy ( Cover and Thomas , 1991 ) , mutual information ratio ( Church and Hanks , 1990 ) , ¢ 2 ( Gale and Church , 1991 ) .
Others+Other statistical measures to score SRs In this section we propose the application of other measures apart from Assoc for learning SRs : loglikelihood ratio ( Dunning , 1993 ) , relative entropy ( Cover and Thomas , 1991 ) , mutual information ratio ( Church and Hanks , 1990 ) , ¢ 2 ( Gale and Church , 1991 ) .
Others+In the recent literature ( ( Grishman and Sterling , 1992 ) , ( Resnik , 1993 ) , ... ) several task oriented schemes to test Selectional Restrictions ( mainly on syntactic ambiguity resolution ) have been proposed .
Others+Kehler ( 1993a ) has convincingly argued that this problem arises because DSP do not distinguish between merely co-referential and co-indexed ( in his terminology , role - linked ) expressions .
Others+The use of terms and indices has parallels to proposals due to Kehler and Kamp ( Kehler , 1993a ; Gawron and Peters , 1990 ) .
Others+Selecting ellipsis antecedents and parallel elements within them is an open problem ( Priist , 1992 ; Prfist et al. , 1994 ; Kehler , 1993b ; Grover et al. , 1994 ) .
Extends+Section 2 describes the substitutional treatment of ellipsis by way of a few examples presented in a simplified version of Quasi Logical Form ( QLF ) ( Alshawi and Crouch , 1992 ; Alshawi et el. , 1992 ) .
Extends+Terms and indices not dischargeable in this manner lead to uninterpretable QLFs ( Alshawi and Crouch , 1992 ) .
Extends+Figure 1 defines a valuation relation for the QLF fragment used above , derived from ( Alshawi and Crouch , 1992 ; Cooper et al. , 1994a ) .
Extends+Inappropriate scoping may leave undischarged and hence uninterpretable terms and indices ( which accounts for the so - called freevariable and vacuous quantification constraints on scope ( Alshawi and Crouch , 1992 ) ) .
Extends+When Sforns axe described in ( Alshawi and Crouch , 1992 ) .
Others+Kehler ( 1993a ) has convincingly argued that this problem arises because DSP do not distinguish between merely co-referential and co-indexed ( in his terminology , role - linked ) expressions .
Others+The use of terms and indices has parallels to proposals due to Kehler and Kamp ( Kehler , 1993a ; Gawron and Peters , 1990 ) .
Others+Selecting ellipsis antecedents and parallel elements within them is an open problem ( Priist , 1992 ; Prfist et al. , 1994 ; Kehler , 1993b ; Grover et al. , 1994 ) .
Others+M u l t i p l e V P Ellipsis Multiple VP ellipsis ( Gardent , 1993 ) poses problems at the level of determining which VP is the antecedent of which ellipsis .
Others+Selecting ellipsis antecedents and parallel elements within them is an open problem ( Priist , 1992 ; Prfist et al. , 1994 ; Kehler , 1993b ; Grover et al. , 1994 ) .
Others+The category constrains resolution to look for verb phrase / sentence sources , which come wrapped in forms with categories like [ t ease = past , modalffino , pexf ectffino , p r o g r e s s i v e n o , polffipos . . . . ] Heuristics simi ] ar to those described by Hardt ( 1992 ) may be used for this .
Others+We provide an extension to the previous account of LFG semantics ( Dalrymple et al. , 1993a ) according to which dependencies between f - structures are viewed as resources ; as a result a one - to - one correspondence between uses of f - structures and meanings is maintained .
Others+In the system described in Dalrymple et al. ( 1993a ) , the ~ relation associates expressions in the meaning language with f - structures .
Others+In the examples discussed in Dalrymple et al. ( 1993a ) there is a one - to - one correspondence between the set of path sets S and the set of f - structures S I picked out by such path sets , so the two methods yield the same predictions for those cases .
Others+The method combines a constraint - based approach with an approach based on preferences : we exploit the HPSG type hierarchy and unification to arrive at a temporal structure using constraints placed on that structure by tense , aspect , rhetorical structure and temporal expressions , and we use the temporal centering preferences described by ( Kameyama et al. , 1993 ; Poesio , 1994 ) to rate the possibilities for temporal structure and choose &apos; the best among them .
Others+thread with parallel tense , are employed ( See ( Kameyama et al. , 1993 ; Poesio , 1994 ) for details ) , and the resulting ratings are factored into the rating for each thread .
Others+This approach improves upon earlier work on discourse structure such as ( Lascarides and Asher , 1991 ) and ( Kehler , 1994 ) in reducing the number of possible ambiguities ; it is also more precise than the Kamp / Hinrichs / Partee approach in that it takes into account ways in which the apparent defaults can be overridden and differentiates between events and activities , which behave differently in narrative progression .
Others+This is a technique similar to the type of centering used for nominal a n a p h o r a ( Sidner , 1983 ; Grosz et al. , 1983 ) .
Others+The constraints that we identify for the tree - based system can be regarded equally well as constraints on unification - based grammar formalisms such as PArR ( Shieber , 1984 ) .
Others+Vijay - Shanker and Weir ( 1993 ) show that Linear Indexed Grammars ( I_IG ) can be processed in polynomial time by exploiting constraints which make possible the extensive use of structure - sharing .
Others+Vijay - Shanker and Weir ( 1993 ) show that Linear Indexed Grammars ( I_IG ) can be processed in polynomial time by exploiting constraints which make possible the extensive use of structure - sharing .
Others+The key to this algorithm is the use of structure sharing techniques similar to those used to process I_lG efficiently ( Vijay - Shanker and Weir , 1993 ) .
Others+While we know of previous work which associates scores with feature structures ( Kim , 1994 ) are not aware of any previous t r e a t m e n t which makes explicit the link to classical probability theory .
Others+Transformation - based tagging as introduced by Brill ( 1993 ) also requires a handtagged text for training .
Others+Brill et al. ( 1990 ) try to infer grammatical category from bigram statistics .
Others+Biber ( 1993 ) applies factor analysis to collocations of two target words ( " certain " and " right " ) with their immediate neighbors .
Others+Brill and Marcus ( 1992a ) have shown that the effort necessary to construct the part - of - speech lexicon can be considerably reduced by combining learning procedures and a partial part - of - speech categorization elicited from an informant .
Others+Categories that can be induced well ( those characterized by local dependencies ) could be input into procedures that learn phrase structure ( e.g. ( Brill and Marcus , 19925 ; Finch , 1993 ) ) .
Others+This approach is exemplified by Combinatory Categorial G r a m m a r , C C G ( Steedman 1991 ) , which takes a basic CG with just application , and adds various new ways of combining elements together 2 .
Extends+There are also potential computational applications for incremental interpretation , including early parse filtering using statistics based on logical form plausibility , and interpretation of fragments of dialogues ( a survey is provided by Milward and Cooper , 1994 , henceforth referred to as M & : C ) .
Others+ronis and Ide ( 1990 ) used reference networks as neural networks , llearst ( 1991 ) used ( shallow ) syntactic similarity between contexts , Cowie el al. ( 1992 ) used simulated annealing for quick parallel disambignation , and Yarowsky ( 1992 ) used co-occurrence statistics between words and thesaurus categories .
Others+To recognize normal state implicatures one must consider mutual beliefs and plans ( Green , 1990 ) .
Others+To understand conversationM implicatures associated with indirect replies one must consider discourse expectations , discourse plans , and discourse relations ( Green , 1992 ; Green and Carberry , 1994 ) .
Others+Green ( 1992 ) assumes that implicatures are connected to discourse entities and not to utterances , but her approach still does not allow cancellations across discourse units .
Others+C o n v e r s a t i o n a l i m p l i c a t u r e s in i n d i r e c t replies The same methodology can be applied to modeling conversational impIicatures in indirect replies ( Green , 1992 ) .
Others+To understand conversationM implicatures associated with indirect replies one must consider discourse expectations , discourse plans , and discourse relations ( Green , 1992 ; Green and Carberry , 1994 ) .
Others+For Frege ( 1892 ) , Russell ( 1905 ) , and Quine ( 1949 ) &quot; everything exists &quot; ; therefore , in their logical systems , it is impossible to formalize the cancellation of the presupposition that definite referents exist ( Hirst , 1991 ; Marcu and Hirst , 1994 ) .
Others+P r a g m a t i c i n f e r e n c e s in s e q u e n c e s o f utterances We have already mentioned that speech repairs constitute a good benchmark for studying the generation and cancellation of pragmatic inferences along sequences of utterances ( McRoy and Hirst , 1993 ) .
Others+Such an approach allows for the treatment of missing , extraneous , interchanged or misused words ( Teitelbaum , 1973 ; Saito and Tomita , 1988 ; Nederhof and Bertsch , 1994 ) .
Others+Cycles might emerge to treat unknown sequences of words , i.e. sentences with unknown parts of unknown lengths ( Lang , 1988 ) .
Others+It is also straightforward to show that the complexity of this process is cubic in the number of states of the FSA ( in the case of ordinary parsing the number of states equals n 1 ) ( Lang , 1974 ; Billot and Lang , 1989 ) ( assuming the right - hand - sides of grammar rules have at most two categories ) .
Others+But if we use existing techniques for parsing DCGs , then we are also confronted with an undecidability problem : the recognition problem for DCGs is undecidable ( Pereira and Warren , 1983 ) .
Others+Introduction Lexicalist approaches to MT , particularly those incorporating the technique of Shake - and - Bake generation ( Beaven , 1992a ; Beaven , 1992b ; Whitelock , 1994 ) , combine the linguistic advantages of transfer ( Arnold et al. , 1988 ; Allegranza et al. , 1991 ) and interlingual ( Nirenburg et al. , 1992 ; Dorr , 1993 ) approaches .
Others+For example , ( Beaven , 1992a ) employs a chart to avoid recalculating the same combinations of signs more than once during testing , and ( Popowich , 1994 ) proposes a more general technique for storing which rule applications have been attempted ; ( Brew , 1992 ) avoids certain pathological cases by employing global constraints on the solution space ; researchers such as ( Brown et al. , 1990 ) and ( Chen and Lee , 1994 ) provide a system for bag generation that is heuristically guided by probabilities .
Others+Even in ( Beaven , 1992a ) , the derivation information is used simply to cache previous results to avoid exact recomputation at a later stage , not to improve on previous guesses .
Others+The Shake - an & Bake generation algorithm of ( Whitelock , 1992 ) combines target language signs using the technique known as generate - and - test .
Others+The Shake - an & Bake generation algorithm of ( Whitelock , 1992 ) combines target language signs using the technique known as generate - and - test .
Others+For example , ( Beaven , 1992a ) employs a chart to avoid recalculating the same combinations of signs more than once during testing , and ( Popowich , 1994 ) proposes a more general technique for storing which rule applications have been attempted ; ( Brew , 1992 ) avoids certain pathological cases by employing global constraints on the solution space ; researchers such as ( Brown et al. , 1990 ) and ( Chen and Lee , 1994 ) provide a system for bag generation that is heuristically guided by probabilities .
Others+These 30 questions are determined by growing a classifiu p the node is neither the first nor the last child cation tree on the word vocabulary as described in of a constituent ; ( Brown et al. , 1992 ) .
Others+While substantial work on noun compounds exists in both linguistics ( e.g. Levi , 1978 ; Ryder , 1994 ) and computational linguistics ( Finin , 1980 ; McDonald , 1982 ; Isabelle , 1984 ) , techniques suitable for broad coverage parsing remain unavailable .
Others+The problem is analogous to the prepositional phrase attachment task explored in Hindle and Rooth ( 1993 ) .
Others+In Lauer ( 1994 ) , the degree of acceptability is again provided by statistical measures over a corpus .
Others+In Lauer ( 1994 ) , the degree of acceptability is again provided by statistical measures over a corpus .
Others+1Lauer and Dras ( 1994 ) give a formal construction motivating the algorithm given in Lauer ( 1994 ) .
Others+The simplest of these is reported in Pustejovsky et al ( 1993 ) .
Others+To distinguish nouns from other words , the University of Pennsylvania morphological analyser ( described in Karp et al , 1992 ) was used to generate the set of words that can only be used as nouns ( I shall henceforth call this set AZ ) .
Others+Resnik and Hearst ( 1993 ) coined the term CONCEPTUAL ASSOCIATION to refer to association values computed between groups of words .
Others+Related W o r k Researchers have studied the analysis and generation of arguments ( Birnbaum et al. , 1980 ; Reichman , 1981 ; Cohen , 1987 ; Sycara , 1989 ; Quilici , 1992 ; Maybury , 1993 ) ; however , agents engaging in argumentative dialogues are solely interested in winning an argument and thus exhibit different behavior from collaborative agents .
Others+This differentiates collaborative negotiation from argumentation ( Birnbaum et al. , 1980 ; Reichman , 1981 ; Cohen , 1987 ; Quilici , 1992 ) .
Others+Related W o r k Researchers have studied the analysis and generation of arguments ( Birnbaum et al. , 1980 ; Reichman , 1981 ; Cohen , 1987 ; Sycara , 1989 ; Quilici , 1992 ; Maybury , 1993 ) ; however , agents engaging in argumentative dialogues are solely interested in winning an argument and thus exhibit different behavior from collaborative agents .
Others+This differentiates collaborative negotiation from argumentation ( Birnbaum et al. , 1980 ; Reichman , 1981 ; Cohen , 1987 ; Quilici , 1992 ) .
Others+Webber and Joshi ( 1982 ) have noted the importance of a cooperative system providing support for its responses .
Extends+In order to capture the agents ' intentions conveyed by their utterances , our model of collaborative negotiation utilizes an enhanced version of the dialogue model described in ( Lambert and Carberry , 1991 ) to represent the current status of the interaction .
Others+Following Walker 's weakest link assumption ( Walker , 1992 ) the strength of the evidence is the weaker of the strength of the belief and the strength of the evidential relationship .
Others+The collaborative planning principle in ( Whittak ~ and Stenton , 1988 ; Walker , 1992 ) suggests that &quot; conversants must provide evidence of a detected discrepancy in belief as soon as possible .
Others+Therefore , for each unaccepted top - level belief , our process for selecting the focus of modificatkm involves two steps : identifying a candidate foci tree from the proposed belief tree , and selecting a eThis subdialogue is considered an interrupt by Whittaker , Stenton , and Walker ( Whittaker and Stenton , 1988 ; Walker and Whittaker , 1990 ) , initiated to negotiate the truth of a piece of information .
Others+See ( Jowsey , 1990 ) and ( Moore , 1989 ) for a thorough discussion .
Others+( Moore , 1989 ) in fact borrows the notation for Aabstraction from AProlog .
Others+( Park , 1992 ) proposes a solution within first - order unification that can handle not only sentence ( la ) , but also more complex examples with determiners .
Others+Also , whereas ( Park , 1992 ) requires careful consideration of handling of determiners with coordination , here such sentences are handled just like any others .
Others+This paper is meant to be viewed as furthering the exploration of the utility of higher - order logic programming for computational linguistics see , for example , ( Miller & Nadathur , 1986 ) , ( Pareschi , 1989 ) , and ( Pereira , 1990 ) .
Others+This necessitates a dynamic processing strategy , i.e. , memoization , extended with an abstraction function like , e.g. , restriction ( Shieber , 1985 ) , to weaken filtering and a subsumption check to discard redundant results .
Others+Through trimming this magic rule , e.g. , given a bounded term depth ( Sato and Tamaki , 1984 ) or a restrictor ( Shieber , 1985 ) , constructing an abstract unfolding tree reveals the fact that a cycle results from the magic rule .
Others+In generation , examples of such extended processing strategies are head corner generation with its semantic linking ( Shieber et al. , 1990 ) or bottom - up ( Earley ) generation with a semantic filter ( Shieber , 1988 ) .
Others+The use of such a semantic filter in bottom - up evaluation requires the grammar to obey the semantic monotonicity constraint in order to ensure completeness ( Shieber , 1988 ) ( see below ) .
Others+In order to obtain a generator similar to the bottom - up generator as described in Shieber ( 1988 ) the compilation process can be modified such that only lexical entries are extended with magic literals .
Others+Motivated by a concern for lexical organization and global coherence in the structure of lexicon , some researchers have moved towards nlore expressive semantic descriptions , as well as more powerful methods of combining them ( see for example Pustejovsky , 1991 , 1995 ; Briscoe , 1993 ) .
Others+Machine Readable Dictionaries ( MIH ) s ) are a good sour ( : e of lexical information and have been shown to be al ) plical ) le to the task of I , KII COllStruction ( l ) ola.n ct al. , 1993 ; Calzolari , t992 ; Copestake , [ 990 ; Wilks et al. , 1989 ; Byrd et al. , 1987 ) .
Others+Such information is frequently used for noun taxonomy construction ( Byrd et al. , 1987 ; Klavans et al. , 1990 ; Barri ~ re and Popowich , To appear August 1996 ) .
Others+ntactic roles , or part - of - speech ( POS ) tags , of the words involved ( Bear , Dowding , and Shriberg , 1992 ; Heeman and Allen , 1994 ) .
Others+E x a m p l e 5 ( d93 - 14.3 u t t 4 2 ) we need to um manage to get the bananas to Dansville T editing term interruption point There is typically a correspondence between the reparandum and the alteration , and following Bear et al. ( 1992 ) , we annotate this using the labels m for word matching and r for word replacements ( words of the same syntactic category ) .
Others+ntactic roles , or part - of - speech ( POS ) tags , of the words involved ( Bear , Dowding , and Shriberg , 1992 ; Heeman and Allen , 1994 ) .
Others+null tokens between each pair of consecutive words wi - 1 and wi ( Heeman and Allen , 1994 ) , which wilt be tagged as to the occurrence of these events .
Others+The POS tagset , based on the Penn Treebank tagset ( Marcus , Santorini , and Marcinkiewicz , 1993 ) , includes special tags for denoting when a word is being used as a discourse marker .
Others+To allow the decision tree to ask about the words and POS tags in the context , we cluster the words and POS tags using the algorithm of Brown et al. ( 1992 ) into a binary classification tree .
Others+Unlike other work ( e.g. ( Black et al. , 1992 ; Materman , 1995 ) ) , we treat the word identities as a further refinement of the POS tags ; thus we build a word classification tree for each POS tag .
Others+Computational instances of systemic grammar are successfully employed in some of the largest and most influential text generation projects -- such as , for example , PENMAN ( Mann , 1983 ) , COMMUNAL ( Fawcett and Tucker , 1990 ) , TECHDOC ( KSsner and Stede , 1994 ) , Drafter ( Paris and Vander Linden , 1996 ) , and Gist ( Not and Stock , 1994 ) .
Others+This interface is organized with the help of so - called Subject : D Theme : [ &quot; i ' ] Directcomplement : [ 7 ] Given the notational equivalence of HPSG and systemic grammar first mentioned by ( Carpenter , 1992 ) and ( Zajac , 1992 ) , and further elaborated in ( Henschel , 1995 ) , one can characterize a systemic grammar as a large type hierarchy with multiple ( conjunctive and disjunctive ) and multi-dimensional inheritance with an open - world semantics .
Others+The following type axioms taken from the large systemic English grammar NXGI ~ L ( Matthiessen , 1983 ) shall illustrate the nature of systems in a systemic grammar :
Others+They also serve to enforce deterministic choice an important property for practical generation ( cfXXX ( Reiter , 1994 ) ) .
Others+Our approach shows some similarities to that proposed by ( Rayner and Carter , 1996 ) for improving parsing performance by grammar pruning and specialization with respect to a training corpus .
Others+Profiles for a large number of entities were compiled using our earlier system , PROFILE ( Radev and McKeown , 1997 ) .
Others+Some examples of language reuse include collocation analysis ( Smadja , 1993 ) , the use of entire factual sentences extracted from corpora ( e.g. , &quot; ' Toy Story ' is the Academy Award winning animated film developed by Pixar ~ ' ) , and summarization using sentence extraction ( Paice , 1990 ; Kupiec et al. , 1995 ) .
Others+Introduction Anaphora resolution is still present as a significant linguistic problem , both theoretically and practically , and interest has recently been renewed with the introduction of a quantitative evaluation regime as part of the Message Understanding Conference ( MUC ) evaluations of Information Extraction ( IE ) systems ( Grishman and Sundheim , 1996 ) .
Others+As part of MUC ( Grishman and Sundheim , 1996 ) , coreference resolution was evaluated as a sub-task of information extraction , which involved negotiating a definition of coreference relations that could be reliably evaluated .
Extends+This paper describes an evaluation of a focusbased approach to pronoun resolution ( not anaphora in general ) , based on an extension of Sidner &apos;s algorithm ( Sidner , 1981 ) proposed in ( Azzam , 1996 ) , with further refinements from development on real - world texts .
Others+Automatically annotated texts , produced by systems using the same markup scheme , were then compared with the manually annotated versions , using scoring software made available to MUC participants , based on ( Vilain et al. , 1995 ) .
Others+The labels are as in ( Eisner , 1996 ) .
Others+But , as pointed out by Eisner ( 1996 , p. 85 ) , this is not spurious ambiguity in the technical sense , just multiple derivations due to alternative lexical category assignments .
Others+Other normal form parsers , e.g. that of Hepple and Morrill ( 1989 ) , have the same problem .
Others+( Joshi , 1988 ) and ( Joshi and Schabes , 1992 ) are good introductions to the formalism and its linguistic relevance .
Others+Factors affecting the evaluation process are : ( 1 ) Training and test experiments are usually performed over noisy corpora which distorts the obtained results , ( 2 ) performance figures are too often calculated from only a single or very small number of trials , though average results from multiple trials are crucial to obtain reliable estimations of accuracy ( Mooney , 1996 ) , ( 3 ) testing experiments are usually done on corpora with the same characteristics as the training data - usually a small fresh portion of the training corpusbut no serious attempts have been done in order to determine the reliability of the results when moving from one domain to another ( Krovetz , 1997 ) , and ( 4 ) no figures about computational effort - space / time complexityare usually reported , even from an empirical perspective .
Others+Factors affecting the evaluation process are : ( 1 ) Training and test experiments are usually performed over noisy corpora which distorts the obtained results , ( 2 ) performance figures are too often calculated from only a single or very small number of trials , though average results from multiple trials are crucial to obtain reliable estimations of accuracy ( Mooney , 1996 ) , ( 3 ) testing experiments are usually done on corpora with the same characteristics as the training data - usually a small fresh portion of the training corpusbut no serious attempts have been done in order to determine the reliability of the results when moving from one domain to another ( Krovetz , 1997 ) , and ( 4 ) no figures about computational effort - space / time complexityare usually reported , even from an empirical perspective .
Others+Although ( Church , 1992 ) questions the concept of correct analysis , ( Samuelsson and Voutilainen , 1997 ) establish that there exists a - statistically significantabsolute correct disambiguation , respect to which the error rates of either the tagger or the test corpus can be computed .
Others+Although asymptotically faster methods can be constructed , as discussed in ( Rajasekaran and Yooseph , 1995 ) , these methods are not of practical interest , due to large hidden constants .
Others+More generally , in ( Satta , 1994 ) it has been argued that methods for TAG parsing running in time asymptotically faster than O ( n 6 ) are unlikely to have small hidden constants .
Others+the proposed vast majority been currently English and of Several restrictions on the adjunction operation for TAG have been proposed in the literature ( Schabes and Waters , 1993 ; Schabes and Waters , 1995 ) ( Rogers , 1994 ) .
Others+Our restriction is fundamentally different from those in ( Schabes and Waters , 1993 ; Schabes and Waters , 1995 ) and ( Rogers , 1994 ) , in that we allow wrapping auxiliary trees to nest inside each other an unbounded number of times , so long as they only adjoin at one place in each others ' spines .
Others+In contrast to the formalisms of Schabes and Waters ( Schabes and Waters , 1993 ; Schabes and Waters , 1995 ) , our restriction allows wrapping complement auxiliaries as in Figure 4 ( Schabes and Waters , 1995 ) .
Others+the proposed vast majority been currently English and of Several restrictions on the adjunction operation for TAG have been proposed in the literature ( Schabes and Waters , 1993 ; Schabes and Waters , 1995 ) ( Rogers , 1994 ) .
Others+Our restriction is fundamentally different from those in ( Schabes and Waters , 1993 ; Schabes and Waters , 1995 ) and ( Rogers , 1994 ) , in that we allow wrapping auxiliary trees to nest inside each other an unbounded number of times , so long as they only adjoin at one place in each others ' spines .
Others+Rogers ' regular form restriction ( Rogers , 1994 ) , we can cite verb - raised complement auxiliary trees in Dutch as in Figure 5 ( Kroch and Santorini , 1991 ) .
Others+Generalizing L e x i c a l Co-occurrence Evidence - based models represent context as a set of features , say words , that are observed to co-occur with , and thereby predict , a word ( Yarowsky , 1992 ; Golding and Schabes , 1996 ; Karow and Edelman , 1996 ; Ng and Lee , 1996 ) .
Others+Generalizing L e x i c a l Co-occurrence Evidence - based models represent context as a set of features , say words , that are observed to co-occur with , and thereby predict , a word ( Yarowsky , 1992 ; Golding and Schabes , 1996 ; Karow and Edelman , 1996 ; Ng and Lee , 1996 ) .
Others+Generalizing L e x i c a l Co-occurrence Evidence - based models represent context as a set of features , say words , that are observed to co-occur with , and thereby predict , a word ( Yarowsky , 1992 ; Golding and Schabes , 1996 ; Karow and Edelman , 1996 ; Ng and Lee , 1996 ) .
Others+Generalizing L e x i c a l Co-occurrence Evidence - based models represent context as a set of features , say words , that are observed to co-occur with , and thereby predict , a word ( Yarowsky , 1992 ; Golding and Schabes , 1996 ; Karow and Edelman , 1996 ; Ng and Lee , 1996 ) .
Others+We had hoped that some approach to a limit would be seen using P T B II ( Marcus et al. , 1994 ) , which larger and more consistent for bracketting than P T B I .
Others+Improvements and extensions to this algorithm have been provided by Karttunen ( 1995 ) , Karttunen ( 1997 ) , Karttunen ( 1996 ) and Mohri and Sproat ( 1996 ) .
Others+Improvements and extensions to this algorithm have been provided by Karttunen ( 1995 ) , Karttunen ( 1997 ) , Karttunen ( 1996 ) and Mohri and Sproat ( 1996 ) .
Extends+Improvements and extensions to this algorithm have been provided by Karttunen ( 1995 ) , Karttunen ( 1997 ) , Karttunen ( 1996 ) and Mohri and Sproat ( 1996 ) .
Extends+In the following section , we initially concentrate on the simple Case in ( 1 ) and show how ( 1 ) may be compiled assuming left - to - right processing along with the overall longest match strategy described by Karttunen ( 1996 ) .
Extends+anonymous reviewer suggested theft could be implemented in the framework of Karttunen ( 1996 ) as : lm_concat [ toltoplolpolo ] - ... # ; Indeed the resulting transducer from this expression would transduce t o p o l o g i c a l into top # o # 1ogical .
Extends+Improvements and extensions to this algorithm have been provided by Karttunen ( 1995 ) , Karttunen ( 1997 ) , Karttunen ( 1996 ) and Mohri and Sproat ( 1996 ) .
Extends+The major components of the algorithm are not new , but straightforward modifications of components presented in Karttunen ( 1996 ) and Mohri and Sproat ( 1996 ) .
Extends+This functionality has been used in van Noord and Gerdemann ( 1999 ) to provide an implementation of the algorithm in Mohri and Sproat ( 1996 ) .
Extends+This functionality has been used in van Noord and Gerdemann ( 1999 ) to provide an implementation of the algorithm in Mohri and Sproat ( 1996 ) .
Others+For instance , the lenient_composition operator ( Karttunen , 1998 ) is defined by : macro ( priorityiunion ( Q , R ) , { Q , - domain ( Q ) o R } ) .
Others+For even a moderately complex domain such as the ATIS corpus , a grammar trained on data with constituent bracketing information produces much better parses than one trained on completely unmarked raw data ( Pereira and Schabes , 1992 ) .
Others+To induce a grammar from the sparsely bracketed training data previously described , we use a variant of the Inside - Outside re-estimation algorithm proposed by Pereira and Schabes ( 1992 ) .
Others+The inferred grammars are represented in the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( Schabes and Waters , 1993 ; Hwa , 1998a ) , which is lexicalized and context - free equivalent .
Others+Ribas ( 1994 ) presented an approach which takes into account the syntactic position of the elements whose semantic relation is to be acquired .
Others+The probability model we use can be found earlier in Pereira et al. ( 1993 ) .
Others+We evaluated our clustering models on a pseudodisambiguation task similar to that performed in Pereira et al. ( 1993 ) , but differing in detail .
Others+Approaches to probabilistic clustering similar to ours were presented recently in Saul and Pereira ( 1997 ) and Hofmann and Puzicha ( 1998 ) .
Others+( Collins , 1997 ; Ratnaparkhi , 1997 ) use cascaded processing for full parsing with good results .
Others+( Collins , 1997 ; Ratnaparkhi , 1997 ) use cascaded processing for full parsing with good results .
Others+Argamon et al. ( 1998 ) applied Memory - Based Sequence Learning ( MBSL ) to NP chunking and subject / object identification .
Others+memory - based approaches to parsing , see ( Bod , 1992 ) and ( Sekine , 1998 ) .
Others+Ramshaw and Marcus ( 1995 ) first assigned a chunk tag to each word in the sentence : I for inside a chunk , O for outside a chunk , and
Others+( Ramshaw and Marcus , 1995 ) describe an error - driven transformation - based learning ( TBL ) method for finding NP chunks in texts .
Others+D a t a representation We have compared four complete and three partial data representation formats for the baseNP recognition task presented in ( Ramshaw and Marcus , 1995 ) .
Others+In ( Ramshaw and Marcus , 1995 ) a set of transformational rules is used for modifying the classification of words .
Others+The chunking classification was made by ( Ramshaw and Marcus , 1995 ) based on the parsing information in the WSJ corpus .
Others+( Ramshaw and Marcus , 1995 ) shows that baseNP recognition ( Fz = I = 92.0 ) is easier than finding both NP and VP chunks ( Fz = 1 = 88.1 ) and that increasing the size of the training data increases the performance on the test set .
Others+D a t a representation We have compared four complete and three partial data representation formats for the baseNP recognition task presented in ( Ramshaw and Marcus , 1995 ) .
Others+However , they use the ( Ramshaw and Marcus , 1995 ) data set in a different training - test division ( 10-fold cross validation ) which makes it ( tifficult to compare their results with others .
Others+We will follow ( Argamon et al. , 1998 ) and use a combination of the precision and recall rates : F ~ = I = ( 2 &quot; precision * recall ) / ( precision recall ) .
Others+( Argamon et al. , 1998 ) introduce Memory - Based Sequence Learning and use it for different chunking experiments .
Others+The algorithm is very fast and it reaches the same performance as ( Argamon et al. , 1998 ) ( F , ~ = 1 = 91.6 ) .
Others+For problem ( 2 ) , we use &quot; transliteration &quot; ( Chen et al. , 1998 ; Knight and Graehl , 1998 ; Wan and Verspoor , 1998 ) .
Others+The third approach transfers both queries and documents into an interlingual representation : bilingual thesaurus classes ( Mongar , 1969 ; Salton , 1970 ; Sheridan and Ballerini , 1996 ) and language - independent vector space models ( Carbonell et al. , 1997 ; Dumais et al. , 1996 ) .
Extends+The new grammar is basically a scaleddown and adapted version of the Core Language Engine grammar for English ( Pulman 1992 ; Rayner 1993 ) ; concrete development work and testing were organized around a speech interface to a set of functionalities o ered by a simple simulation of the Space Shuttle ( Rayner , Hockey and James 2000 ) .
Extends+Given the value of selectional restrictions in practical applications , we explore how they can be utilised in the hpsg theory ( Pollard and Sag , 1994 ) , assuming that the reader is familiar with hpsg .
Extends+has already a hierarchy of feature structure sorts ( Pollard and Sag , 1994 ) .
Extends+Additional layers can be included between qfpsoa and the leaf sorts , as sketched in section 8.5 of ( Pollard and Sag , 1994 ) , to group together qfpsoas with common semantic roles .
Extends+For the implementation we extended the HPSG grammar ( Pollard and Sag , 1994 ) which Gerald Penn and Bob Carpenter first encoded in ALE ( Carpenter , 1993 ) .
Extends+Our system is currently being developed in the framework of Metis - II ( Vandeghinste et al. , 2006 ) .
Extends+We adapted a previous morphological analyzer / generator for Basque ( Alegria et al. , 1996 ) and transformed it according to the format used in Apertium .
Extends+The distilling methodologies include simulating BERT with a much smaller model ( e.g. , LSTM ) ( Tang et al. , 2019b ) and reducing some of the components , such as transformers , attentions to obtain the smaller BERT based model ( Sun et al. , 2019 ; Barkan et al. , 2019 ) .
Extends+Besides , Tang et al. ( 2019a , b ) proposed to distill BERT into a BiLSTM based model with penalizing the mean square error between the student ’s logits and the ones given by BERT as the objective on specific tasks , and introduced various data augmentation methods during distillation .
Extends+As our non-task-specific distillation task has no labeling data , and the signal given by the teacher is a real value vector , it is not feasible to minimize the cross-entropy loss over the soft labels and ground truth labels ( Sun et al. , 2019 ; Barkan et al. , 2019 ; Tang et al. , 2019b ) .
Extends+BiLSTMKD ( Tang et al. , 2019b ) introduces a new distillation objective to distill a BiLSTM based model from BERT for a specific task .
Extends+Besides , Tang et al. ( 2019a , b ) proposed to distill BERT into a BiLSTM based model with penalizing the mean square error between the student ’s logits and the ones given by BERT as the objective on specific tasks , and introduced various data augmentation methods during distillation .
Extends+augmentation , we use the rule - based method originally suggested by Tang et al. ( 2019b ) .
Extends+We first present a derivation of a Bayesian model of word recognition ( broadly similar to Norris and McQueen 2008 ) that incorporates both linguistic context and a model of noise estimated from the
Extends+The most similar previous channel model ( Norris and McQueen 2008 ) was based on Dutch gating data ( Smits et al. 2003 ) comparable to that used here .
Extends+Inspired by this , Yang et al. ( 2019 ) incorporate sememes ( Bloomfield , 1949 ; Dong and Dong , 2003 ) , i.e. minimum units of semantic meanings of human languages , in the task of generating definition in Chinese .
Extends+Previously , Yang et al. ( 2019 ) take sememes as one kind of such semantic components , and leverage external sememe annotations HowNet ( Dong and Dong , 2003 ) to help definition generation .
Extends+Sememe Annotation Resources Following previous work ( Yang et al. , 2019 ) , we take HowNet as the sememe annotation resource , which is an ontology that contains annotations for over 100,000 words with sememes .
Extends+As we mentioned in Section 2.3 , sememes are also known to be helpful for definition generation ( Yang et al. , 2019 ) .
Extends+Sememe Annotation Resources Following previous work ( Yang et al. , 2019 ) , we take HowNet as the sememe annotation resource , which is an ontology that contains annotations for over 100,000 words with sememes .
Extends+We select linguistic features based on a broad psycholinguistically - motivated composition by Vajjala Balakrishna ( 2015 ) .
Extends+To capture a variety of features , we utilized existing linguistic feature computation software2 developed by Vajjala Balakrishna ( 2015 ) based on 86 feature descriptions in existing readability literature .
Extends+Moving beyond basic features from syntactic parses , Vajjala Balakrishna ( 2015 ) also incorporated “ word characteristic ” features from linguistic databases .
Extends+Our model exploits the recent success of the hierarchical LSTM to generate image paragraph captions ( Krause et al. , 2017 ) .
Extends+Krause et al. ( 2017 ) was one of the early image paragraph captioning studies , which employed a sentence - level recurrent neural network ( RNN ) to generate sentence topic vectors , and then applied a word - level RNN to decode each topic vector into a sentence .
Extends+Following previous works ( Johnson et al. , 2016 ; Krause et al. , 2017 ) , given an image , we use a dense captioning method , i.e. , DenseCap ( Johnson et al. , 2016 ) as our image encoder to detect a set of semantic regions and produces the corresponding dense phrases describing the regions in natural language .
Extends+Sentence - level LSTM Similar to ( Krause et al. , 2017 ) , we aggregate a set of semantic feature vectors V ¯ which compactly describes the content of the image .
Extends+Dataset We conduct the experiments and evaluate our IMAP model on the widely used Stanford image paragraph dataset ( Krause et al. , 2017 ) , which is the only open source benchmark dataset available for image paragraph captioning .
Extends+Baseline Methods In the experiments , we compare the proposed IMAP with the following state - ofthe - art methods : DenseCap - Concat ( Johnson et al. , 2016 ) , Regions - Hierarchical ( Krause et al. , 2017 ) , RTT - GAN ( Liang et al. , 2017 ) , TMOS ( Mao et al. , 2018 ) , CapG - RevG ( Chatterjee and Schwing , 2018 ) , VREN ( Che et al. , 2019 ) , CAVP ( Zha et al. , 2019 ) , IAP that adopts the Top - Down attention ( Anderson et
Extends+Word2DM is an extension of Word2Vec ( Mikolov et al. , 2013a , b ) skip - gram with negative sampling ( SGNS ) .
Extends+Word2DM is an extension of Word2Vec ( Mikolov et al. , 2013a , b ) skip - gram with negative sampling ( SGNS ) .
Extends+The MU segmentation is implemented by a classification model under the pre-training & fine - tuning framework ( Devlin et al. , 2018 ; Sun et al. , 2019 ) .
Extends+Our training follows the pre-training and fine - tuning framework ( Devlin et al. , 2018 ; Sun et al. , 2019 ) .
Extends+The MU segmentation is implemented by a classification model under the pre-training & fine - tuning framework ( Devlin et al. , 2018 ; Sun et al. , 2019 ) .
Extends+Our training follows the pre-training and fine - tuning framework ( Devlin et al. , 2018 ; Sun et al. , 2019 ) .
Extends+Following Gal and Ghahramani ( 2016 ) , we apply variational dropout sharing the same dropout mask between all time steps in a sequence , and DropConnect ( Wan et al. , 2013 ; Merity et al. , 2017 ) on the hidden states of the BiLSTM .
Extends+We use a revised version of the corpus described in ( Downs et al. , 2017 ) 3 , which contains free text documents from the EHRs of adolescent patients who have an Autism Spectrum Disorder ( ASD ) and have been referred to the South London and Maudsley NHS Foundation Trust ( SLaM ) .
Extends+Other contributions of this paper are : ( 1 ) Mapping the suicidal behaviour data presented in ( Downs et al. , 2017 ) from the mention level to the sentence level .
Extends+We use a revised version of the corpus described in ( Downs et al. , 2017 ) 3 , which contains free text documents from the EHRs of adolescent patients who have an Autism Spectrum Disorder ( ASD ) and have been referred to the South London and Maudsley NHS Foundation Trust ( SLaM ) .
Extends+The current corpus contains one patient less and seven documents more than described in ( Downs et al. , 2017 ) .
Extends+For document and patient level labelling , we adapted the patient level process described in ( Downs et al. , 2017 ) 6 .
Extends+The work presented in this paper is part of a project on Simplification of Arabic Masterpieces for Extensive Reading ( SAMER ) ( Al Khalil et al. , 2017 ; Al Khalil et al. , 2018 ) .
Extends+Although neural NER networks have achieved superior performance when provided large - scale of training examples ( Li et al. , 2019 ) , it remains a non-trivial task to learn from limited new samples , also known as few - shot NER ( Fritzler et al. , 2019 ) .
Extends+Fritzler et al. ( 2019 ) does not calculate the prototype of O class from data , but directly sets a hyper - parameter bo as the fake distance similarity and optimize bo during training , which still regards O class as a whole .
Extends+Following ( Fritzler et al. , 2019 ) , we ensure there are at least K labels for each few - shot class .
Extends+Warm Prototypical Network ( WPN ) ( Fritzler et al. , 2019 ) is the transfer learning version of PN , which is first pre-trained on base classes and then fine - tuned on few - shot classes .
Extends+The centrality estimator essentially is an extension of the well - known LexRank algorithm ( Erkan and Radev , 2004 ) .
Extends+Third , to give a global estimations of the importance of the segments to the summary , we apply a Markov Chain ( Erkan and Radev , 2004 ) based estimator ( Global Estimator ) .
Extends+We perform a grid search for some of the hyperparameters , consistent with previous work ( Tsai et al. , 2019 ; Rahman et al. , 2020 ; Hazarika et al. , 2020 ; Sun et al. , 2020 ) , and empirically select the remaining ones .
Extends+Hazarika et al. ( 2020 ) project multimodal input into modality - invariant and modalityspecific spaces , and use a Transformer encoder on the concatenated projected representations .
Extends+The use of hidden states is the main difference between our method and the previous Transformerbased methods ( Tsai et al. , 2019 ; Rahman et al. , 2020 ; Hazarika et al. , 2020 ) .
Extends+Previous work ( Tsai et al. , 2019 ; Hazarika et al. , 2020 ) concatenate the representations of modalities to fuse cross-modal information .
Extends+We compare and follow the methodology from Hazarika et al. ( 2020 ) for UR - FUNNY which is state - of - the - art for the Glove feature on text .
Extends+Hazarika et al. ( 2020 ) project multimodal input into modality - invariant and modalityspecific spaces , and use a Transformer encoder on the concatenated projected representations .
Extends+At every layer , multiple hidden states of - the - art methods , MAG ( Rahman et al. , 2020 ) , will attend to the original input sequence for difMISA ( Hazarika et al. , 2020 ) use Transformer and
Extends+Previous work ( Conneau et al. , 2020 ; Hu et al. , 2020 ) mainly uses XLM - R for cross-lingual transfer on NLU tasks .
Extends+While multilingual pretrained models with encoder - decoder - based architecture ( Liu et al. , 2020 ; Chi et al. , 2020 ) work well on cross-lingual transfer for NLG tasks , multilingual pretrained encoders ( Wu and Dredze , 2019 ; Conneau and Lample , 2019 ; Conneau et al. , 2020 ) are mainly applied to cross-lingual NLU tasks ( Hu et al. , 2020 ) .
Extends+As an extension of conventional single - turn visual question answering , Das et al. ( 2017 ) introduce a multi-turn visual question answering task named visual dialogue , which aims to explore the ability of an AI agent to hold a meaningful multi-turn dialogue with humans in natural language about visual content .
Extends+We test the effectiveness of our proposed model on large - scale datasets : VisDial v0 .9 and v1 .0 ( Das et al. , 2017 ) .
Extends+We conduct experiments on the VisDial v0 .9 and v1 .0 datasets ( Das et al. , 2017 ) to verify our approach .
Extends+There are some successes on introducing the incremental structure into tasks related to dialog systems ( Zilka and Jurcicek , 2015 ; Coman et al. , 2019 ; Li et al. , 2019 ; Das et al. , 2017 ) .
Extends+There are some successes on introducing the incremental structure into tasks related to dialog systems ( Zilka and Jurcicek , 2015 ; Coman et al. , 2019 ; Li et al. , 2019 ; Das et al. , 2017 ) .
Extends+We use a retrieval setting to evaluate individual responses at each round of a dialogue , following Das et al. ( 2017 ) .
Extends+( 1 ) Fusion - based models : LF ( Das et al. , 2017 ) and HREA ( Das et al. , 2017 ) directly encode the multimodal inputs and decode the answer .
Extends+There are some successes on introducing the incremental structure into tasks related to dialog systems ( Zilka and Jurcicek , 2015 ; Coman et al. , 2019 ; Li et al. , 2019 ; Das et al. , 2017 ) .
Extends+While a large body of literature has studied the use of monolingual data to boost translation performance when limited supervision is available , two recent approaches have explored the fully unsupervised setting ( Lample et al. , 2018 ; Artetxe et al. , 2018 ) , relying only on monolingual corpora in each language , as in the pioneering work by Ravi and Knight ( 2011 ) .
Extends+For instance , Klementiev et al. ( 2012 ) used a provided bilingual dictionary , while Lample et al. ( 2018 ) and Artetxe et al. ( 2018 ) used dictionaries inferred in an unsupervised way ( Conneau et al. , 2018 ; Artetxe et al. , 2017 ) .
Extends+We now introduce a new unsupervised NMT method , which is derived from earlier work by Artetxe et al. ( 2018 ) and Lample et al. ( 2018 ) .
Extends+The first two pairs are used to compare to recent work on unsupervised MT ( Artetxe et al. , 2018 ; Lample et al. , 2018 ) .
Extends+For the LSTM model we use the same architecture as in Lample et al. ( 2018 ) .
Extends+Finally , our work can be seen as an extension of recent studies ( Lample et al. , 2018 ; Artetxe et al. , 2018 ; Yang et al. , 2018 ) on fully unsupervised MT with two major contributions .
Extends+For the NMT , we also consider two model selection procedures : an unsupervised criterion based on the BLEU score of a “ round - trip ” translation ( source → target → source and target → source → target ) as in Lample et al. ( 2018 ) , and crossvalidation using a small validation set with 100 parallel sentences .
Extends+Finally , our work can be seen as an extension of recent studies ( Lample et al. , 2018 ; Artetxe et al. , 2018 ; Yang et al. , 2018 ) on fully unsupervised MT with two major contributions .
Extends+Finally , our work can be seen as an extension of recent studies ( Lample et al. , 2018 ; Artetxe et al. , 2018 ; Yang et al. , 2018 ) on fully unsupervised MT with two major contributions .
Extends+HPSG ( Pollard and Sag 1994 ; Sag , Wasow , and Bender 2003 ) is a syntactic theory that follows the lexicalist framework .
Extends+In this paper , we propose a generalization of linguistic signs for Combinatory Categorial Grammar ( CCG ) ( Steedman , 2000b ) .
Extends+The approach is inspired on the one hand by earlier work by Steedman ( 2000a ) and Hoffman ( 1995 ) , and on the other by the signs found in constraint - based approaches to grammar .
Extends+Prosody & information structure Steedman ( 2000a ) presents a detailed , CCG - based account of how prosody is used in English as a means to realize information structure .
Extends+For example , consider ( 15 ) ( based on example ( 70 ) in Steedman ( 2000a ) ) :
Extends+Following Lewis ( 1995 ) , we estimate the F Score based on the current estimates of the class probabilities .
Extends+Text classification problems involve a rich and large feature space ( e.g. , bag - of - words features ) and so linear classifiers work very well ( Joachims , 1999 ) .
Extends+Inspired by the effectiveness of the TSVM model of Joachims ( 1999 ) , there have been a number of works on the solution of ( 3 ) for binary classification with large margin losses .
Extends+The semi-supervised learning algorithm for multi-class and hierarchical classification problems follows the spirit of the TSVM algorithm ( Joachims , 1999 ) .
Extends+Previous works ( Joachims , 1999 ; Sindhwani Pand Keerthi , 2006 ) do not make this neat connection to linear programming .
Extends+The method exploits the duality of a relation ( Bollegala et al. , 2010 ) .
Extends+( Bollegala et al. , 2010 ) , for example , used the duality in their work on co-clustering of entity - pairs and relation expressions .
Extends+Although the method is similar to that used in Bollegala et al. ( 2010 ) , we do not use any further constraints based on part - of - speech tags , lexical - syntactic information , etc. .
Extends+( Bollegala et al. , 2010 ) , for example , used the duality in their work on co-clustering of entity - pairs and relation expressions .
Extends+We used the same evaluation measures used in ( Bollegala et al. , 2010 ) .
Extends+Although the method is similar to that used in Bollegala et al. ( 2010 ) , we do not use any further constraints based on part - of - speech tags , lexical - syntactic information , etc. .
Extends+Following the work ( Lin and Pantel , 2001 ) , we constructed a simple space , in which a relation expression is characterized by the entities which it links .
Extends+Lin and Pantel ( 2001 ) proposed a weakly supervised framework of mining paraphrases based on shortest paths as basic units to be mined .
Extends+They demonstrated the effectiveness of their models compared with USP ( Poon and Domingos , 2009 ) or DIRT ( Lin and Pantel , 2001 ) .
Extends+We introduce then a new method called Hierarchical Graph Factorization Clustering ( HGFC ) ( Yu et al. , 2006 ) .
Extends+Our new method HGFC derives a probabilistic bipartite graph from the similarity matrix ( Yu et al. , 2006 ) .
Extends+Yu et al. ( 2006 ) performs the extraction via a propagation of probabilities from the bottom level clusters .
Extends+Yu et al. ( 2006 ) performs the extraction via a propagation of probabilities from the bottom level clusters .
Extends+According to the chain rule of the Markov process , the multi-hop transitions indicate a decaying similarity function on the graph ( Yu et al. , 2006 ) .
Extends+While several methods have been proposed for sentence compression ( Witbrock and Mittal , 1999 ; Jing and McKeown , 1999 ; Vandeghinste and Pan , 2004 ) , this paper focuses on Knight and Marcu ’s noisy - channel model ( Knight and Marcu , 2000 ) and presents an extension of their method .
Extends+Maximum Entropy Model for Sentence Compression We describe a maximum entropy method as a natural extension of Knight and Marcu ’s noisychannel model ( Knight and Marcu , 2000 ) .
Extends+Maximum Entropy Model for Sentence Compression We describe a maximum entropy method as a natural extension of Knight and Marcu ’s noisychannel model ( Knight and Marcu , 2000 ) .
Extends+( Knight and Marcu , 2000 ) .
Extends+This is a brief example ( Sidner 1983 ) : To resolve anaphors one of the most suitable existing approaches when dealing with anaphor issues in a conceptual analysis process is the focusing approach proposed by Sidner .
Extends+Introduction It is now generally accepted , especially since the work of Lakoff and associates ( e.g. [ 14,13,15 ] ) that much of everyday discourse shows evidence of metaphor .
Extends+Correspondence approaches to interpreting metaphor The work of Lakoff and Johnson e.g. [ 14,15 ] not only stressed the ubiquity of metaphor in everyday discourse , but also noted that many metaphorical utterances could be systematically related to each other , all appealing to different aspects of the same source domain and being used to describe the same target domain .
Extends+They might make use of what Lakoff and Johnson ( [ 14 ] p 53 ) call “ the unused part ” of the source domain .
Extends+We investigate the user needs of a challenging task yet to be tackled by TM but identified as an important potential application for it ( Lewin et al. 2008 ) : Cancer Risk Assessment ( CRA ) .
Extends+Expanding on our preliminary experiments ( Lewin et al. 2008 ) , we present a taxonomy which specifies the scientific evidence needed for CRA at the level of detail required for TM .
Extends+The role of stemming in improving retrieval effectiveness , particularly for highly inflected languages and monolingual retrieval has been well documented in studies including ( Larkey and Connell , 2003 ; Majumder et al. , 2007 ; Dolamic and Savoy , 2010 ) .
Extends+Stemming experiments and their effectiveness in IR have been carried out for English and other European languages ( Goldsmith et al. , 2000 ; Savoy , 2006 ; Fautsch and Savoy , 2009 ; Korenius et al. , 2004 ; Majumder et al. , 2008 ) and for a few Indian languages ( Majumder et al. , 2007 ; Dolamic and Savoy , 2010 ) .
Extends+Further , indexing and search strategies have been found to perform significantly better with the application of the various stemming strategies ( compared to an indexing scheme without a stemmer ) , for Hindi , Bengali and Marathi ( Dolamic and Savoy , 2010 ; Majumder et al. , 2007 ) .
Extends+We consider the set of string distance measures proposed in ( Majumder et al. , 2007 ) for clustering a set of words .
Extends+Motivated by the the effectiveness of the statistical stemmer ( using the three measures ) for the Indian languages Hindi , Marathi and Bengali as in ( Majumder et al. , 2007 ; Dolamic and Savoy , 2010 ) , we use the same measures to study stemming for Kannada which is of the strongly suffixing type .
Extends+In ( Majumder et al. , 2007 ) , the optimum distance threshold in the hierarchical clustering stage was chosen based upon the number ˇ of clusters being generated .
Extends+Model overview The proposed model is an extension of that proposed by Nakazawa and Kurohashi ( 2011 ) .
Extends+The earlier study ( Nakazawa and Kurohashi , 2011 ) only considered treelets as alignment units .
Extends+Our model considers dependency relations between treelets and assigns a weight to each relation following the previous work ( Nakazawa and Kurohashi , 2011 ) .
Extends+We train the model by means of a collapsed Gibbs sampling , which has been used in some recent NLP works ( Nakazawa and Kurohashi , 2011 ; DeNero et al. , 2008 ) .
Extends+They are borrowed from the previous work ( DeNero et al. , 2008 ; Nakazawa and Kurohashi , 2011 ) and changed a little .
Extends+Langlais et al. ( 2009 ) applied it to translating medical terms , and Langlais and Patry ( 2007 ) investigated the more specific task of translating unknown words , a problem simultaneously investigated in ( Denoual , 2007 ) .
Extends+We adopted the solution proposed in ( Langlais et al. , 2009 ) which consists in sampling this automaton without building it .
Extends+See ( Langlais et al. , 2009 ) for the tree - count solution we implemented in this work .
Extends+Langlais et al. ( 2009 ) trained a binary classifier to recognize good examples from bad ones .
Extends+An instance in this setting is simply a candidate form , and not a pair of analogies as in ( Langlais et al. , 2009 ) .
Extends+Our generator is more efficient than the one described in ( Langlais et al. , 2009 ) .
Extends+Overall , we confirm the findings of Langlais et al. ( 2009 ) and Langlais ( 2013 ) that analogical devices are typically more accurate than statistical phrase - based SMT , but that they are too often silent .
Extends+The first - order rule feature space , introduced by ( Zanzotto and Moschitti , 2006 ) , gives high performances in term of accuracy for textual entailment recognition with respect to other features spaces .
Extends+In ( Zanzotto and Moschitti , 2006 ) , tripartite directed acyclic graphs are implicitly introduced and exploited to build first - order rule feature spaces .
Extends+Finally , in Sec. 4.5 , we report the kernel computation we compare against presented by ( Zanzotto and Moschitti , 2006 ; Moschitti and Zanzotto , 2007 ) .
Extends+Yet , both in ( Zanzotto and Moschitti , 2006 ) and in ( Moschitti and Zanzotto , 2007 ) , the model proposed has two major limitations : it can represent rules with less than 7 variables and the proposed kernel is not a completely valid kernel as it uses the max function .
Extends+For example , given a source phrase f˜ and a target phrase e˜ , the phrase pair ( f˜ , e˜ ) is said to be consistent ( Och and Ney , 2004 ) with the alignment if and only if : ( 1 ) there must be at least one word inside one phrase aligned to a word inside the other ( 2 ) where count ( f˜ , e˜ ) indicates how often the phrase pair ( f˜ , e˜ ) occurs in the training corpus .
Extends+For example , given a source phrase f˜ and a target phrase e˜ , the phrase pair ( f˜ , e˜ ) is said to be consistent ( Och and Ney , 2004 ) with the alignment if and only if : ( 1 ) there must be at least one word inside one phrase aligned to a word inside the other ( 2 ) where count ( f˜ , e˜ ) indicates how often the phrase pair ( f˜ , e˜ ) occurs in the training corpus .
Extends+For example , given a source phrase f˜ and a target phrase e˜ , the phrase pair ( f˜ , e˜ ) is said to be consistent ( Och and Ney , 2004 ) with the alignment if and only if : ( 1 ) there must be at least one word inside one phrase aligned to a word inside the other ( 2 ) where count ( f˜ , e˜ ) indicates how often the phrase pair ( f˜ , e˜ ) occurs in the training corpus .
Extends+Och and Ney ( 2004 ) describe a “ phrase - extract ” algorithm for extracting phrase pairs from a sentence pair annotated with a 1 - best alignment .
Extends+We follow Och and Ney ( 2004 ) to develop a new phrase extraction algorithm for weighted alignment matrices .
Extends+Since our approach falls into this category ( expanding upon our earlier approach ( Schone and Jurafsky , 2000 ) ) , we describe work in this area in more detail .
Extends+2.3.3 Schone and Jurafsky : induced semantics In our earlier work , we ( Schone and Jurafsky ( 2000 ) ) generated a list of N candidate suffixes and used this list to identify word pairs which share the same stem but conclude with distinct candidate suffixes .
Extends+Using this final lexicon , we can now seek for suffixes in a manner equivalent to what we had done before ( Schone and Jurafsky , 2000 ) .
Extends+In order to obtain semantic representations of each word , we apply our previous strategy ( Schone and Jurafsky ( 2000 ) ) .
Extends+To correlate these semantic vectors , we use normalized cosine scores ( NCSs ) as we had illustrated before ( Schone and Jurafsky ( 2000 ) ) .
Extends+We compare this improved algorithm to our former algorithm ( Schone and Jurafsky ( 2000 ) ) as well as to Goldsmith &apos;s Linguistica ( 2000 ) .
Extends+With the Charniak ( Charniak , 2001 ) language model , our results exceed those of the previous best ( Minnen et al. , 2000 ) on the determiner selection task .
Extends+Brown et al. ( 1993 ) introduce IBM Models 1 - 5 for alignment modelling ; Vogel et al. ( 1996 ) propose a Hidden Markov Model ( HMM ) model for word - to - word alignment , where the words of the source sentence are viewed as states of an HMM and emit target sentence words ; Deng and Byrne ( 2005a ) extend this to an HMM word - tophrase model which allows many - to - one alignments and can capture dependencies within target phrases .
Extends+We follow Brown et al. ( 1993 ) , but extend their modelling framework to include information about the source word from which a target word is emitted .
Extends+We train using Expectation Maximisation ( EM ) , optimising the log probability of the training set { e ( s ) , f ( s ) } Ss = 1 ( Brown et al. , 1993 ) .
Extends+Standard CI Model 1 training , initialised with a uniform translation table so that t ( e | f ) is constant for all source / target word pairs ( f , e ) , was run on untagged data for 10 iterations in each direction ( Brown et al. , 1993 ; Deng and Byrne , 2005b ) .
Extends+We exploit the focusing mechanism proposed by Sidner ( Sidner , 1979 ) ( Sidner , 1981 ) ( Sialher , 1983 ) extending and refining her algorithms .
Extends+We describe the main elements of the focusing approach that are necessary to understand our method , without going into great detail , see ( Sidner , 1979 ) ( Sidner , 1981 ) ( Sidner , 1983 ) .
Extends+• many geographic concepts are inherently vague ( see for example ( Varzi , 2001 ) for a discussion on this topic ) ; • often the underlying data sets contain little explicit geographic information for a generation system to make use of ( Turner et al. , 2008b ) ; • as input to a generation system , georeferenced
Extends+Recently , research into NLG systems that generate text from georeferenced data has begun to emerge ( Dale et al. , 2005 ; Turner et al. , 2006 ; Turner et al. , 2008b ; Thomas and Sripada , 2008 ) .
Extends+The complete input is a series of such snapshots for a number of parameters ( see ( Turner et al. , 2008b ) for details ) .
Extends+The most striking observation about the expert strategy is that the geographic descriptions in the corpora are approximations of the input ( Turner et al. , 2008a ) .
Extends+These pragmatic factors play a large part in determining an experts descriptive strategy , where certain frames of reference may be considered more appropriate to describe certain weather events ( Turner et al. , 2008a ) .
Extends+Using certain frames of reference in certain contexts may result in a poor inference about a particular weather situation ( Turner et al. , 2008b ) .
Extends+Introduction Och ( 2003 ) introduced minimum error rate training ( MERT ) for optimizing feature weights in statistical machine translation ( SMT ) models , and demonstrated that it produced higher translation quality scores than maximizing the conditional likelihood of a maximum entropy model using the same features .
Extends+One solution to the unknown morpheme problem is unknown morpheme acquisition ( Mori and Nagao , 1996 ; Murawaki and Kurohashi , 2008 ) .
Extends+Previous approaches ( Mori and Nagao , 1996 ; Murawaki and Kurohashi , 2008 ) directly or indirectly reply on morphology , or our knowledge on how a morpheme behaves in a sequence of morphemes .
Extends+Mori and Nagao ( 1996 ) , Nagata ( 1999 ) and Murawaki and Kurohashi ( 2008 ) assign POS tags at the morphology level .
Extends+Following unknown morpheme acquisition ( Murawaki and Kurohashi , 2008 ) , we create training data using manually registered nouns , for which we can obtain correct semantic labels .
Extends+The dictionary of JUMAN was augmented with automatically acquired morphemes ( Murawaki and Kurohashi , 2008 ) .
Extends+It can be seen that the formalization in the loglinear combination of our hybrid model is very similar to that of LOP - CRFs ( Smith et al. , 2005 ) .
Extends+Moreover , LOP - CRF ( Smith et al. , 2005 ) is also compared with our hybrid model , since the formalism of our hybrid model can be seen as an extension of LOP - CRFs as described in Section 3 .
Extends+The design of these feature sets was derived from a suggestion in ( Smith et al. , 2005 ) , which exhibited the best performance in the several feature division .
Extends+This makes words to morphemes alignment computationally feasible and the results highly accurate ( Chang et al. , 2001 ; Bai et al. , 2006 ) .
Extends+TransCont is an enhancement over an existing approach that leverages multilingual evidence through projection , SALAAM , described in detail in ( Diab and Resnik , 2002 ) .
Extends+The parallel data we experiment with are the same standard data sets as in ( Diab and Resnik , 2002 ) , namely , Senseval 2 English AW data sets ( SV2AW ) ( Palmer et al. , 2001 ) , and Seneval 3 English AW ( SV3AW ) data set .
Extends+The parallel data we experiment with are the same standard data sets as in ( Diab and Resnik , 2002 ) , namely , Senseval 2 English AW data sets ( SV2AW ) ( Palmer et al. , 2001 ) , and Seneval 3 English AW ( SV3AW ) data set .
Extends+In McIntyre and Lapata ( 2009 ) we presented a story generator that leverages knowledge inherent in corpora without requiring extensive manual involvement .
Extends+In the remainder of this paper we give a brief overview of the system described in McIntyre and Lapata ( 2009 ) and discuss previous applications of GAs in natural language generation ( Section 2 ) .
Extends+Our work also departs from McIntyre and Lapata ( 2009 ) in two important ways .
Extends+Contrary to McIntyre and Lapata ( 2009 ) , we initialize the search with complete stories , rather than generate one sentence at a time .
Extends+In addition to coherence , in McIntyre and Lapata ( 2009 ) we used a scoring function based on interest which we approximated with lexical and syntactic features such as the number of noun / verb tokens / types , the number of subjects / objects , the number of letters , word familiarity , imagery , and so on .
Extends+In the remainder of this paper we give a brief overview of the system described in McIntyre and Lapata ( 2009 ) and discuss previous applications of GAs in natural language generation ( Section 2 ) .
Extends+Evaluation We compared the stories generated by the GA against those produced by the rank - based system described in McIntyre and Lapata ( 2009 ) and a system that creates stories from the plot graph , without any stochastic search .
Extends+Evaluation We compared the stories generated by the GA against those produced by the rank - based system described in McIntyre and Lapata ( 2009 ) and a system that creates stories from the plot graph , without any stochastic search .
Extends+&apos; We also tried the analogous method using thesauruses constructed by the method of ( Li and Abe , 1996 ) 3 T h e m e t h o d of ( Li a n d Abe , 1995 ) o u t p u t s a &apos; tree c u t m o d e l &apos; in a given t h e s a u r u s w i t h c o n d i t i o n a l probabilities att a c h e d to all t h e n o d e s in t h e tree cut .
Extends+which is equivalent to the model proposed by ( Li and Abe , 1996 ) .
Extends+&apos; We also tried the analogous method using thesauruses constructed by the method of ( Li and Abe , 1996 ) 3 T h e m e t h o d of ( Li a n d Abe , 1995 ) o u t p u t s a &apos; tree c u t m o d e l &apos; in a given t h e s a u r u s w i t h c o n d i t i o n a l probabilities att a c h e d to all t h e n o d e s in t h e tree cut .
Extends+The approach of SALAAM is based on work by ( Diab and Resnik , 2002 ) but it goes beyond it in the sense of extending the approach to the tagging of Arabic as a target language .
Extends+The overall parsing algorithm is an inductive statistical parser , which extends the approach by Yamada and Matsumoto ( 2003 ) , by adding six new reduce actions for handling non-projective relations and also performs dependency labeling .
Extends+The parser by Yamada and Matsumoto ( 2003 ) used the following actions : Shift in a configuration 〈 S , n | I , T , A 〉 , pushes n to the stack , producing the configuration 〈 n | S , I , T , A 〉 .
Extends+We use the seems to tree presented in Storoshenko ( 2006 ) , extended with a matching semantic tree .1 Following the derivation in Figure 11 , in syntax , ( βhimselfT ′ ) adjoins to the T ′ root of ( βseems to ) , unifying with its Top φ feature .
Extends+The IE framework extended in this paper utilizes minimally supervised learning of extraction rules for the detection of relation instances ( Xu et al. , 2007 ) .
Extends+DARE ( Xu et al. , 2007 ; Xu , 2007 ) is a minimally supervised machine learning system for RE for free texts consisting of two major parts : 1 ) rule learning , 2 ) relation extraction ( RE ) .
Extends+DARE ( Xu et al. , 2007 ; Xu , 2007 ) is a minimally supervised machine learning system for RE for free texts consisting of two major parts : 1 ) rule learning , 2 ) relation extraction ( RE ) .
Extends+Our work builds on and generalizes earlier efforts by Chambers and Jurafsky ( 2011 ) .
Extends+Table 3 shows the performance our Rel - clusters and compares it with the MUC - 4 template slots discovered by an unsupervised template extraction approach ( Chambers and Jurafsky , 2011 ) .
Extends+We build on prior work that learns narrative chains and narrative schema that link actions by the same protagonists ( Chambers and Jurafsky , 2008 ; Chambers and Jurafsky , 2009 ) , and work that extracts event templates from a narrowly focused corpus ( Chambers and Jurafsky , 2011 ) .
Extends+In ( Elfardy and Diab , 2012a ) , we present a set of guidelines for token - level identification of DA while in ( Elfardy and Diab , 2012b ) , ( Elfardy et al. , 2013 ) we tackle the problem of token - level dialect - identification by casting it as a code - switching problem .
Extends+In ( Elfardy and Diab , 2012a ) , we present a set of guidelines for token - level identification of DA while in ( Elfardy and Diab , 2012b ) , ( Elfardy et al. , 2013 ) we tackle the problem of token - level dialect - identification by casting it as a code - switching problem .
Extends+Elfardy and Diab ( 2013 ) presents our solution for the sentence - level dialect identification problem .
Extends+We instead make use of a simple ‘ two - stage ’ approach for extending the SS - SCM approach to the second - order parsing model of ( Carreras , 2007 ) .
Extends+Previous work ( McDonald and Pereira , 2006 ; Carreras , 2007 ) has shown that second - order parsing models , which include information from “ sibling ” or “ grandparent ” relationships between dependencies , can give significant improvements in accuracy over first - order parsing models .
Extends+We used the method proposed by ( Carreras , 2007 ) for our second - order parsing model .
Extends+In a second set of experiments , we make use of the feature set used in the semi-supervised approach of ( Koo et al. , 2008 ) .
Extends+We simply use the clusterbased feature - vector representation f ( x , y ) introduced by ( Koo et al. , 2008 ) as the basis of our approach .
Extends+Koo et al. ( 2008 ) describe a semi-supervised approach that incorporates cluster - based features , and that gives competitive results on dependency parsing benchmarks .
Extends+Our baseline features ( “ baseline ” ) are very similar to those described in ( McDonald et al. , 2005a ; Koo et al. , 2008 ) : these features track word and POS bigrams , contextual features surrounding dependencies , distance features , and so on .
Extends+These settings match the evaluation setting in previous work such as ( McDonald et al. , 2005a ; Koo et al. , 2008 ) .
Extends+Our baseline features ( “ baseline ” ) are very similar to those described in ( McDonald et al. , 2005a ; Koo et al. , 2008 ) : these features track word and POS bigrams , contextual features surrounding dependencies , distance features , and so on .
Extends+Table 3 gives results for the SS - SCM method under various configurations : for first and secondorder parsing models , with and without the cluster features of ( Koo et al. , 2008 ) , and for varying amounts of labeled data .
Extends+It is clear that the gains from our method are larger for smaller labeled data sizes , a tendency that was also observed in ( Koo et al. , 2008 ) .
Extends+Our baseline features ( “ baseline ” ) are very similar to those described in ( McDonald et al. , 2005a ; Koo et al. , 2008 ) : these features track word and POS bigrams , contextual features surrounding dependencies , distance features , and so on .
Extends+This paper has described an extension of the semi-supervised learning approach of ( Suzuki and Isozaki , 2008 ) to the dependency parsing problem .
Extends+We follow a similar approach to that of ( Suzuki and Isozaki , 2008 ) in partitioning f ( x , y ) , where the k different feature vectors correspond to different feature types or feature templates .
Extends+Note that it is possible to iterate the method — steps 2 and 3 can be repeated multiple times ( Suzuki and Isozaki , 2008 ) — but in our experiments we only performed these steps once .
Extends+We follow a similar approach to that of ( Suzuki and Isozaki , 2008 ) in partitioning f ( x , y ) , where the k different feature vectors correspond to different feature types or feature templates .
Extends+Based on this observation , Kennedy and Boguraev ( 1996 ) have suggested an adaptation of Lappin and Leass 's approach to the shallow analysis frontend of English Constraint Grammar ( Karlsson et al. 1995 ) , which provides a part - of - speech tagging comprising an assignment of syntactic function but no constituent structure .
Extends+In contrast to the approach of Kennedy and Boguraev ( 1996 ) , which follows the shallow description model , the ROSANA algorithm is based on the deficient description model .
Extends+As in the approaches of Lappin and Leass ( 1994 ) and Kennedy and Boguraev ( 1996 ) , weighted salience factors are employed for scoring and choosing among the candidates that remain after restriction application ( cfXXX Step 2 ( a ) of the ROSANA algorithm ) .
Extends+The precision measure coincides with the accuracy measure employed by , for example , Lappin and Leass ( 1994 ) or Kennedy and Boguraev ( 1996 ) .
Extends+Based on this observation , Kennedy and Boguraev ( 1996 ) have suggested an adaptation of Lappin and Leass 's approach to the shallow analysis frontend of English Constraint Grammar ( Karlsson et al. 1995 ) , which provides a part - of - speech tagging comprising an assignment of syntactic function but no constituent structure .
Extends+As in the approaches of Lappin and Leass ( 1994 ) and Kennedy and Boguraev ( 1996 ) , weighted salience factors are employed for scoring and choosing among the candidates that remain after restriction application ( cfXXX Step 2 ( a ) of the ROSANA algorithm ) .
Extends+The weaknesses of our Earley SLM have led us to consider probabilistic left - corner grammar ( PLCG ) parsing ( Manning and Carpenter , 1997 ) , which follows a mixed bottom - up and top - down approach .
Extends+Some of them used corpora ( Hatzivassiloglou and McKeown , 1997 ; Turney and Littman , 2003 ) , while others used dictionaries ( Kobayashi et al. , 2001 ; Kamps et al. , 2004 ; Takamura et al. , 2005 ; Esuli and Sebastiani , 2005 ) .
Extends+We construct a lexical network , which Takamura et al. ( 2005 ) call the gloss network , by linking two words if one word appears in the gloss of the other word .
Extends+Takamura et al. ( 2005 ) used the Ising model to extract semantic orientations of words ( not phrases ) .
Extends+Takamura et al. ( 2005 ) proposed two kinds of criteria .
Extends+Introduction Lexical weighting features ( Koehn et al. , 2003 ) estimate the probability of a phrase pair or translation rule word - by - word .
Extends+This paper extends the IBM Machine Translation Group 's concept of fertility ( Brown et al. , 1993 ) to the generation of clumps for natural language understanding .
Extends+In earlier IBM translation systems ( Brown et al. , 1993 ) each English word would be generated by , or " aligned to " , exactly one formal language word .
Extends+As an example of such work , Zhang et al. ( 2008 ) have shown in the past that deep linguistic parsing outputs can be integrated to help improve the performance of the English semantic role labeling task .
Extends+Comparing to Zhang et al. ( 2008 ) , this architecture simplified the syntactic component , and puts more focus on the integration of deep parsing outputs .
Extends+Confirming the observation of Zhang et al. ( 2008 ) , the gain with HPSG features is more significant on outdomain tests , this time on German as well .
Extends+The semantic role labeling component used in the submitted system is similar to the one described by Zhang et al. ( 2008 ) .
Extends+Confirming the observation of Zhang et al. ( 2008 ) , the gain with HPSG features is more significant on outdomain tests , this time on German as well .
Extends+The determiner system builds on the ideas described in Rozovskaya and Roth ( 2010c ) .
Extends+These errors are some of the most common and also some of the most difficult for ESL learners ( Leacock et al. , 2010 ) ; even very advanced learners make these mistakes ( Rozovskaya and Roth , 2010a ) .
Extends+Our article and preposition modules build on the elements of the systems described in Rozovskaya and Roth ( 2010b ) , Rozovskaya and Roth ( 2010c ) and Rozovskaya and Roth ( 2011 ) .
Extends+The method was implemented within the Averaged Perceptron ( AP ) algorithm ( Rozovskaya and Roth , 2010c ; Rozovskaya and Roth , 2010b ) , a discriminative learning algorithm , and this is the algorithm that we use in this work .
Extends+The article classifier is based on the artificial errors approach ( Rozovskaya and Roth , 2010c ) .
Extends+The determiner system builds on the ideas described in Rozovskaya and Roth ( 2010c ) .
Extends+These errors are some of the most common and also some of the most difficult for ESL learners ( Leacock et al. , 2010 ) ; even very advanced learners make these mistakes ( Rozovskaya and Roth , 2010a ) .
Extends+Our article and preposition modules build on the elements of the systems described in Rozovskaya and Roth ( 2010b ) , Rozovskaya and Roth ( 2010c ) and Rozovskaya and Roth ( 2011 ) .
Extends+The method was implemented within the Averaged Perceptron ( AP ) algorithm ( Rozovskaya and Roth , 2010c ; Rozovskaya and Roth , 2010b ) , a discriminative learning algorithm , and this is the algorithm that we use in this work .
Extends+The article classifier is based on the artificial errors approach ( Rozovskaya and Roth , 2010c ) .
Extends+The preposition classifier uses a combined system , building on work described in Rozovskaya and Roth ( 2011 ) and Rozovskaya and Roth ( 2010b ) .
Extends+Our article and preposition modules build on the elements of the systems described in Rozovskaya and Roth ( 2010b ) , Rozovskaya and Roth ( 2010c ) and Rozovskaya and Roth ( 2011 ) .
Extends+The NBpriors method is a special adaptation technique for the Na ¨ ıve Bayes algorithm ( Rozovskaya and Roth , 2011 ) .
Extends+We use training data to replace the prior parameters of the model ( see Rozovskaya and Roth , 2011 for more detail ) .
Extends+As two examples , ( Rabiner , 1989 ) and ( Charniak et al. , 1993 ) give good overviews of the techniques and equations used for Markov models and part - ofspeech tagging , but they are not very explicit in the details that are needed for their application .
Extends+The method is based on Reynar &apos;s maximisation algorithm ( Reynar , 1998 ; Helfman , 1996 ; Church , 1993 ; Church and Helfman , 1993 ) .
Extends+The method is based on Reynar &apos;s maximisation algorithm ( Reynar , 1998 ; Helfman , 1996 ; Church , 1993 ; Church and Helfman , 1993 ) .
Extends+The uses of this procedure include information retrieval ( Hearst and Plaunt , 1993 ; Hearst , 1994 ; Yaari , 1997 ; Reynar , 1999 ) , summarization ( Reynar , 1998 ) , text understanding , anaphora resolution ( Kozima , 1993 ) , language modelling ( Morris and Hirst , 1991 ; Beeferman et al. , 199717 ) and improving document navigation for the visually disabled ( Choi , 2000 ) .
Extends+Multi-source methods combine lexical cohesion with other indicators of topic shift such as cue phrases , prosodic features , reference , syntax and lexical attraction ( Beeferman et al. , 1997a ) using decision trees ( Miike et al. , 1994 ; Kurohashi and Nagao , 1994 ; Litman and Passonneau , 1995 ) and probabilistic models ( Beeferman et al. , 1997b ; Hajime et al. , 1998 ; Reynar , 1998 ) .
Extends+The method is based on Reynar &apos;s maximisation algorithm ( Reynar , 1998 ; Helfman , 1996 ; Church , 1993 ; Church and Helfman , 1993 ) .
Extends+Our evaluation strategy is a variant of that described in ( Reynar , 1998 , 71 - 73 ) and the TDT segmentation task ( Allan et al. , 1998 ) .
Extends+Other performanc ( ; measures include the popular precision and recall metric ( PR ) ( Hearst , 1994 ) , fuzzy PR ( Reynar , 1998 ) and edit distance ( Ponte and Croft , 1997 ) .
Extends+Space precludes the description of a final automaton operation called Bounded Local Optimization ( Walther , 1999 ) that turns out to be useful here to
Extends+The new grammar is basically a scaleddown and adapted version of the Core Language Engine grammar for English ( Pulman 1992 ; Rayner 1993 ) ; concrete development work and testing were organized around a speech interface to a set of functionalities o ered by a simple simulation of the Space Shuttle ( Rayner , Hockey and James 2000 ) .
Extends+( A dendroid distribution can also be considered as a re.stricted form of the Bayesian Network ( Pearl , 1988 ) . )
Extends+In this section we outline the main ideas underlying a polynomial time recognition algorithm for PlPATR that generalizes the CKY algorithm ( Kasami , 1965 ; Younger , 1967 ) .
Extends+In this section we outline the main ideas underlying a polynomial time recognition algorithm for PlPATR that generalizes the CKY algorithm ( Kasami , 1965 ; Younger , 1967 ) .
Extends+Rather than adapting the Application Rule to allow functions to be applied to one argument at a time , Bar - Hillel &apos;s second system ( often called AB Categorial Grammar , or Adjukiewicz / Bar - Hillel CG , Bar - Hillel 1964 ) adopted a &apos; Curried &apos; notation , and this has been adopted by most CGs since .
Extends+It is important to note that this is a consequence of a general limitation of dataflow analysis ( see also Mellish , 1981 ) .
Extends+For the implementation we extended the HPSG grammar ( Pollard and Sag , 1994 ) which Gerald Penn and Bob Carpenter first encoded in ALE ( Carpenter , 1993 ) .
Extends+This semantics was constructed ( Zadrozny 1987a , 1987b ) as a formal framework for default and commonsense reasoning .
Extends+Because it is also consistent , it will be chosen as a best interpretation of S , ( cfXXX Zadrozny 1987a , 1987b ) .
Extends+It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX Zadrozny 1987b ) .
Extends+A formula 4 of L ( =) , the language with equality , is weakly R M - abductible from an object theory T , denoted by T ~ - a M G iff there exists a partial theory T E PT ( T ) and a preferred model M E PM ( T ) such that M ~ G i.e. 4 is true in at least one preferred model of the partial theory T. Note : The notions of strong provability and strong R M - abduction can be introduced by replacing &quot; there exists &quot; by &quot; all &quot; in the above definitions ( cfXXX Zadrozny 1987b ) .
Extends+This semantics was constructed ( Zadrozny 1987a , 1987b ) as a formal framework for default and commonsense reasoning .
Extends+Because it is also consistent , it will be chosen as a best interpretation of S , ( cfXXX Zadrozny 1987a , 1987b ) .
Extends+It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX Zadrozny 1987b ) .
Extends+A formula 4 of L ( =) , the language with equality , is weakly R M - abductible from an object theory T , denoted by T ~ - a M G iff there exists a partial theory T E PT ( T ) and a preferred model M E PM ( T ) such that M ~ G i.e. 4 is true in at least one preferred model of the partial theory T. Note : The notions of strong provability and strong R M - abduction can be introduced by replacing &quot; there exists &quot; by &quot; all &quot; in the above definitions ( cfXXX Zadrozny 1987b ) .
Extends+∗ A brief version of this work , with some additional material , first appeared as ( Eisner , 2001a ) .
Extends+Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; Eisner , 2001b ; Lafferty et al. , 2001 ) .
Extends+Per - state joint normalization ( Eisner , 2001b , § 8.2 ) is similar but drops the dependence on a .
Extends+∗ A brief version of this work , with some additional material , first appeared as ( Eisner , 2001a ) .
Extends+Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; Eisner , 2001b ; Lafferty et al. , 2001 ) .
Extends+Per - state joint normalization ( Eisner , 2001b , § 8.2 ) is similar but drops the dependence on a .
Extends+3 For our analysis of gapping , we follow Sag ( 1976 ) in hypothesizing that a post-surface-structure level of syntactic representation is used as the basis for interpretation .
Extends+Given the representation in Figure 1 as the source , the semantics for the missing VP may be recovered in both the target and the source clauses , and that abstracting these elements results in an " open proposition " that both clauses share ( Sag , 1976 ; Prince , 1986 ; Steedman , 1990 ) .
Extends+The following pair of examples are adapted from those of Sag ( 1976 , pg .
Extends+This process is reliant on performing comparison and generalization operations on the corresponding representations ( Scha and Polanyi , 1988 ; Hobbs , 1990 ; Priist , 1992 ; Asher , 1993 ) .
Extends+SHobbs ( 1990 ) , following Hume ( 1748 ) , suggests a classification of coherence relations into three broad categories , namely Resemblance , Cause or Effect , and Contiguity ( Hume &apos;s terminology ) .
Extends+Previous proposals to circumvent the above problem ( Good , 1953 ; Jelinek , Mercer , and Roukos , 1992 ; Katz , 1987 ; Church and Gale , 1991 ) take the MLE as an initial estimate and adjust it so that the total probability of seen bigrams is less than one , leaving some probability mass for unseen bigrams .
Extends+Previous proposals to circumvent the above problem ( Good , 1953 ; Jelinek , Mercer , and Roukos , 1992 ; Katz , 1987 ; Church and Gale , 1991 ) take the MLE as an initial estimate and adjust it so that the total probability of seen bigrams is less than one , leaving some probability mass for unseen bigrams .
Extends+W ( w ~ , wl ) is defined as 1The perplexity of a conditional bigram probability model / 5 with respect to the true bigram distribution is an information - theoretic measure of model quality ( Jelinek , Mercer , and Roukos , 1992 ) that can be empirically estimated by exp - ~ ~ - ~ i log P ( w , tu,i_l ) for a test set of length N. Intuitively , the lower the perplexity of a model the more likely the model is to assign high probability to bigrams that actually occur .
Extends+Caswey et al. ( Cawsey et al. , 1993 ; Logan et al. , 1994 ) introduced the idea of utilizing a belief revision mechanism ( Galliers , 1992 ) to predict whether a set of evidence is sufficient to change a user 's existing belief and to generate responses for information retrieval dialogues in a library domain .
Extends+We model the strength of a belief using endorsements , which are explicit records of factors that affect one 's certainty in a hypothesis ( Cohen , 1985 ) , following ( Galliers , 1992 ; Logan et al. , 1994 ) .
Extends+The evaluator then employs a simplified version of Galliers ' belief revision mechanism 2 ( Galliers , 1992 ; Logan et al. , 1994 ) to compare the strengths of the evidence that supports and attacks _ bel .
Extends+7 Notice that steps 2 and 4 of the algorithm invoke a function , Predict , that makes use of the belief revision mechanism ( Galliers , 1992 ) discussed in Section 4.1 to predict the user 's acceptance or unacceptance of .
Extends+Each feature has a fixed the CART algorithm in ( Breiman et al. , 1984 ) .
Extends+I discuss two possible filter optimizations based on a program transformation technique called unfolding ( Tamaki and Sato , 1984 ) also referred to as partial execution , e.g. , in Pereira and Shieber ( 1987 ) .
Extends+As shown by Baum et al. ( 1970 ) , these expectations can be calculated efficiently using dynamic programming techniques .
Extends+In the framework of the EM algorithm ( Dempster et al. , 1977 ) , we can formalize clustering as an estimation problem for a latent class ( LC ) model as follows .
Extends+In our experiments , we have used a conjugate - gradient optimization program adapted from the one presented in Press et al. ( 1992 ) .
Extends+We had the most success with a slightly modified version of the simulated annealing optimizer described in Press et al. ( 1992 ) .
Extends+WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( Nakano et al. , 1999b ) , a video - recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather information system ( Dohsaka et al. , 2000 ) .
Extends+Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns — syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .
Extends+The shallow parser used is the SNoW - based CSCL parser ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) .
Extends+Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL - 2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .
Extends+In his analysis of Yarowsky ( 1995 ) , Abney ( 2004 ) formulates several variants of bootstrapping .
Extends+This paper is an extension of our previous work in Snider and Diab ( 2006 ) , which found a preliminary effect of syntactic frames on the precision of MSA verb clustering .
Extends+( Snider and Diab , 2006 ) Finally , only active verbs are included in this study , rather than attempt to reconstruct the argument structure of passives .
Extends+Merlo and Stevenson ( 2001 ) found that this feature improved their English verb clusters , but in Snider & Diab ( 2006 ) , we found this feature to not contribute significantly to Arabic verb clustering quality .
Extends+This error analysis uses the error metric from Snider & Diab ( 2006 ) that allows us to test just the HYP verbs that match the GOLD set .
Extends+It is worth noting that comparing our current results to those obtained in Snider & Diab ( 2006 ) , we show a significant improvement given the same precision oriented metric .
Extends+The caption generation task bears some resemblance to headline generation ( Dorr et al. , 2003 ; Banko et al. , 2000 ; Jin and Hauptmann , 2002 ) where the aim is to create a very short summary for a document .
Extends+Word - based Model Our first abstractive model builds on and extends a well - known probabilistic model of headline generation ( Banko et al. , 2000 ) .
Extends+Following Banko et al. ( 2000 ) , we approximated the length distribution with a Gaussian .
Extends+Shinyama and Sekine ( 2006 ) describe an approach to template learning without labeled data .
Extends+We have previously attempted to address the second question , by proposing the information ordering task for evaluating dialogue coherence ( Gandhe and Traum , 2008 ) .
Extends+Nothman et al. ( 2008 ) used a similar method to create a NE annotated text in English .
Extends+We followed a similar path to Nothman et al. ( 2008 ) and broke down the process into four steps :
Extends+Our approach is based on the work of Nothman et al. ( 2008 ) .
Extends+For a detailed presentation of the UNFOLD relation we refer the reader to ( Soricut , 2006 ) .
Extends+The system presented here extends a previous version of ELEON [ 3 ] , which supports using an external DL reasoner to catch logical errors by checking the consistency of the authored ontology .
Extends+Recent work in computational psycholinguistics shows that morpheme lexica can be acquired in an unsupervised manner from a corpus of words by selecting the lexicon that best balances productivity and reuse ( e.g. Goldwater et al. ( 2009 ) and others ) .
Extends+In the case of word formation , recent work in computational psycholinguistics has shown how an inventory of morphemes can be acquired by selecting a lexicon that best balances the ability of individual sound sequences to combine productively against the reusability of those sequences ( e.g. , Brent ( 1999 ) , Goldwater et al. ( 2009 ) , Feldman et al. ( 2009 ) , O’Donnell et al. ( 2011 ) , Lee et al. ( 2011 ) . )
Extends+Following earlier work on Bayesian lexicon learning ( e.g. Goldwater et al. ( 2009 ) , we use a distribution over lexical items known as the Pitman – Yor Process ( PYP ) ( Pitman and Yor , 1995 ) .
Extends+The approach of this paper builds on previous work on Bayesian lexicon learning starting with Goldwater et al. ( 2009 ) .
Extends+The standard n - gram language model ( Goodman , 2001 ) assigns probabilities to hypotheses in the target language conditioning on a context history of the preceding n − 1 words .
Extends+Approach We extend the min - cut formulation of Blum and Chawla ( 2001 ) to multiple labels and structured variables by adapting a linear - programming encoding of metric labeling problems .
Extends+Relation to Previous work Our work is set of extensions to the work ofBlum and Chawla ( 2001 ) , which we have already described .
Extends+The text can also be represented by a low - dimensional dense vector derived by linear projection models like latent semantic analysis ( LSA ) ( Deerwester et al. , 1990 ) , by discriminative learning methods like Siamese neural networks ( Yih et al. , 2011 ) , recurrent neural networks ( Mikolov et al. , 2013 ) and recursive neural networks ( Socher et al. , 2011 ) , or by graphical models such as probabilistic latent semantic analysis ( PLSA ) ( Hofmann , 1999 ) and latent Dirichlet allocation ( LDA ) ( Blei et al. , 2003 ) .
Extends+In many of these applications , Latent Semantic Analysis ( LSA ) ( Deerwester et al. , 1990 ) has been widely used , serving as a fundamental component or as a strong baseline .
